<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>学习 &amp; 技术 :: 计算机 on LRYP&#39;s Blog</title>
        <link>https://littlereuben.github.io/categories/cs/</link>
        <description>Recent content in 学习 &amp; 技术 :: 计算机 on LRYP&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>LRYP</copyright>
        <lastBuildDate>Mon, 23 Feb 2026 00:00:00 +0000</lastBuildDate><atom:link href="https://littlereuben.github.io/categories/cs/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>段设（研究生程序分析与设计）笔记</title>
        <link>https://littlereuben.github.io/study/course-galgo/</link>
        <pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/course-galgo/</guid>
        <description>&lt;div style=&#34;background-color: #FFF9B9; color: #796E00; padding-left: 1em; border-left: 4px solid #DED041; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;学习笔记&lt;/b&gt;。&lt;/div&gt;
&lt;p&gt;$\gdef\O{\Omicron}\gdef\Ot{\tilde\Omicron}\gdef\e{\mathrm{e}}\gdef\eps{\varepsilon}$&lt;/p&gt;
&lt;p&gt;很遗憾下半学期的没空记笔记了。有机会补。&lt;/p&gt;
&lt;h3 id=&#34;apsp&#34;&gt;APSP
&lt;/h3&gt;&lt;p&gt;APASP 相关更全面的算法见 &lt;a class=&#34;link&#34; href=&#34;https://littlereuben.github.io/study/course-galgo-survey&#34; &gt;我的 survey&lt;/a&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/pii/S0022000085710781&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Seidel &amp;lsquo;95]&lt;/a&gt; 无向无权图 APSP $\Ot(n^\omega)$&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/document/743464&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Zwick &amp;lsquo;98]&lt;/a&gt; 有向无权图 APSP $\Ot(n^\mu)$，无向有权图 $1+\epsilon$ 近似 APSP $\Ot(n^\omega\epsilon^{-1}\log W)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=814635&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Shoshan, Zwick &amp;lsquo;99&lt;/a&gt;] 无向有权图 APSP $\Ot(Wn^\omega)$&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1312.6680&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Williams &amp;lsquo;14]&lt;/a&gt; 有（实数）权图 APSP $n^3/2^{\Omega(\sqrt{\log n})}$&lt;/li&gt;
&lt;li&gt;[Aingworth, Chekuri, Indyk, Motwani &amp;lsquo;96] 无向无权图 $+2$ APSP $\Ot(n^{2.5})$&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://theory.stanford.edu/~virgi/cs267/papers/dorhalperinzwick.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Dor, Halperin, Zwick &amp;lsquo;96]&lt;/a&gt; 无向无权图 $+k$（偶数）APSP $\Ot(\min{kn^{2-1/k}m^{1/k},kn^{2+1/(3k-4)}})$&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://drops.dagstuhl.de/storage/00lipics/lipics-vol229-icalp2022/LIPIcs.ICALP.2022.50/LIPIcs.ICALP.2022.50.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Deng, Kirkpatrick, Rong, Williams, Zhong &amp;lsquo;22]&lt;/a&gt; 无向无权图 $+2$ APSP $\Ot(n^{2.29})$&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2208.02862&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Dürr &amp;lsquo;23]&lt;/a&gt; 无向无权图 $+2$ APSP $\Ot(n^{2.2593})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/pdf/10.5555/314161.314190&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cohen-Zwick &amp;lsquo;97&lt;/a&gt;] 无向有权图 $\times 3$ APSP $\Ot(n^2)$，无向有权图 $\times 7/3$ APSP $\Ot(n^{7/3})$，无向有权图 $\times 2$ APSP $\Ot(n^{3/2}m^{1/2})$&lt;/li&gt;
&lt;li&gt;[Thorup, Zwick &amp;lsquo;01] 无向有权图 distance oracle $\times(2k-1)$ 空间 $\O(kn^{1+1/k})$，单次查询 $\O(k)$&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://link.springer.com/chapter/10.1007/978-3-540-73951-7_47&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Berman, Kasiviswanathan &amp;lsquo;07]&lt;/a&gt; 无向无权图 $(2,1)$ APSP $\Ot(n^2)$&lt;/li&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2307.09258&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Dory et al. &amp;lsquo;23]&lt;/a&gt; 无向无权图 $\times 2$ APSP $\Ot(n^{2.032})$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;hitting-set-的问题&#34;&gt;Hitting set 的问题
&lt;/h4&gt;&lt;p&gt;如果以 $p$ 的概率选点，要覆盖所有度数 $\ge d$ 的点 $u_1,\cdots,u_t$，那么成功率是
$$
\Pr[\text{cover}(u_1)\land\cdots\land\text{cover}(u_t)]=1-\Pr[\overline{\text{cover}(u_1)}\lor\cdots\lor\overline{\text{cover}(u_t)}]\ge 1-t\Pr[\overline{\text{cover}(u_1)}]\ge 1-t(1-p)^d\ge 1-t\e^{-dp}
$$
可以取 $p=\frac{c\ln n}d$。但是问题是 $|S|$ 太大也不行。因此算法实际上做的是如果随机出来的 $S$ 没有覆盖所有的大度点，&lt;strong&gt;或者&lt;/strong&gt; $|S|$ 太大，就直接 Fail。我们假设一个阈值 $2pn$，成功率是
$$
\Pr[\text{allcover}\land |S|\le 2pn]\ge 1-\Pr[\overline{\text{allcover}}]-\Pr[|S|&amp;gt;2pn]\ge 1-n^{-c+1}-\e^{-pn/3}\tag{Chernoff}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;一个简单的确定性算法如下：每次选择能覆盖最多 $\ge d$ 度点的点，记 $f(t)$ 为 $t$ 个 $\ge d$ 度点的 hitting set 大小，
$$
f(t)\le f\left(t-\left\lceil\frac{td}{n}\right\rceil\right)+1\le-\log_{1-d/n}t=-\frac{\ln t}{\ln(1-d/n)}\le\frac nd\ln t
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;seidel-95&#34;&gt;[Seidel &amp;lsquo;95]
&lt;/h4&gt;&lt;p&gt;对于邻接矩阵 $A$，考虑以下算法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;若 $A$ 是完全图，则直接返回。&lt;/li&gt;
&lt;li&gt;递归计算 $A^2+A$ 对应邻接矩阵的距离矩阵 $D^\prime$。
&lt;ul&gt;
&lt;li&gt;这时 $d^\prime_{i,j}=\lceil d_{i,j}/2\rceil$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;计算 $P=D^\prime A$。$d_{i,j}=2d^\prime_{i,j}-[p_{i,j}&amp;lt;\mathrm{deg}_jd^\prime_{i,j}]$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;时间复杂度 $\O(n^\omega\log n)$。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;考虑算出最短路树相关信息。定义后继矩阵 $S$，$s_{i,j}$ 表示 $\pi(i,j)$ 上 $i$ 之后第一个点。&lt;/p&gt;
&lt;p&gt;对于 01 矩阵 $A,B$ 定义 $(A*B)_{i,j}=\set{k\mid a_{i,k}=b_{k,j}=1\text{ 中任意一个}}$，这称为 witness。那么 $S_{i,j}=(A*D^{(d_{i,j}-1)})_{i,j}$，其中 $D^{(d)}=\set{[d_{i,j}=d]}$。这样需要对于每一个 $d$ 求 witness。&lt;/p&gt;
&lt;p&gt;优化是这样的：注意到对于 $i$ 的相邻点 $k$，$d_{k,j}-d_{i,j}\in\set{1,0,-1}$，因此只需要找 $d_{k,j}\equiv d_{i,j}-1\pmod 3$ 的 $k$ 即可，这样就减少到 $3$ 次 witness。求 witness 考虑以下算法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将 $A$ 的第 $i$ 列乘 $i$。&lt;/li&gt;
&lt;li&gt;枚举 $l=0\sim\log n$，对于每个 $l$ 重复 $c\log n$ 次：
&lt;ul&gt;
&lt;li&gt;随机取出 $A$ 的 $2^l$ 列和 $B$ 的对应 $2^l$ 行相乘得到 $C$。检查 $C_{i,j}$ 是否是个 witness，若是则记下。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;对于 $(AB)_{i,j}\ne 0$ 但没有得到 witness 的 $(i,j)$ 暴力。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;简而言之，如果 $(AB)_{i,j}=p$，那么在 $n/2\le 2^lp\le n$ 的那一轮 $l$，有大概率有&lt;strong&gt;恰好&lt;/strong&gt;一对贡献的 $(a_{i,k},b_{k,j})$ 被选中。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;综上所述，APSP 的距离矩阵与后继矩阵都可以在 $\Ot(n^\omega)$ 内算出。&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;dor-halperin-zwick-96&#34;&gt;[Dor, Halperin, Zwick &amp;lsquo;96]
&lt;/h4&gt;&lt;p&gt;无向无权图。&lt;a class=&#34;link&#34; href=&#34;https://littlereuben.github.io/study/paper4/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这里&lt;/a&gt;讲过 $+2$ 的 $\Ot(n^{5/2})$，可以回顾一下。其中涉及两种做法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果只考虑最短路上的最大点度在 $a\sim b$ 之间，那么从 $\ge a$ 度点的 hitting set 出发，在只保留 $\le b$ 度点邻边的图上跑 MSSP，$\Ot(n^2b/a)$。求时&lt;strong&gt;枚举关键点&lt;/strong&gt;，$\Ot(n^3/a)$。&lt;/li&gt;
&lt;li&gt;如果只考虑最短路上的最大点度 $&amp;lt;a$ 的情况，直接在只保留 $&amp;lt;a$ 度点邻边的稀疏图上跑 APSP，$\O(n^2a)$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇 paper 主要提了一个折中的做法，其思路是“枚举一侧 BFS 另一侧”：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从 $\ge a$ 度点的 hitting set 出发，在只保留 $\le b$ 度点邻边的图上跑 MSSP，然后，对于每个 $u$，在原图只保留 $&amp;lt;a$ 度点邻边和 hitting set 边的基础上，$u$ 向各关键点连边权为 $d_{u,\cdot}$ 的边，再跑 Dijkstra。这时，如果我们考虑 $\pi(u,v)$ 上最后一个 $a\sim b$ 度点，它之后的路径边都在图上。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这里的三类边在课件/paper 里对应的就是形如 $E_{i+1}\cup E^*\cup(\set{u}\times D_i)$。&lt;/p&gt;
&lt;p&gt;仔细分析的话，MSSP 的复杂度是 $\Ot(n\min\set{nb,m}/a)$ 的，对每个点跑最短路还是 $\O(n^2a)$ 的。用该做法代替上面枚举关键点，取 $a=(m/n)^{1/2}$，得到 $\Ot(n^{3/2}m^{1/2})$ 的算法。&lt;/p&gt;
&lt;p&gt;而原做法由于 $n^3/a$ 盖过了 $n^2b/a$，所以复杂度和 $m$ 无关。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;现在我们将点度分三层 $n-b-a-1$：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最大点度 $&amp;gt;b$：跑原方法，$\Ot(n^3/b)$。&lt;/li&gt;
&lt;li&gt;最大点度 $\in[a,b]$：跑新做法，$\Ot(n\min\set{nb,m}/a+n^2a)$。&lt;/li&gt;
&lt;li&gt;最大点度 $&amp;lt;a$：稀疏图暴力（实际上新做法自动覆盖了这部分），$\Ot(n^2a)$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;取 $a=n^{1/3}$，$b=n^{2/3}$ 即得到 $\Ot(n^{7/3})$。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;现在考虑 $+2k$ 近似，我们考虑对关键点分层，每层的关键点依赖上一层的关键点快速求出最短路，代价是近似 $+2$。&lt;/p&gt;
&lt;p&gt;假设点度分层为 $n-s_1-s_2-\cdots-s_k=1$，$D_i$ 表示度数 $&amp;gt;s_i$ 的点的 hitting set。假如我们得到了 $D_i$ 到 $V$ 的 $+2k$ 近似，那么可以用新做法得到 $D_{i+1}$ 到 $V$ 的 $+2(k+1)$ 近似：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 $u\in D_{i+1}$，连上 $u$ 与 $D_i$ 的边，保留所有度数 $&amp;lt;s_i$ 点邻边和 hitting set 边，跑 Dijkstra。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个的时间是 $\Ot(|D_{i+1}|ns_i)=\Ot(n^2s_i/s_{i+1})$，或者更仔细地算，$\Ot(n\min\set{ns_i,m}/s_{i+1})$。&lt;/p&gt;
&lt;p&gt;取 $s_i=n^{1-i/k}$，得到 $\Ot(n^{2+1/k})$；取 $s_i=(m/n)^{1-i/k}$，得到 $\Ot(n^{2-1/k}m^{1/k})$。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我们发现，如果用旧方法去做这件事情，应该这样做：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 $u\in D_{i+1}$，$v\in V$，枚举 $D_1\sim D_i$ 中的点，松弛；最后，还是要只保留度数 $&amp;lt;s_i$ 点邻边跑 Dijkstra。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;时间是 $\Ot(|D_{i+1}|n\sum_{j\le i}|D_j|+|D_{i+1}|ns_i)=\Ot(n^3/s_is_{i+1}+n^2s_i/s_{i+1})$（这个等号假定 $\set{|D_i|}$ 是等比数列）。&lt;/p&gt;
&lt;p&gt;现在我们发现，对于前面较大的 $s_i$，$n^3/s_is_{i+1}$ 这项并非瓶颈；同时注意到，这样求出来永远是 $+2$ 近似。这一不同，是由于旧方法的证明是考虑 $\pi$ 上最大度点，而新方法的证明是考虑 $\pi$ 上最后一个超过阈值的点，所导致的。&lt;/p&gt;
&lt;p&gt;于是，考虑分成 $t+k-1$ 层，$s_i=n^{1-i/(t+k-1)}$。对于前 $t$ 层跑旧方法，后 $k-1$ 层跑新方法，得到 $+2k$ 近似。我们希望 $n^3/s_ts_{t-1}$ 不 dominant，即
$$
1+\frac{2t-1}{t+k-1}\le2+\frac{1}{t+k-1}\implies t\le k+1
$$
得到 $\Ot(n^{2+1/2k})$。进一步地，仔细考虑如何保证第 $t+1$ 层是 $+4$ 近似：只需保证 $D_t$ 到 $D_{t+1}$ 是 $+2$ 近似即可。因此松弛这步可以不枚举 $v\in V$ 而是枚举 $v\in D_{i+1}$，时间变为 $n^3/s_{t+1}s_ts_{t-1}$。
$$
\frac{3t}{t+k-1}\le 2+\frac{1}{t+k-1}\implies t\le 2k-1
$$
得到 $\Ot(n^{2+1/(3k-2)})$。进一步，注意到 $u\in D_t$ 到 $v\in D_{t+1}$ 的 $\pi$ 上如果没有度 $&amp;gt;s_{t-2}$ 的点，那么 $+2$ 近似可以通过在新方法的图中，$u\to$ 最后一个度 $&amp;gt;s_{t-1}$ 的点 $\to$ $v$，这样得到，因此无需枚举 $D_{t-1}$ 中的点作为中继点。时间降为 $n^3/s_{t+1}s_ts_{t-2}$，得到 $\Ot(n^{2+1/(3k-1)})$。&lt;/p&gt;
&lt;p&gt;由于 $n^{1/\log_cn}=c$，故这个算法做到了 $\Ot(n^2)$ 的 $+\Theta(\log n)$ 近似。&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;deng-kirkpatrick-rong-williams-zhong-22&#34;&gt;[Deng, Kirkpatrick, Rong, Williams, Zhong &amp;lsquo;22]
&lt;/h4&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://littlereuben.github.io/study/paper4/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;这里&lt;/a&gt;讲过倍增优化，转化为 $(\min,+)$ 矩乘与稀疏图 APSP。关键在于，如果取出一棵生成树，然后将节点按欧拉序排列，只会将矩阵的大小 $\times 2$，但是，这时相邻数之差就变成 $\pm 1$ 了。根据 [Chi, Duan, Xie, Zhang &amp;lsquo;22]，这样的 $(\min,+)$ 矩乘可以做到 $\Ot(n^{(3+\omega)/2})$。&lt;/p&gt;
&lt;h4 id=&#34;berman-kasiviswanathan-07&#34;&gt;[Berman, Kasiviswanathan &amp;lsquo;07]
&lt;/h4&gt;&lt;p&gt;还是倍增优化，但这里把整个 $1\sim n$ 的区间全划分了。现在不跑 $(\min,+)$ 矩乘，考虑类似于有权 $\times 2$ 的思路：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;跑多源 BFS，找到离每个点最近的关键点。然后 $\delta(u,v)=\min\set{d(u,u^\prime)+d(u^\prime,v),d(u,v^\prime)+d(v^\prime,v)}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不妨设 $d(u,w)\le d(u,v)/2$。这里核心的不等式是：
$$
d(u,u^\prime)+d(u^\prime,v)\le 2d(u,u^\prime)+d(u,v)\le 2d(u,x)+d(u,v)\le 2d(u,v)+2
$$
但是由于建稀疏图实际上是保留所有度数 $&amp;lt;2^{i+1}$ 的点的邻边，因此 $\pi(u,v)$ 上的最大度点和次大度点的覆盖点，均可以作为中介。于是我们可以假设 $d(u,w)\le (d(u,v)-1)/2$，得到 $\le 2d(u,v)+1$。最差情况就是：&lt;/p&gt;
&lt;h3 id=&#34;堆&#34;&gt;堆
&lt;/h3&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;二叉堆&lt;/th&gt;
&lt;th&gt;二项堆&lt;/th&gt;
&lt;th&gt;配对堆&lt;/th&gt;
&lt;th&gt;左偏树&lt;/th&gt;
&lt;th&gt;斐波那契堆&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;插入&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)/\O(1)^\dagger$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;删除&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)^\dagger$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)^\dagger$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;查 $\min$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;删 $\min$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)^\dagger$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)^\dagger$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;合并&lt;/td&gt;
&lt;td&gt;$\O(n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(1)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;decrease key&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(\log n)^\dagger$&lt;/td&gt;
&lt;td&gt;$\O(\log n)$&lt;/td&gt;
&lt;td&gt;$\O(1)^\dagger$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;$\dagger$ 表示均摊。&lt;/p&gt;
&lt;p&gt;注：严格来说，配对堆的那几个 $\O(1)$ 也应当视作均摊。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www2.eecs.berkeley.edu/Pubs/TechRpts/1975/ERL-m-501.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Cheriton, Tarjan &amp;lsquo;76&lt;/a&gt;] MST $\O(m\log\log n)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://web.eecs.umich.edu/~pettie/matching/Fredman-Tarjan-Fibonacci-Heaps.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Fredman, Tarjan &amp;lsquo;87&lt;/a&gt;] 斐波那契堆、MST $\O(m\beta(m,n))$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;&#34; &gt;Dixon, Rauch, Tarjan &amp;lsquo;92&lt;/a&gt;] 验证 MST $\O(m)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/pii/S0022000005800649&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Fredman, Willard &amp;lsquo;92&lt;/a&gt;] 整数边权 MST $\O(m)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://cs.brown.edu/research/pubs/pdfs/1995/Karger-1995-RLT.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Karger, Klein, Tarjan &amp;lsquo;95&lt;/a&gt;] 非确定性算法 MST $\O(m)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/pdf/10.1145/316542.316548&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Thorup &amp;lsquo;99&lt;/a&gt;] 非负整数边权无向图 SSP $\O(m)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/pdf/10.1145/355541.355562&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Chazelle &amp;lsquo;00&lt;/a&gt;] 确定性算法 MST $\O(m\alpha(m,n))$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://web.eecs.umich.edu/~pettie/papers/jacm-optmsf.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Pettie, Ramachandran &amp;lsquo;02&lt;/a&gt;] 确定性算法 MST 时间等于决策树复杂度（但后者未知）&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2311.11793&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Haeupler, Hladík, Rozhoň, Tarjan, Tětek &amp;lsquo;23&lt;/a&gt;] Dijkstra 的 universal optimality&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2504.17033&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Duan, Mao, Mao, Shu, Yin &amp;lsquo;25&lt;/a&gt;] 不要求按距离顺序输出的非负权 SSP 确定性算法 $\O(m\log^{2/3}n)$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;二项堆&#34;&gt;二项堆
&lt;/h4&gt;&lt;p&gt;简而言之就是二进制分组的思想。定义 $B_k$ 为 $B_{k-1}$ 的根下面接一个 $B_{k-1}$，维护 $n$ 个数，即 $n$ 个节点的二项堆形如
$$
\bigcup_{i\in\mathrm{binary}(n)}B_i
$$
其中每个 $B_i$ 中满足堆的大小关系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并：考虑二进制加法的过程。如果要合并两个 $B_i$，就把根大的那个接到根小的儿子，变成一个 $B_{k+1}$。&lt;/li&gt;
&lt;li&gt;查 $\min$：扫描各根，当然也可以维护一个指针。&lt;/li&gt;
&lt;li&gt;删除：decrease 成 $-\infty$ 后删 $\min$。&lt;/li&gt;
&lt;li&gt;删 $\min$：拆开各个儿子，做一个形如 $(n-2^i)+(2^i-1)=n-1$ 的合并。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;斐波那契堆&#34;&gt;斐波那契堆
&lt;/h4&gt;&lt;p&gt;核心思想是弱化二项堆的结构限制，通过允许每个点少一个儿子，来允许 decrease key 直接扔到根。&lt;/p&gt;
&lt;p&gt;对于一个堆，维护它根的链表。对于每个非根点，如果它有一个儿子在 decrease key 时被切掉了，就标记它。记
$$
\Phi(H)=\text{\# roots} + 2\times\text{\# marked nodes}
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并：直接把链表接起来。&lt;/li&gt;
&lt;li&gt;查 $\min$：维护一个指针。&lt;/li&gt;
&lt;li&gt;decrease key：直接把这个点的子树扔到根链表。如果它的父亲没被标记过则标记，否则也扔到根链表，以此向上类推。势能函数设计得很巧妙，因此扔一次根，$\Phi$ 就减一，刚好抵掉。&lt;/li&gt;
&lt;li&gt;删 $\min$：拆开各个儿子扔到根链表，然后合并儿子数相同的根。这时 $\hat T=\text{\# sons}+\text{\# roots}+\Delta\Phi=\text{\# sons}+\text{new \# roots}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此要 bound 儿子数，也就是 bound $F(k)$ 表示根有 $k$ 个儿子，树的最小大小。考虑一个儿子刚接上当前点的时候，然后再去掉一个儿子：
$$
F(k)\ge F(k-2)+\cdots+F(0)+F(0)\implies F=1,2,3,5,8,13,\cdots
$$&lt;/p&gt;
&lt;h4 id=&#34;优化-mst&#34;&gt;优化 MST
&lt;/h4&gt;&lt;p&gt;尽管 Prim 和 Dijkstra 通过斐波那契堆都 $\O(m\log n)\to\O(m+n\log n)$ 了，但在 $m=\Theta(n)$ 时相当于没优化。考虑到严格来说复杂度是 $\O(m+n\log(\max|H|))$，因此有一个想法是，考虑强制 bound $|H|$：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选一个点出发做 Prim，如果堆的大小达到 $k$ 则停止。再从另一个点出发做 Prim，如果堆的大小达到 $k$，或者它这部分的树和已有的树连通了，就停止。对所有点都做上述过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;时间为 $\O(m+n\log k)$。现在，相当于选出了一些属于 MST 的边，构成森林。将各森林合并为单点后递归做（这个思想和 Borůvka 一样）。由于每个连通块最初都必须由 $\ge k$ 个邻边的情形形成，故合并后的点数 $\le 2m/k$。因此：
$$
T(n,m)\le T(2m/k,m)+\O(m+n\log k)
$$
取 $n\log k=\Theta(m)$，具体而言取 $k=2^{2m/n}$。这时相当于要解
$$
f(n)=f(2m/2^{2m/n})+1
$$
换元。令 $r=2m/n$，$g(r)=f(n)$，则
$$
g(r)=f(2m/r)=f(2m/2^r)+1=g(2^r)+1=\cdots=\Theta(\log^*m)
$$
因此我们得到一个 $\O(m\log^*m)$ 的做法。严格来说应该是 $\O(m\beta(m,n))$，因为是从 $r=2m/n$ 开始，当 $k\ge n$ 就结束了。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://tmt514.github.io/algorithm-analysis/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://tmt514.github.io/algorithm-analysis/index.html&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;并查集&#34;&gt;并查集
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/pdf/10.1145/321879.321884&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Tarjan &amp;lsquo;75&lt;/a&gt;] 路径压缩 + 按秩合并的 $\Theta(m\alpha(m,n))$ 上下界&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/pdf/10.1145/73007.73040&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Fredman, Saks &amp;lsquo;89&lt;/a&gt;] 等价类问题的 $\Omega(\alpha(m,n))$ word operation 下界&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;明确一下，$rk_u$ 表示 $u$ 的子树在未经路径压缩时的高度。&lt;/p&gt;
&lt;h4 id=&#34;分析-1&#34;&gt;分析 1
&lt;/h4&gt;&lt;p&gt;令 $\delta_u=rk_{fa_u}-rk_u$。每次 &lt;code&gt;find&lt;/code&gt; 操作中每个点 $u$ 有两种情况：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$2\delta_u^\prime\ge\delta_u$。由于 $rk_u=\O(\log n)$，这种情况对于每个 $u$ 只会出现 $\O(\log\log n)$ 次。&lt;/li&gt;
&lt;li&gt;$2\delta_u^\prime&amp;lt;\delta_u$。然而，$\delta^\prime_{fa_u}=\delta^\prime_u-\delta_u&amp;lt;\delta^\prime_u/2$，因此这种情况对于一次 &lt;code&gt;find&lt;/code&gt; 只会出现 $\O(\log\log n)$ 次。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;得到 $\O((n+m)\log\log n)$ 的复杂度。&lt;/p&gt;
&lt;h4 id=&#34;分析-2&#34;&gt;分析 2
&lt;/h4&gt;&lt;p&gt;考虑不路径压缩的最终合并树。对于路径压缩，我们从 $u$ 到当时的根连接 shortcut 边，这样形成 shortcut 图。&lt;/p&gt;
&lt;p&gt;令 $T(m,n,r)$ 表示共 $m$ 次操作，$n$ 个点，只考虑 $rk\le r$ 的那些点以及根在这些点内的 &lt;code&gt;find&lt;/code&gt;，总的时间。这里的参数 $r$ 是为了递归子问题用的。&lt;/p&gt;
&lt;p&gt;现在我们划一道线，分割 $rk\le s$ 和 $&amp;gt;s$ 的点。对于一次 &lt;code&gt;find&lt;/code&gt; 中将 $x$ 的父亲置为当前根 $y$ 的操作，分类讨论：&lt;/p&gt;
&lt;p&gt;令 $m_+$ 表示 $rk_y&amp;gt;s$ 的 &lt;code&gt;find&lt;/code&gt; 次数，$m_-=m-m_+$。3.1 每个 $x$ 只出现一次，3.2 每个 &lt;code&gt;find&lt;/code&gt; 只出现一次。
$$
T(m,n,r)\le T(m_-,n,s)+\boxed{T(m_+,n/2^s,r)}+m_++n
$$
已知 $T(m,n,r)\le m+nr$。
$$
T(m,n,r)\le T(m_-,n,s)+\frac{n}{2^s}r+2m_++n
$$
取 $s=\log r$。
$$
\begin{align*}
T(m,n,r)-2m&amp;amp;\le T(m_-,n,s)-2m_-+2n\\
&amp;amp;\le \cdots\\
&amp;amp;\le 2n\log^*n
\end{align*}
$$
因此 $T(m,n,r)\le 2m+2n\log^*n$，将这个代回 $\boxed{}$ 得
$$
T(m,n,r)\le T(m_-,n,s)+2m_++2\frac{n}{2^s}r\log^*n+m_++n
$$
取 $s=\log^*r$：
$$
T(m,n,r)-3m\le T(m_-,n,s)-3m_-+3n\le\cdots\le 3n\log^{**}n
$$
以此类推，得到 $T(m,n,r)\le (m+n)\alpha(n)$，其中 $\alpha(n)=\min\set{c\mid\log^{\overbrace{**\cdots*}^c}n\le 3}$。&lt;/p&gt;
&lt;h4 id=&#34;分析-3&#34;&gt;分析 3
&lt;/h4&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://codeforces.com/blog/entry/98275&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;《我的并查集启蒙》&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇文章的大致想法是，划定从密到疏的 $0\sim z-1$ 级的 $rk$ 线，对于跨 $i$ 级线且不跨 $i+1$ 级线的边 $x-fa_x$，有两种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x-fa_x$ 是该次 &lt;code&gt;find&lt;/code&gt; 中最后一条这样的边：每次 &lt;code&gt;find&lt;/code&gt; 只有一条。（对应 3.2）&lt;/li&gt;
&lt;li&gt;$x-fa_x$ 不是：它会多跨至少一条 $i$ 级线，最终会跨 $i+1$ 级线。（对应 3.1）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;它们对应的总复杂度分别是 $mz$ 和 $\sum_i\sum_js_{i,j}c_{i,j}$，其中 $a/s/c_{i,j}$ 分别表示第 $j$ 条 $i$ 级线的划分位置/和下一条 $i$ 级线间的点数/到上一条 $i+1$ 级线所需的 $i$ 级线数。注意到 $s_{i,j}=\O(n/2^{a_{i,j}})$，因此就变成
$$
\sum_i\sum_j\frac{n}{2^{a_{i,j}}}c_{i,j}
$$
取 $c_{i,j}=a_{i,j}$，就能做到 $nz$，同时有了递推式 $a_{i+1,j}=a_{i,a_{i,j-1}}$。&lt;/p&gt;
&lt;p&gt;容易发现，第 $i$ 级线就（不完全）对应着上一个分析中第 $i$ 次代入分析时划的 $s$ 值。如果限制 $z$ 为常数，那么最后一层会有一个 $nc_z=n\log^{**\cdots*}n$，在上一个分析中就对应递推式末尾的 $+n$；剩余层的 $nc_{i,j}/2^{a_{i,j}}$ 都是被调参抵消。&lt;/p&gt;
&lt;h4 id=&#34;一些细节&#34;&gt;一些细节
&lt;/h4&gt;&lt;p&gt;定义 Ackermann 函数：
$$
\Alpha(m,n)=\begin{cases}n+1,&amp;amp;m=0\\  \Alpha(m-1,1),&amp;amp;n=0\\  \Alpha(m-1,\Alpha(m,n-1)),&amp;amp;\text{otherwise}\end{cases}
$$
定义 $\alpha(n)=\min\set{k\mid A(k,1)\ge n}$……等等，怎么有两个定义啊？&lt;/p&gt;
&lt;p&gt;首先我们可以归纳证明：
$$
\Alpha(m,n)=\begin{cases}n+1,&amp;amp;m=0\\ n+2,&amp;amp;m=1\\ 2\uparrow^{m-2}(n+3)-3,&amp;amp;m\ne 0\end{cases}
$$
其中
$$
a\uparrow^kb=\underbrace{a\uparrow^{k-1}(a\uparrow^{k-1}(\cdots \uparrow^{k-1}a))}_{(b-1)\times\uparrow^{k-1}}
$$
而我们可以定义 $\log^{**\cdots*}$ 的反函数，例如 $\log^*n=\min\set{k\mid \underbrace{2^{2^{\cdot^{\cdot^{\cdot^{2}}}}}}_k=2\uparrow^2k\ge n}$。而
$$
\log^{**}n=\min\set{k\mid \left.\underbrace{2^{2^{\cdot^{\cdot^{\cdot^{2}}}}}}_{\underbrace{2^{2^{\cdot^{\cdot^{\cdot^{2}}}}}}_{\underbrace{\vdots}_{2}}}\right\}{(k-1)\;\underbrace{}}=2\uparrow^3k\ge n}
$$
以此类推。值得注意的是，$\log^{**\cdots*}n$ 在 $n=1,2,3,4,5$ 时总是等于 $0,1,2,2,3$，因为 $2\uparrow^{k}2\equiv 4$。这就是为什么前一个定义里要求 $\log^{**\cdots*}n\ge 3$。那么现在，Ackermann 函数和 $\log^{**\cdots*}$ 反函数的关系很明显了，因此两种 $\alpha$ 的定义只差一个常数。&lt;/p&gt;
&lt;h4 id=&#34;alphan-下界构造-tarjan-75&#34;&gt;$\alpha(n)$ 下界构造 [Tarjan &amp;lsquo;75]
&lt;/h4&gt;&lt;p&gt;对于一棵树 $T$，定义 $T(i)$ 为 $T$ 的 $i$ 级二项树：$T(0)=T$，$T(n)=T(n-1)$ 的根上挂一个 $T(n-1)$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若 $T$ 有 $\ge 2$ 个节点和 $s$ 个叶子，那么 $T(\Alpha(4k,4s))$ 存在一半的叶子可以做长度为 $k$ 的 &lt;code&gt;find&lt;/code&gt; 操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于 $k\le 2$ 这是显然的。现在对于 $k\ge 3$ 作归纳。先考虑 $T$ 只有两个节点，或者说 $s=1$ 的情况：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-galgo/3-2.png&#34;
	width=&#34;954&#34;
	height=&#34;351&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;271&#34;
		data-flex-basis=&#34;652px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;橙色节点的 $3/4$。
&lt;ol&gt;
&lt;li&gt;把所有叶子扔了，根据归纳假设，$T(\Alpha(4k-4,4)+1)$ 中有一半的紫色节点可以做长度为 $k-1$ 的 &lt;code&gt;find&lt;/code&gt; 操作，因此有一半的橙色节点可以做长度为 $k$ 的 &lt;code&gt;find&lt;/code&gt; 操作。&lt;/li&gt;
&lt;li&gt;在 $T(\Alpha(4k-4,4)+1)$ 中，剩余的一半紫色节点以及它们的祖先形成有 $2^{\Alpha(4k-4,4)-1}$ 个叶子的树。根据归纳假设，$T(\Alpha(4k-4,4)+1+\Alpha(4k-4,2^{\Alpha(4k-k,4)+1}))$ 中，这剩余的一半紫色节点中有一半可以做长度为 $k-1$ 的 &lt;code&gt;find&lt;/code&gt; 操作，因此有额外 $1/4$ 的橙色节点可以做长度为 $k$ 的 &lt;code&gt;find&lt;/code&gt; 操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;蓝色节点的 $1/4$。令 $p=\Alpha(4k-4,4)+1+\Alpha(4k-4,2^{\Alpha(4k-k,4)+1})$。在 $T(p)$ 中，只保留绿色节点，那么有 $2^{p-2}$ 个叶子。根据归纳假设，$T(p+\Alpha(4k-4,2^p))$ 中这些绿色节点叶子中有一半可以做长度为 $k-1$ 的 &lt;code&gt;find&lt;/code&gt; 操作，因此有 $1/4$ 的蓝色节点可以做长度为 $k$ 的 &lt;code&gt;find&lt;/code&gt; 操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;于是只需证明
$$
1+\Alpha(4k-4,4)+\Alpha(4k-4,2^{\Alpha(4k-k,4)+1})+\Alpha(4k-4,2^{1+\Alpha(4k-4,4)+\Alpha(4k-4,2^{\Alpha(4k-k,4)+1})})\le\Alpha(4k,4)
$$
注意，当我们说“有……的节点可以做长度为 $k$ 的 &lt;code&gt;find&lt;/code&gt; 操作”时，就是指直接做 &lt;code&gt;find&lt;/code&gt; 了。这时树的结构会被路径压缩改变，因此在最初的命题中，我们说的 $T$ 中可能存在 shortcut 边。同时，尽管橙色节点上的 &lt;code&gt;find&lt;/code&gt; 会影响绿色节点的结构，但是叶子经过路径压缩还是叶子，没有祖先后代关系的点对经过路径压缩还是没有祖先后代关系，因此不影响 2，以及下面的论证。&lt;/p&gt;
&lt;p&gt;如果 $s&amp;gt;1$，先扔掉一个叶子，做归纳。现在在 $T(\Alpha(4k,4s-4))$ 中已经做了一堆 &lt;code&gt;find&lt;/code&gt; 了，但是这类扔掉的叶子及其父亲（最高 shortcut 边祖先）仍然能按上面一个叶子的情况分类并构造，最后需要证明
$$
\begin{align*}
&amp;amp;\Alpha(4k,4s-4)\\
+{}&amp;amp;\Alpha(4k-4,2^{\Alpha(4k,4s-4)+1})\\
+{}&amp;amp;\Alpha(4k-4,2^{\Alpha(4k,4s-4)+\Alpha(4k-4,2^{\Alpha(4k,4s-4)+1})})\\
+{}&amp;amp;\Alpha(4k-4,2^{\Alpha(4k,4s-4)+\Alpha(4k-4,2^{\Alpha(4k,4s-4)+1})+\Alpha(4k-4,2^{\Alpha(4k,4s-4)+\Alpha(4k-4,2^{\Alpha(4k,4s-4)+1})})})\\
\le{}&amp;amp;\Alpha(4k,4s)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-galgo/3-3.png&#34;
	width=&#34;3610&#34;
	height=&#34;1460&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;这里红(shortcut)边不一定是连到根的&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;247&#34;
		data-flex-basis=&#34;593px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;这样就证明了 $\Omega(m\alpha(n))$ 的下界。&lt;/p&gt;
&lt;h3 id=&#34;splay--lct&#34;&gt;Splay &amp;amp; LCT
&lt;/h3&gt;&lt;p&gt;略。&lt;/p&gt;
&lt;h3 id=&#34;ett&#34;&gt;ETT
&lt;/h3&gt;&lt;p&gt;考虑用平衡树维护树的欧拉序。欧拉序本质上是一个环，重定根操作相当于循环位移。具体来说，用平衡树维护欧拉序，如果将根定为 $u$，只需找到序列中的任意一个 $u$，将它之前的部分移到结尾（再去掉一个重复的原来根，加一个 $u$，这个只是维护环的小细节）即可。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;link(u,v)&lt;/code&gt;：分别 &lt;code&gt;makeroot(u), makeroot(v)&lt;/code&gt;，然后把欧拉序拼起来。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cut(u,v)&lt;/code&gt;：&lt;code&gt;makeroot(u)&lt;/code&gt; 之后把序列中 &lt;code&gt;u, [v, ..., v], u&lt;/code&gt; 这部分分离出来。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实现时需要维护，每个节点对应的（任意）一个平衡树中节点，以及每条边在平衡树中的位置。一个实现方法是在每个点上加一个自环，然后序列中记录每次走的边。这个的问题是，似乎动态序列中查某个数第一次出现的位置是没法做的，因此除非给定所查点的父亲，不然没法求子树信息。链信息更是不用想了。如果只维护 dfs 序的话，只能做根连父亲的动态操作。&lt;/p&gt;
&lt;h3 id=&#34;动态图连通性&#34;&gt;动态图连通性
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cs.princeton.edu/courses/archive/fall03/cs528/handouts/Poly%20logarithmic.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;[Holm, Lichtenberg, Thorup &amp;lsquo;98]&lt;/a&gt; （都是均摊）完全动态图连通性 $\O(\log^2n)$，只删不增动态 MST $\O(\log^2n)$，完全动态 MST $\O(\log^4n)$，完全动态边双 $\O(\log^4n)$，完全动态点双 $\O(\log^4n)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1609.05867&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Huang, Huang, Kopelowitz, Pettie, Thorup &amp;lsquo;16&lt;/a&gt;] 非确定性完全动态图连通性均摊 $\O(\log n(\log\log n)^2)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://epubs.siam.org/doi/epdf/10.1137/1.9781611973105.81&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Kapron, King, Mountjoy &amp;lsquo;13&lt;/a&gt;] 非确定性完全动态图连通性最坏 $\O(\log^5n)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/pdf/10.1145/265910.265914&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Eppstein, Galil, Italiano, Nissenzweig &amp;lsquo;92&lt;/a&gt;] 确定性完全动态图连通性最坏 $\O(\sqrt n)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://erikdemaine.org/papers/DynamicConnectivity_STOC2004/paper.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Pătrașcu, Demaine &amp;lsquo;04&lt;/a&gt;] 完全动态图连通性均摊 $\Omega(\log n)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/0808.1128&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Chan, Pătrașcu, Roditty &amp;lsquo;08&lt;/a&gt;] 完全动态子图连通性均摊 $\Ot(m^{2/3})/\Ot(m^{1/3})$&lt;/li&gt;
&lt;li&gt;[Duan &amp;lsquo;10] 完全动态子图连通性均摊 $\Ot(m^{4/5})/\Ot(m^{1/5})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1611.09072&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Duan, Zhang &amp;lsquo;10&lt;/a&gt;] 完全动态子图连通性均摊 $\Ot(m^{3/4})/\Ot(m^{1/4})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1511.06773&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Henzinger, Krinninger, Nanongkai, Saranurak &amp;lsquo;15&lt;/a&gt;] 在 OMv hypothesis 下完全动态子图连通性的改查之积无法 $\omicron(m)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2202.11250&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Jin, Xu &amp;lsquo;22&lt;/a&gt;] 在 Combinatorial k-Clique hypothesis 完全动态子图连通性无法做到改 $\O(m^{2/3-\eps})$，查 $\O(m^{1-\eps})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;代码实现：&lt;a class=&#34;link&#34; href=&#34;https://etaoinwu.com/blog/fully-dynamic-connectivity/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://etaoinwu.com/blog/fully-dynamic-connectivity/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;离线的话直接线段树分治 + 并查集就行。在线的情况我们初步的想法是，维护生成森林：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;add(u,v)&lt;/code&gt;：如果 $u$ 和 $v$ 处于不同连通块，则加边；否则不管。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;del(u,v)&lt;/code&gt;：如果 $(u,v)$ 是树边，需要找一条非树边把两侧连起来。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;增删边连通性-holm-lichtenberg-thorup-98&#34;&gt;增删边连通性 [Holm, Lichtenberg, Thorup &amp;lsquo;98]
&lt;/h4&gt;&lt;p&gt;这个算法我觉得可以从两个角度理解：一个是离线线段树分治算法的惰性划分，一个是启发式分裂。前者可以马上帮你理解为什么莫名其妙要给边分层，不过可能还是后者更本质一些吧。就是很多时候优化的算法不是完全新想一个方式，而只是去控制暴力算法所运行的局部的 size。如果暴力算法的运行时间是线性的，那么优化之后就是 $n\operatorname{polylog}(n)$；如果是平方之类涉及“每一对 pair 的枚举”，那么优化之后就是 $n$ 的某个 $1\sim 2$ 之间的指数。&lt;/p&gt;
&lt;p&gt;给每条边赋权 $\ell:E\to\N$，令 $E_i=\set{e\mid \ell(e)\ge i}$，我们再对于每个 $E_i$ 维护生成森林 $F_i$，这样就有&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-galgo/5-0.png&#34;
	width=&#34;2235&#34;
	height=&#34;861&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;259&#34;
		data-flex-basis=&#34;622px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;注意这里我们要求 $F_0\supseteq F_1\supseteq\cdots$，那就有个很简单的推论：一条边 $e$ 在 $0\sim \ell(e)$ 层，要么一直是树边，要么一直是非树边。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;要求 $F_i$ 中，每棵树的大小 $\le n/2^i$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，$\ell_{\max}&amp;lt;\log n$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;要求一条边的 $\ell$ 不降。且在它被枚举到时，就向上一层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样，总的枚举次数就是 $\O(m\log n)$，我们得到一个均摊的复杂度。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;5-1.png&#34; style=&#34;zoom:20%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;接下来，我们用 ETT 维护每个 $F_i$，时间就是 $\O(m\log^2n)$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;add(u,v)&lt;/code&gt;：初始化 $\ell(e)=0$，这时只出现在 $E_0$ 中，如果 $E_0$ 中两端不连通，则作为树边加入 $F_0$。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;del(u,v)&lt;/code&gt;：如果 $e$ 是树边，那就要将它从 $F_{0\sim\ell(e)}$ 中删去，然后需要以一种优化的方法找到新的非树边。调用 &lt;code&gt;reconnect(e,ℓ(e))&lt;/code&gt;。&lt;code&gt;reconnect(e,ℓ)&lt;/code&gt;：
&lt;ul&gt;
&lt;li&gt;现在 $e$ 的两端在 $\ell$ 这层是两个连通块 $T_u$ 和 $T_v$，我们希望找到一条 $\ell$ 层的非树边，将两侧连起来。&lt;/li&gt;
&lt;li&gt;注意 $\ell+1$ 层及以上不可能存在非树边连通两者，初始情况是由于 $F_{\ell(e)+1}\subset F_{\ell(e)}\setminus\set{e}$，后续可以归纳。&lt;/li&gt;
&lt;li&gt;现在取两个连通块中较小的，不妨设为 $T_u$，枚举一端在 $T_u$ 内的 $\ell$ 层边。这样的边，要么另一端也在 $T_u$ 内，这样就提升到 $\ell+1$ 层；否则另一端必然在 $T_v$ 中，那么就将它加到 $F_{0\sim\ell}$ 中。如果全扫完了还没找到，则调用 &lt;code&gt;reconnect(e,l-1)&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;当提升到 $\ell+1$ 层时，原先在 $\ell$ 层的树边加到 $F_{\ell+1}$，非树边就不用管。&lt;/li&gt;
&lt;li&gt;关键实现细节：为了枚举 $T_u$ 内的 $\ell$ 层边，我们不能枚举 $T_u$ 内的点，因为 $\ell$ 层边数可能小于点数。同时我们也无法对于每个连通块维护它的非树边，因为涉及到分裂和合并。而且为了维持 $F_{\ell}\supseteq F_{\ell+1}$，$\ell$ 层树边必须提升。一种实现是，对于每个点维护它的 $\ell$ 层邻边，同时在 ETT 中维护子树信息：当前平衡树子树中是否存在点有 $\ell$ 层邻边（注意对于同一个点，存在邻边的标记只能挂在其中一个平衡树节点上）。这样做的话，把 $T_u$ 中的所有 $\ell$ 层边用一个 dfs 拿出来会比较方便，于是可以先把它们全部提升到 $\ell+1$ 层（注意讨论树边和非树边）。这样做会在原先“提升 $m\log n$ 条边”的基础上多乘个 $\log$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$F_i$ 性质的维持：选的是 $\le$ 原连通块一半大小的连通块。&lt;/p&gt;
&lt;p&gt;查询可以进一步优化到 $\O(\log n/\log\log n)$。&lt;/p&gt;
&lt;h4 id=&#34;删边-mst-holm-lichtenberg-thorup-98&#34;&gt;删边 MST [Holm, Lichtenberg, Thorup &amp;lsquo;98]
&lt;/h4&gt;&lt;p&gt;还是维护上面那个结构，初始所有边都在 $0$ 级，$F_0$ 存 MST。每次删边，如果 $e$ 是树边，则删去后调用 &lt;code&gt;reconnect(e,ℓ(e))&lt;/code&gt;。这里唯一不同的是，&lt;code&gt;reconnect&lt;/code&gt; 中要按边权从小到大枚举在 $\ell$ 层的边。&lt;/p&gt;
&lt;p&gt;我们本质上是要找 $T_u$ 和 $T_v$ 之间的 cut，即之间边权最小的边。这里的核心问题是：凭什么边权最小的边不会出现在 $&amp;lt;\ell$ 层？性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;原边集中任意一个环，它的最大边一定在最低层。如果有 tie，表述可以修改为：存在一条最大边在最低层。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;于是通过这张图就可以理解：&lt;/p&gt;
&lt;p&gt;该性质的维持：对于一个环 $C$ 中的一条最大权最低层边 $e$，如果它被提升一层到 $\ell(e)+1$，这是 $C$ 的所有原 $\ell(e)$ 层边一定在之前已被枚举到且提升了，因为 $C$ 的所有点都在 $T_u$ 中（根据归纳假设，以及 $F_{\ell}$ 的定义）。&lt;/p&gt;
&lt;p&gt;实现：在 ETT 中维护子树中所有点的 $\ell$ 层邻边的最小权，每次取出一条。注意这时由于 MST 的性质，如果所有边权都不同的话，是不用担心非树边升层而树边未升导致 $F_{\ell}\supseteq F_{\ell+1}$ 性质未维持的，因此不用显式提升树边。只要在边权相同 break tie 时优先选树边就行。&lt;/p&gt;
&lt;p&gt;如果有加边的话，很简单，我们想如果加了一条边破圈了，那新边的 $\ell$ 一定得 $\ge$ 被删边的 $\ell$，那如果这样加进去违反了 $\le n/2^i$ 的要求，就无解了。&lt;/p&gt;
&lt;h4 id=&#34;增删点连通性-chan-pătrașcu-roditty-08&#34;&gt;增删点连通性 [Chan, Pătrașcu, Roditty &amp;lsquo;08]
&lt;/h4&gt;&lt;p&gt;这个问题是说，固定图的形态，点有 on 和 off 两种状态，我们只考虑亮的点的导出子图。&lt;/p&gt;
&lt;p&gt;算法的直觉是，考虑调用上面的算法，既然暴力加删边会由于反复切换大度点被卡，那我们首先考虑以 $\Delta$ 的间隔定期重构，只维护这段时间内被操作的点集 $Q$，这样点数限制在 $\Delta$ 以内。这时影响连通性的最讨厌的问题是形如 $Q-(V\setminus Q)-Q$ 的连边结构。对于一个 $V\setminus Q$ 中的连通块 $C$，处理它的时间是 $\deg(C)^2$。于是我们又考虑按 $\deg(C)$ 的大小分治，把大度的连通块缩起来。总而言之，最后图会被分成三部分，我们考虑它们互相影响的 $5$ 种关系，就会平衡到 $m^{2/3}$。&lt;/p&gt;
&lt;p&gt;维护两个集合 $P$ 和 $Q$，其中初始所有点都在 $P$，$P$ 只支持删点；$Q$ 支持加删点。每 $m^{2/3}$ 次操作后重构，将所有点放回 $P$，于是 $|Q|\le m^{2/3}$。&lt;/p&gt;
&lt;p&gt;将 $P$ 对应的导出子图的连通块中，（在原图中）度数之和 $&amp;gt;m^{1/3}$ 的称为 high component，其余称为 low component。&lt;/p&gt;
&lt;p&gt;维护图 $G^*$，包含所有 $Q$ 的点以及 $P$ 中各个 high 各自缩成一个点，这样 $|V^*|\le m^{2/3}$。&lt;/p&gt;
&lt;p&gt;再维护
$$
V\times V\supseteq\Gamma=\set{(u,v)\mid \exists\text{ low component adjacent to both }u\text{ and }v}
$$
这样 $|\Gamma|\le\sum_{\text{low }C}(\sum_{u\in C}\deg_u)^2\le m^{4/3}$。现在 $G^*$ 中加三类边：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$Q$ 内的边&lt;/li&gt;
&lt;li&gt;$Q$ 和 high 之间的边（要记一个重边计数表）&lt;/li&gt;
&lt;li&gt;$\Gamma$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样 $G^*$ 中的连通性与原图中的连通性等价。用前面的算法维护 $G^*$，一张 $\O(m^{2/3})$ 点 $\O(m^{4/3})$ 边的图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-galgo/5-3.png&#34;
	width=&#34;1442&#34;
	height=&#34;650&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;221&#34;
		data-flex-basis=&#34;532px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;qry(u,v)&lt;/code&gt;：如果两个点都在 $G^*$ 里就直接查。如果一个点在 low 里，就枚举和这个 low 相邻的 $G^*$ 中点。如果两个都在 low 里，就先枚举一个的邻点在 $G^*$ 的 $F_0$ 的 ETT 里标记，然后再拿另一个查。这样就是 $\Ot(m^{1/3})$ 的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;add(u)&lt;/code&gt;：一定有 $u\in Q$，直接在 $G^*$ 里暴力加边。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;del(u)&lt;/code&gt;：
&lt;ul&gt;
&lt;li&gt;$u\in Q$：暴力删边。&lt;/li&gt;
&lt;li&gt;$u\in$ high：假设分裂为连通块 $R_1,\cdots,R_k$，按度数和从大到小排列。除 $R_1$ 以外的 high，扫描各邻边，更新 $G^*$ 的边集；变为 low 的部分，更新 $\Gamma$。这样的复杂度类似于 $\deg(R_2)+\cdots+\deg(R_j)+\deg(R_{j+1})^2+\cdots+\deg(R_k)^2$，前一部分根据启发式分裂，每轮重构是 $\Ot(m)$ 的；后一部分每轮只会有一次从 high 到 low 的情况，因此是 $m^{2/3}$ 的。关键就是别主动扫 $R_1$ 的邻边，而要在扫 $R_2\sim R_k$ 时撤掉和 $R_1$ 没边的 $G^*$ 中点。&lt;/li&gt;
&lt;li&gt;$u\in$ low：暴力更新 $\Gamma$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意还要维护 $P$ 中的每个点属于哪个连通块，这个也是随便做。每轮重构会引入 $m$ 条边，分 $m^{2/3}$ 次删去，因此均摊 $m^{1/3}$，不是瓶颈。&lt;/p&gt;
&lt;p&gt;总的来说，每轮重构所需的时空都是 $\Ot(m^{4/3})$，每次查询 $\Ot(m^{1/3})$，每次修改均摊 $\Ot(m^{2/3})$。空间可以优化到 $\O(m)$。&lt;/p&gt;
&lt;h3 id=&#34;网络流&#34;&gt;网络流
&lt;/h3&gt;&lt;h4 id=&#34;capacity-scaling&#34;&gt;Capacity Scaling
&lt;/h4&gt;&lt;p&gt;有两种实现。一种是，只取残量网络中容量 $\ge 2^i$ 的边跑；另一种是，在容量为 $\lfloor\frac{c}{2^i}\rfloor$ 的图中跑完后把容量和流量都 $\times 2$ 再加上当前二进制位。这两种每轮都至多增广 $m$ 次，所以是 $\O(m^2\log U)$ 的。&lt;/p&gt;
&lt;h4 id=&#34;dinic&#34;&gt;Dinic
&lt;/h4&gt;&lt;p&gt;注意复杂度的严格证明：每次流满一条边或到不了汇，都会导致当前弧更新。更新至多 $m$ 次，每次递归至多 $n$ 层。&lt;/p&gt;
&lt;p&gt;LCT 优化：维护以汇为根的一棵内向树，每次 &lt;code&gt;split(s,t)&lt;/code&gt; 找到 $\min$ 并全体减，删掉瓶颈边。这时如果源只能到 $u$ 了，就再找一条 $u$ 的出边加进来。如果 $u$ 的出边都用光了就删 $u$，再看 $u$ 的前驱。这样一轮就从 $\O(mn)$ 降到了 $\O(m\log n)$。&lt;/p&gt;
&lt;h4 id=&#34;单位容量图-dinic-复杂度&#34;&gt;单位容量图 Dinic 复杂度
&lt;/h4&gt;&lt;p&gt;不能有重边，所以最大流 $&amp;lt;n$。&lt;/p&gt;
&lt;p&gt;在 $n^{2/3}$ 轮后，$d(s,t)&amp;gt;n^{2/3}$，根据鸽巢存在相邻两层，点数之和 $\le 2n^{1/3}$，因此这两层之间的边数为 $\O(n^{2/3})$。这时我们就找到了残量网络上一个容量为 $\O(n^{2/3})$ 的割，因为根据 bfs 的性质不会有跨越 $\ge 2$ 层的正向边。&lt;/p&gt;
&lt;p&gt;在 $m^{1/2}$ 轮后，根据鸽巢存在相邻两层之间的边 $\ge m^{1/2}$，同样得到一个割。由于单位容量图是流一条边就没一条边，故一轮为 $\O(m)$，于是对于单位容量图有了一个 $\O(m\min\set{n^{2/3},m^{1/2}})$ 的算法，结合 capacity scaling 得到一个 $\O(m\min\set{n^{2/3},m^{1/2}}\log U)$ 的算法。&lt;/p&gt;
&lt;p&gt;对于二分图，在 $n^{1/2}$ 轮后，每条增广路长度都 $&amp;gt;n^{1/2}$。二分图的性质导致一个点不可能被两条流流过，换句话说流可以分解成点不交的路径。于是剩余至多增广 $n^{1/2}$ 轮，复杂度就是 $\O(m\sqrt n)$。&lt;/p&gt;
&lt;h4 id=&#34;无向单位容量图&#34;&gt;无向单位容量图
&lt;/h4&gt;&lt;p&gt;我们有 LCT 优化 Dinic，又有鸽巢 bound 流量，现在考虑结合起来。但是问题是，单位容量图里 Dinic 一轮已经是 $\O(m)$ 了，还能咋优化？所以考虑退回到 EK。&lt;/p&gt;
&lt;p&gt;对于无向图，找路径相当于动态图连通性问题，而我们可以说明无向单位容量网络流中有流（即有流）的边不多：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无环流至多用 $\O(n^{3/2})$ 条边。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设残量网络上 $s$ 到 $t$ 的最短路长为 $d$，则（和上面的说法一样）可流的流量不超过 $(n/d)^2$。换句话说，如果剩余流量为 $v$，那么 $s$ 到 $t$ 的最短路不超过 $n/\sqrt v$。这就说明，以 EK 的方式去增广，流经的边数不超过
$$
\frac{n}{\sqrt n}+\frac{n}{\sqrt{n-1}}+\cdots+\frac{n}{\sqrt1}\le \int_0^nx^{-1/2}\mathrm{d}x=\O(n^{3/2})
$$
我们知道 EK 不会产生环流，如果有另一个边更多的最大流可以在上面跑 EK 然后对称差搞出环。因此命题得证。&lt;/p&gt;
&lt;p&gt;现在的想法是，如果每次只流 $1$ 的流量，那么没必要考虑全部无向边，只需要取一个生成森林即可。于是 bfs 的边数就是 $\O(n^{3/2})$。增广时从动态图连通性的那个结构里取边就行。复杂度瓶颈在 bfs，总时间 $\O(n^{5/2})$。&lt;/p&gt;
&lt;h3 id=&#34;最小割与-k-连通性&#34;&gt;最小割与 $k$-连通性
&lt;/h3&gt;&lt;p&gt;以下 Karger 的都是非确定性算法。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://people.csail.mit.edu/karger/Papers/mincut.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Karger &amp;lsquo;92&lt;/a&gt;] 无向图全局最小割 $\Ot(mn^2)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://people.csail.mit.edu/karger/Papers/fastcut.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Karger, Stein &amp;lsquo;93&lt;/a&gt;] 无向图全局最小割 $\Ot(n^2)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://people.csail.mit.edu/karger/Papers/approxcut.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Karger &amp;lsquo;93&lt;/a&gt;] 无向图全局近似最小割常数近似比 $\Ot(m)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://people.csail.mit.edu/karger/Papers/stcut.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Benczúr, Karger &amp;lsquo;96&lt;/a&gt;] 无向图有源汇近似最小割常数近似比 $\Ot(n^2)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://people.csail.mit.edu/karger/Papers/lincut.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Karger &amp;lsquo;98&lt;/a&gt;] 无向图全局最小割 $\Ot(m)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://people.csail.mit.edu/karger/Papers/resflow.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Karger, Levine &amp;lsquo;02&lt;/a&gt;] 无向图最大流 $\Ot(m+nv)$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://link.springer.com/article/10.1007/BF01758778&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Nagamochi, Ibaraki &amp;lsquo;92&lt;/a&gt;] sparse $k$-connectivity (both edge &amp;amp; vertex connectivity) certificate $\Ot(m)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finding the edge connectivity：&lt;a class=&#34;link&#34; href=&#34;https://www.sciencedirect.com/science/article/pii/S0022000085710227&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.sciencedirect.com/science/article/pii/S0022000085710227&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;karger-算法&#34;&gt;Karger 算法
&lt;/h4&gt;&lt;p&gt;适用于无向有权图。&lt;/p&gt;
&lt;p&gt;考虑以下神秘算法：每次（按权重）随机选一条边，把两端点合并。保留重边。剩下两个点的时候就得到一个割。&lt;/p&gt;
&lt;p&gt;设最小割为 $c$，如果最小割上的边都没被选到，则该算法正确。每个点的度数都 $\ge c$，因此边数 $\ge cn/2$，这是随机选一条边未选到割中的边的概率为
$$
\frac{\frac{cn}{2}-c}{\frac{cn}2}=\frac{n-2}n
$$
因此正确概率至少为
$$
\frac{n-2}{n}\times\frac{n-3}{n-1}\times\frac{n-4}{n-2}\times\cdots\times\frac13=\frac{2}{n(n-1)}
$$
不牛。&lt;/p&gt;
&lt;p&gt;接下来是很精妙的一步：考虑到，如果合并到中途停下来，最小割仍有很大概率是还在的。因此考虑合并到一定时候，再多轮随机。进一步递归这一想法，我们就想到：从初始情况开始，合并到剩余 $f(n)$ 个点时，就复制两份分别继续合并（递归），返回两种情况结果的较小值。分析概率的递归式我们会看出，选择 $f(n)$ 使得
$$
\frac{n-2}{n}\times\frac{n-3}{n-1}\times\frac{n-4}{n-2}\times\cdots\frac{f(n)+2}{f(n)+1}=\frac12
$$
时是最平衡的。容易看出应当取 $f(n)\approx n/\sqrt2$。这样会得到 $2\log_2n$ 层的二叉树。实现上我们不需要每合并一条边就重构整张图。论文里给的做法是维护每对点之间的边数，我感觉是不是用并查集就行。反正递归树叶子数是 $n^2$ 的，所以不管怎样时间都是 $\Ot(n^2)$。&lt;/p&gt;
&lt;p&gt;记 $P(n)$ 为成功率，有
$$
P(n)=1-\left(1-\frac12P\left(\frac{n}{\sqrt2}\right)\right)^2=P\left(\frac{n}{\sqrt2}\right)-\frac14P\left(\frac{n}{\sqrt2}\right)^2
$$
课上讲的神奇代换是 令 $Q(n)=4/P(n)-1$，得到
$$
Q(n)=Q(n/\sqrt2)+1+\frac{1}{Q(n/\sqrt2)}=2\log_2n+\sum_i\frac1{Q(n/2^{i/2})}=\Theta(\log n)
$$
问 chatgpt，给了个更精确的渐进式：
$$
P(2^{n/2})=\frac{4}{n+\log n+C+\omicron(1)}
$$
（话说这种二次递推不等式我在 ML 也见过，似乎关键一步都是代换倒数，感觉应该是个通法）&lt;/p&gt;
&lt;p&gt;因此只需 $\Ot(1)$ 轮就可以 w.h.p.，总时间 $\Ot(n^2)$。&lt;/p&gt;
&lt;p&gt;直觉上来说，该分治算法利用的是各步正确率的不均匀性。可以验证，如果每步的正确率是常数 $1-\epsilon$，则分治是无法优化时间的。&lt;/p&gt;
&lt;p&gt;最初的暴力算法还神奇地证明了：最小割数量 $\le\binom n2$。考虑 union bound，由于最后只会给出一个割，所以不同的割之间是不重合的，union bound 取等。所以超过 $\binom n2$ 个最小割，概率就 $&amp;gt;1$ 了。达到上界的构造是一个环。&lt;/p&gt;
&lt;h4 id=&#34;近似全局最小割&#34;&gt;近似全局最小割
&lt;/h4&gt;&lt;p&gt;论文里有讲有权的做法，时间复杂度不变。这里我们只考虑无权。&lt;/p&gt;
&lt;p&gt;整体思路是说，对每条边只以某概率保留，然后在采样后的边集上跑最小割，得到的最小割在原图中一定不会差太多。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果最小割为 $c$，则容量 $\le\alpha c$ 的割有 $\O(n^{2\alpha})$ 个。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大概口胡的证明：用一样的方法，缩到剩余 $\lfloor 2\alpha\rfloor$ 个点之后，随机选择 $S$ 返回。对于一个容量 $\le \alpha c$ 的割，得到该割的概率至少为
$$
\begin{align*}
&amp;amp;\frac{n-2\alpha}{n}\times\frac{n-1-2\alpha}{n-1}\times\cdots\times\frac{1-{2\alpha}}{1+\lfloor 2\alpha\rfloor}\times 2^{-\lfloor 2\alpha\rfloor+1}\\
={}&amp;amp;\frac{\Gamma(n-2\alpha+1)}{\Gamma(1-{2\alpha})}\frac{\Gamma(1+\lfloor 2\alpha\rfloor)}{\Gamma(n+1)}2^{-\lfloor 2\alpha\rfloor+1}\\
\ge{}&amp;amp;\Theta\left(\frac{\Gamma(n-2\alpha+1)}{\Gamma(n+1)}\right)\\
={}&amp;amp;\Theta(n^{-2\alpha})
\end{align*}
$$
倒数第二步类似于 $2^{-n}n!=\Omega(1)$；最后一步用斯特林公式展开一下。另外如果 ${2\alpha}$ 很接近 $1$ 的话可以取到剩余 $\lfloor 2\alpha\rfloor+1$ 个点。&lt;/p&gt;
&lt;p&gt;对于一个容量为 $v$ 的割，如果以 $p$ 的概率保留每条割边，那么我们知道对于 $\epsilon\in(0,1)$，剩下割边数 $v^\prime$ 满足
$$
\Pr[(1-\epsilon)pv\le v^\prime\le(1+\epsilon)pv]\ge1-2\e^{-\epsilon^2pv/3}
$$
因此如果取 $p=C\ln n/\epsilon^2c$（$c$ 是最小割），得到的概率就是 $1-\operatorname{poly}(n)^{-1}$，这个是对于单个割而言的。对于所有割，至少有其中一个在采样后的图中偏差过大的概率，根据 union bound 不超过
$$
\int_1^{+\infty}2\e^{-\epsilon^2p\alpha c/3}\mathrm{d}(n^{2\alpha})=\int_1^{+\infty}n^{-C^\prime\alpha}\mathrm{d}(n^{2\alpha})
$$
取个大点的 $C$ 就行了。这样一来，再调下参，w.h.p. 取样后的图的最小割在原图中容量 $\le(1+\epsilon)c$。&lt;/p&gt;
&lt;p&gt;如果使用 [Gabow &amp;lsquo;95] 的 $\Ot(mc)$ 算法求最小割，总的时间就是 $\Ot((m/c)c)=\Ot(m)$。&lt;/p&gt;
&lt;h4 id=&#34;近似有源汇最小割&#34;&gt;近似有源汇最小割
&lt;/h4&gt;&lt;p&gt;上面的做法不适用于有源汇最小割，因为最小割数量的 bound 的证明假设了每个点度数 $\ge c$。在有源汇的情况下，例如可以考虑 $s$ 和 $t$ 之间是 $s\to 1\to t$、$s\to 2\to t$、……，这样就有 $2^{n/2}$ 种最小割。在这种情况下 union bound 就炸了。核心的点在于，$s-t$ 最小割 $m$ 可能远大于全局最小割 $c$。&lt;/p&gt;
&lt;p&gt;一个想法是，如果一条边只出现在相当大的割中，那么它还是能像上面那样采样的。对于一条边，定义它的（普通）连通性为，其两端点的最小割。定义一条边是 $k$-strong 的，当且仅当它的两端在同一个 $k$-边连通分量中，定义强连通性 $c_e=\max\set{k\mid e\text{ is }k\text{-strong}}$。$k$-边连通分量的定义就是极大的最小割恰为 $k$ 的子图。容易看出，一条边的强连通性 $\le$ 普通连通性。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;根据上面的分析我们的直觉是，对于 $k$-边连通，以 $\Theta(\ln n/\epsilon^2k)$ 密度采样，误差可以接受。我们希望保证采样后割的期望不变，因此如果以 $p_k$ 的概率选择 $k$-连通边，那么边权要设置为 $1/p_k$。但这样不行，原因见下。&lt;/p&gt;
&lt;p&gt;需要按照强连通性采样，也就是对于 $k$-strong 边，以 $\Theta(\ln n/\epsilon^2k)$ 密度采样。这样：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;w.h.p. 所有割都偏差不超过 $\epsilon$。&lt;/li&gt;
&lt;li&gt;w.h.p. 图只有 $\O(n\ln n/\epsilon^2)$ 条边。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一个的分析不能直接用 Chernoff。[Benczúr, Karger &amp;lsquo;96] 给出了一个引理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;边 $e$ 的容量为随机变量 $X_e$，则以 $1-\operatorname{poly}(n)^{-1}$ 的概率，任何一个割的偏差都不超过 $\pm\epsilon$ 倍，这里 $\epsilon=\sqrt{C\frac{M}{\hat c}\log n}$，其中 $\hat c$ 是最小期望割，$X_e$ 满足 $0\le X_e\le M$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;容易发现无源汇那边给出的分析是一个特例（$X_e$ 是 Bernoulli 随机变量）。当 $X_e$ 不只取 $\set{0,1}$ 时，就得用 Hoeffding 而不是 Chernoff 证了，但是我怀疑这个引理 state 得有问题，我怀疑 $M$ 应该在根号外面。不管怎样，当我们以不均匀的密度采样时，直接用这个引理给出的 $\epsilon$ 太大了。&lt;/p&gt;
&lt;p&gt;魔改的证明思路是，把整个采样过程拆分为：依次在仅由 $[2^i,+\infty)$-strong 边构成的图（也就是所有 $2^i$-边连通分量）中采样 $[2^i,2^{i+1})$-strong 的边，这时 $M/\hat c=2$。于是近似比就是 $(1\pm\epsilon)^{\log m}$ 这样的一个东西。仔细分析可以得到 $1\pm\epsilon\log m$。在子阶段的分析中设置非单位容量可以进一步得到 $1+\epsilon$。&lt;/p&gt;
&lt;p&gt;需要用强连通性而不是普通连通性的原因就在于这里的分析。如果用普通连通性，会发现 $\ge k$ 的边组成的子图的最小割不一定 $\ge k$。最简单的例子是 $E=\set{(s,t),(s,1),(1,t),(s,2),(2,t),\cdots,(s,n-1),(n-1,t)}$，这里 $(s,t)$ 的普通连通性是 $n$ 而强连通性是 $2$。&lt;/p&gt;
&lt;p&gt;第二个我们通过证 $\sum1/c_e\le n$ 来说明。考虑一个连通块的最小割 $C$。根据 $k$-strong 的定义，这个连通块里的所有边 $c_e\ge|C|$，因此 $\sum_{e\in C}1/c_e\le |C|/|C|=1$。删掉 $C$ 之后归纳（删边只会使 $c_e$ 变小）即可。最后用 Chernoff bound。&lt;/p&gt;
&lt;p&gt;现在用 LCT 优化最大流就可以做到 $\Ot(n^2)$，但是我们难以求得 $c_e$，只能考虑近似。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;接下来会说明，存在算法，可以求出所有 $c_e&amp;lt;k$ 的边的一个超集 $S_k$，并且 $|S_k|&amp;lt;k(n-1)$。由于把 $c_e$ 估小是不会出错的，因此可以求出 $S_1,S_2,S_4,S_8,\cdots$，把 $S_{2^i}\setminus S_{2^{i-1}}$ 当中的边的 $c_e$ 估为 $2^{i-1}$。这样采样之后图的边数期望还是
$$
\frac{\ln n}{\epsilon^2}\sum_i\frac{|S_{2^i}|}{2^{i-1}}=\O\left(\frac{n\log^2n}{\epsilon^2}\right)
$$
另外有一个命题刚好和 $S_k$ 的大小上限 match：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\ge k(n-1)$ 条边的图一定有一个 $k$-边连通分量，于是 $c_e&amp;lt;k$ 的边数 $&amp;lt;k(n-1)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;证：考虑最小的反例 $G$，也就是 $G$ 有 $\ge k(n-1)$ 条边，且没有 $k$-边连通分量。那么 $G$ 的最小割 $c&amp;lt;k$。取走这个割，剩下的两个连通块也不含 $k$-边连通分量。因此边数 $&amp;lt;k(n_1-1)+k(n_2-1)+k=k(n-1)$，矛盾。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;定义一个 sparse $k$-connectivity certificate 为 $G$ 的一个子图，满足：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;包含所有普通连通性 $\le k$ 的边。&lt;/li&gt;
&lt;li&gt;边数 $\le k(n-1)$。&lt;/li&gt;
&lt;li&gt;对于每个割 $C$，在 $H$ 当中的容量 $\ge\min\set{|C|,k}$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;构造：每次取 $G$ 的一个生成森林。这里的关键是取生成森林是把能使连通的边都尽量取了。&lt;/p&gt;
&lt;p&gt;证明：对于一个容量为 $c$ 的割，如果某轮取得了其中 $t&amp;lt;c$ 条边，那么下一轮的生成树根据定义一定会再取至少一条。&lt;/p&gt;
&lt;p&gt;重复该构造 $2\log_2n$ 次可以包含所有强连通性 $&amp;lt;k$ 的边。考虑将所有 $k$-边连通分量缩点，得到的图就没有 $k$-边连通分量，于是边数 $&amp;lt;k(n-1)$。因此有一半的点度 $\le 2k$。每 $2$ 次可以将一半的点变为孤点，因此 $2\log_2n$ 次就行。&lt;/p&gt;
&lt;p&gt;取 $n$ 次生成森林，时间 $\O(nm)$，反而成为瓶颈。&lt;/p&gt;
&lt;hr&gt;
&lt;h5 id=&#34;求-k-连通边-nagamochi-ibaraki-92&#34;&gt;求 $k$-连通边 [Nagamochi, Ibaraki &amp;lsquo;92]
&lt;/h5&gt;&lt;p&gt;以下算法能在 $\O(m)$ 时间内给出整个划分 $F_1,\cdots,F_k$：&lt;/p&gt;
&lt;p&gt;对于每个点维护一个 $r$，表示它在 $F_1\sim F_r$ 中已经连有父亲。每次取未考虑的，$r(u)$ 最大的点 $u$，对于其未考虑的邻边 $e=(u,v)$，将 $r(v)$ 加一并将 $e$ 加入 $F_{r(v)}$。&lt;/p&gt;
&lt;p&gt;直觉上来说，它是在“并行地增广”多个生成森林。取 $r(u)$ 最大的点的直觉是，你需要保证 $u$ 能把它的邻点“纳入”自身已处于的生成森林里。如果不取最大，就会导致“本来应该早在 $r$ 层取的边，放到 $r^\prime&amp;gt;r$ 层才取”：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-galgo/7-1.png&#34;
	width=&#34;3855&#34;
	height=&#34;1460&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;264&#34;
		data-flex-basis=&#34;633px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;严格的证明是通过归纳证明以下命题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若 $(u,v)\in F_k$，则 $u,v$ 在 $F_1\sim F_{k-1}$ 中都连通。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该命题与正确性命题等价：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u,v$ 在 $F$ 的某个前缀中连通。（如果 $u,v$ 在 $F_k$ 中连通，那么 $F_k$ 中存在路径 $u-x_1-\cdots-x_t-v$，那么 $u,x_1$、$x_1,x_2$、……、$x_t,v$ 在 $F_1\sim F_{k-1}$ 中都连通，于是 $u,v$ 在 $F_1\sim F_{k-1}$ 中连通。）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将这两个命题同时归纳，然后反证。若 $(u,v)\in F_k$ 但 $u,v$ 在 $F_{k-1}$ 中不连通（$k\ge 2$），那么首先 $u$ 和 $v$ 在 $F_{k-1}$ 中各自从属于一个非单点连通块 $T_u$，$T_v$。对于 $T_u$ 和 $T_v$ 我们都可以如下刻画其生成过程：首先树中第一条边被加入时，两端的 $r$ 应该都是 $k-2$，此后该树扩张的过程中，选择的点 $r$ 一定 $\ge k-1$。这说明，不妨设 $T_u$ 的第一条边先被加入，那么在 $T_u$ 扩展到目前状态的过程中，$T_v$ 中的点是没有机会被选择的，因为第一条边加不进来。从而当 $(u,v)$ 加入 $F_k$ 时，$v$ 在 $F_v$ 中是个孤点，矛盾。&lt;/p&gt;
&lt;h3 id=&#34;一些归约矩乘的问题&#34;&gt;一些归约矩乘的问题
&lt;/h3&gt;&lt;p&gt;现代的矩乘方法都很复杂，时间复杂度自动忽略 $\log$ 因子，下面不写 $\Ot$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://sci-hub.vkif.top/10.1016/0020-0190%2891%2990071-o&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Matoušek &amp;lsquo;91&lt;/a&gt;] Dominance product $\O(n^{(3+\omega)/2})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://theoryofcomputing.org/articles/v005a009/v005a009.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vassilevska, Williams, Yuster &amp;lsquo;07&lt;/a&gt;] $(\max,\min)$ 矩乘 &amp;amp; APBP $\O(n^{2+\omega/3})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.math.tau.ac.il/~asafico/bottle.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Shapira, Yuster, Zwick &amp;lsquo;07&lt;/a&gt;] APBP（点权）$\O(n^{2+\mu})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/pdf/10.5555/1496770.1496813&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Duan, Pettie &amp;lsquo;09&lt;/a&gt;] $(\max,\min)$ 矩乘 &amp;amp; APBP $\O(n^{(3+\omega)/2})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://www.cs.cmu.edu/~virgi/nondecpaths.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Vassilevska &amp;lsquo;08&lt;/a&gt;] APNP $\O(n^{(15+\omega)/6})$&lt;/li&gt;
&lt;li&gt;[&lt;a class=&#34;link&#34; href=&#34;https://drops.dagstuhl.de/storage/00lipics/lipics-vol132-icalp2019/LIPIcs.ICALP.2019.48/LIPIcs.ICALP.2019.48.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Duan, Jin, Wu &amp;lsquo;18&lt;/a&gt;] APNP $\O(n^{(3+\omega)/2})$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;min-矩乘&#34;&gt;$(\min,+)$ 矩乘
&lt;/h4&gt;&lt;p&gt;用 $x^{a_{i,j}}$ 代替 $a_{i,j}$，这样单次运算变成多项式乘法。对于值域在 $[0,M]\cup\set{+\infty}$ 的 $(\min,+)$ 矩乘，可以在 $\O(Mn^\omega)$ 完成。&lt;/p&gt;
&lt;h4 id=&#34;无权有向图-apsp&#34;&gt;无权有向图 APSP
&lt;/h4&gt;&lt;p&gt;直接暴力做就是 $\log n$ 次 $(\min,+)$ 矩乘，但是值域每次倍增。这里的想法是，如果 $d(u,v)$ 比较大，那随便抽几个点（松弛）就能抽到 $\pi(u,v)$ 上的点。令 $r=3/2$，$D_i$ 表示所有距离 $\le r^i$ 的点对都正确求出的距离矩阵。对于一对距离为 $r^{i+1}$ 的点对 $(u,v)$，有 $r^{i+1}/3$ 个中间点 $w$ 离 $u$ 和 $v$ 的距离都 $\le r^i$。随机取大小为 $s$ 的子集 $B\subseteq[n]$，w.h.p. $D_{i+1}=(D_i)_{[n],B}(D_i)_{B,[n]}$。我们要求：
$$
\left(1-\frac{s}{n}\right)^{r^{i+1}/3}=\operatorname{poly}(n)^{-1}
$$
取 $s=Cn\ln n/r^i$ 即可。因此总复杂度为
$$
\sum_i\min\Set{r^in^{\omega(1,1-i/l,1)},n^{3-i/l}}=\sum_i\min\Set{n^{i/l+\omega(1,1-i/l,1)},n^{3-i/l}}
$$&lt;/p&gt;
&lt;p&gt;其中 $l=\log_rn$。第一项（指数）递增，第二项递减。忽略整数要求，平衡到 $n^{2.53}$ 这样。这个指数叫 $2+\mu$。&lt;/p&gt;
&lt;h4 id=&#34;maximum-witness&#34;&gt;Maximum Witness
&lt;/h4&gt;&lt;p&gt;对于 01 矩乘，定义 maximum witness
$$
(A*B)_{i,j}=\max\set{k\mid A_{i,k}=B_{k,j}=1}
$$
想法是，先定位出 $\max k$ 处于哪一段，再暴力。一个思路是令
$$
A^\prime_{i,k}=\begin{cases}\left\lfloor\frac{k}{n^{1-r}}\right\rfloor,&amp;amp;A_{i,k}=1\\ -\infty,&amp;amp;A_{i,k}=0\end{cases},\quad B^\prime_{k,j}=\begin{cases}\left\lfloor\frac{k}{n^{1-r}}\right\rfloor,&amp;amp;B_{k,j}=1\\ -\infty,&amp;amp;B_{k,j}=0\end{cases}
$$
然后以 $\O(n^{\omega+r})$ 的时间做值域为 $n^r$ 的 $(\max,+)$ 矩乘，再 $\O(n^{3-r})$ 暴力。$\O(n^{(3+\omega)/2})$。&lt;/p&gt;
&lt;p&gt;实际上这样反而是麻烦了。直接将 $A$ 的列分为 $n^r$ 块，$B$ 的行分为 $n^r$ 块，对应块以 $\O(n^{\omega(1,1-r,1)})$ 做 $(\lor,\land)$ 矩乘，这样就是 $\O(n^{2+\mu})$。&lt;/p&gt;
&lt;h4 id=&#34;近似-min-矩乘与近似-apsp&#34;&gt;近似 $(\min,+)$ 矩乘与近似 APSP
&lt;/h4&gt;&lt;p&gt;近似的意思是求得的值在实际值的 $[1,1+\epsilon]$ 倍内，所以首要的是不能估小。&lt;/p&gt;
&lt;p&gt;若值域范围大，可以通过 $\lceil Ra_{i,j}/M\rceil$ 缩小值域，最后再乘 $M/R$。但是这样近似比不均匀，例如 $M=25$，$R=5$：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;8-1.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;但是没法完美地做到每个值都是 $1+\epsilon$ 的近似比，因为没了步长均匀性，就没法转化为值域更小的矩乘。直觉是，对于每段 $[2^i,2^{i+1})$ 内的值做不同的缩放：$\lceil Ra_{i,j}/2^{i+1}\rceil$，这样和理想情况只差一个常数。近似比为
$$
\max_{a=2^i}^{2^{i+1}}\frac{\frac{2^{i+1}}{R}\left\lceil\frac{R}{2^{i+1}}a\right\rceil}{a}&amp;lt;\max_{a=2^i}^{2^{i+1}}\frac{a+\frac{2^{i+1}}{R}}{a}=1+\frac2R
$$
实际做的时候，对于 $a_{i,k}+b_{k,j}$，我们只能期望在 $\max\set{a_{i,k},b_{k,j}}$ 所在的那层求得。缩放值时，也是将 $&amp;gt;2^{i+1}$ 的值直接设为 $+\infty$。因此最终的近似比应当形如
$$
\max_{a=2^i}^{2^{i+1}}\max_{b=0}^{2^{i+1}}\frac{\frac{2^{i+1}}{R}\left\lceil\frac{R}{2^{i+1}}a\right\rceil+\frac{2^{i+1}}{R}\left\lceil\frac{R}{2^{i+1}}b\right\rceil}{a+b}&amp;lt;\max_{a=2^i}^{2^{i+1}}\max_{b=0}^{2^{i+1}}\frac{a+\frac{2^{i+1}}{R}+b+\frac{2^{i+1}}{R}}{a+b}&amp;lt;\max_{a=2^i}^{2^{i+1}}\frac{a+2\cdot\frac{2^{i+1}}{R}}{a}=1+\frac4R
$$
总的时间就是 $\O(n^\omega\epsilon^{-1}\log M)$。APSP 就是 $\epsilon$ 要多除以个 $\log_2n$。&lt;/p&gt;
&lt;h4 id=&#34;dominance-product&#34;&gt;Dominance Product
&lt;/h4&gt;&lt;p&gt;定义 dominance product
$$
(A\diamond B)_{i,j}=\left\lvert\Set{k\mid A_{i,k}\le B_{k,j}}\right\rvert
$$
注意到所有比较只发生在各个 $k$，$A$ 的第 $k$ 列和 $B$ 的第 $k$ 行之间。将 $\set{A_{\cdot,k}}\cup\set{B_{k,\cdot}}$ 排序为 $L_k$ 后，剩下的就是一个一维偏序型的贡献。要把它归约到矩乘，就需要双方独立，不能有这样一个偏序关系，这个要求在一些 ds 问题中我们见过，是用 cdq 分治优化的。那么这里只能分块了。&lt;/p&gt;
&lt;p&gt;将 $L_k$ 分为 $n^r$ 段 $[L_{k,1},\cdots,L_{k,n^r}]$。对于块内，暴力贡献，时间为 $(n/n^r)^2n^r\cdot n=n^{3-r}$。对于块间，可以这样写贡献：令 $(A_b)_{i,k}=[A_{i,k}\in L_{k,b}]$，$(B_b)_{k,j}=[B_{k,j}\in L_{k,b}]$。那么总的答案应为
$$
\sum_bA_b\sum_{b^\prime&amp;gt;b}B_b
$$
需要 $n^r$ 次矩乘，时间 $n^{r+\omega}$。平衡取 $r=(3-\omega)/2$ 即得 $\O(n^{(3+\omega)/2})$。&lt;/p&gt;
&lt;h4 id=&#34;maxmin-矩乘与-apbp&#34;&gt;$(\max,\min)$ 矩乘与 APBP
&lt;/h4&gt;&lt;p&gt;$$
(A\circ B)_{i,j}=\max_k\min\set{A_{i,k},B_{k,j}}
$$&lt;/p&gt;
&lt;p&gt;这个东西和 dominance product 有些关系。注意到
$$
(A\circ B)_{i,j}=\max\Set{\max_{k\mid A_{i,k}\le B_{k,j}}A_{i,k},\max_{k\mid A_{i,k}&amp;gt;B_{k,j}}B_{k,j}}
$$
因此只需考虑求
$$
D_{i,j}=\max_{k\mid A_{i,k}\le B_{k,j}}A_{i,k}
$$
一个大致的思路就是，通过 dominance product 判断（某一段值域内）有没有 $A_{i,k}\le B_{k,j}$（相当于剪枝），有的话就逐一检查。&lt;/p&gt;
&lt;p&gt;将 $A$ 的第 $i$ 行按值域分为 $r$ 段 $[L_{i,1},\cdots,L_{i,n^r}]$，类似地令 $(A_b)_{i,k}=[A_{i,k}\in L_{i,b}]A_{i,k}$，然后求 $A_b\diamond B$。求 $D_{i,j}$ 时，先找到 $A$ 的第 $i$ 行的最大段，和 $B$ 的第 $j$ 列在 dominance product 中结果非零，然后暴力枚举块内的每个元素去判断。暴力的部分时间为 $n^{3-r}$。&lt;/p&gt;
&lt;p&gt;有一个结果是，稀疏的 dominance product（$A$ 有 $m_1$ 个非 $+\infty$，$B$ 有 $m_2$ 个非 $-\infty$）可以做到 $\O(\sqrt{m_1m_2}n^{(\omega-1)/2})$。因此这里的时间为 $n^r\sqrt{n^{2-r}n^2}n^{(\omega-1)/2}=n^{(3+\omega+r)/2}$。平衡即得 $n^{2+\omega/3}$，即 $\omega$ 和 $3$ 的右三等分点。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;很自然的想法是把 $B$ 也分段了。和 dominance product 不同的是，后者比较的是 $A$ 的第 $k$ 列和 $B$ 的第 $k$ 行的每一对元素，而现在需要比较的是 $A$ 的第 $i$ 行和 $B$ 的第 $j$ 列的对应位元素，所以不能形如“将 $\set{A_{i,\cdot}}\cup\set{B_{\cdot,j}}$ 排序”。考虑将所有元素，即 $\set{A_{i,j}}\cup\set{B_{i,j}}$ 排序后分成 $2n^r$ 段，这时问题就分为两部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$A_b\diamond B_b$，这时 $m_1=m_2=n^{2-r}$，因此总的只有 $\O(n^{(3+\omega)/2})$。&lt;/li&gt;
&lt;li&gt;$A_{\in b}\sum_{b^\prime&amp;gt;b}B_{\in b^\prime}$，这个部分和 dominance product 一样，就是正常矩乘，$\O(n^{r+\omega})$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;同样我们对 $A$ 的第 $i$ 行找到最大的 $b$ 使得存在第 $b$ 段中的 $A_{i,k}\le B_{k,j}$，暴力枚举其中的元素检查，$\O(n^{3-r})$。总的就是 $\O(n^{(3+\omega)/2})$。&lt;/p&gt;
&lt;p&gt;但是由于是全局排序，可能单行内各段元素数量就不均匀。这个容易处理。对于第 $b$ 段，将 $A$ 的第 $i$ 行所有在第 $b$ 段内的元素再排序，如果有 $c$ 个，就再拆成长为 $n^{1-r},\cdots,n^{1-r},c\bmod n^{1-r}$ 的段。令 $A^{\prime\prime}_b$ 为只包含最后那段的矩阵，$A^{\prime}_b$ 则是将各个行的每个长为 $n^{1-r}$ 的段分到不同的行（各元素的列位置不变）的矩阵（由于总共就 $n^{2-r}$ 个第 $b$ 段的数，所以原大小的矩阵能容下）。然后分别做上面的两种，复杂度不变。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>计算机图形学 project：物理渲染</title>
        <link>https://littlereuben.github.io/study/course-acg/</link>
        <pubDate>Mon, 23 Feb 2026 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/course-acg/</guid>
        <description>&lt;img src="https://littlereuben.github.io/study/course-acg/cover.png" alt="Featured image of post 计算机图形学 project：物理渲染" /&gt;&lt;div style=&#34;background-color: #FFF9B9; color: #796E00; padding-left: 1em; border-left: 4px solid #DED041; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;学习笔记&lt;/b&gt;。&lt;/div&gt;
&lt;p&gt;封面图：最终加了 emission 材质的洞穴场景（$\sim 20000\text{ SSP}$）。&lt;/p&gt;
&lt;p&gt;期末 project 提供了 GPU 渲染（hlsl）的仓库，其中实现了基本的 UI 和数据绑定，主要代码是填 &lt;code&gt;shader.hlsl&lt;/code&gt;。原仓库：&lt;a class=&#34;link&#34; href=&#34;https://github.com/lihe50hz/ShortMarch&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/lihe50hz/ShortMarch&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;接下来的内容是 project report 的草稿。或许在困惑于实现细节时能提供一些参考。&lt;/p&gt;
&lt;p&gt;Thanks to my teammate zzt. I wouldn&amp;rsquo;t have gone through such a long &lt;ruby&gt;way&lt;rt&gt;march&lt;/rt&gt;&lt;/ruby&gt; and completed this huge project without you.&lt;/p&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction
&lt;/h3&gt;&lt;p&gt;我们在 ShortMarch 的基础上，写了一个基于 Ray Tracing 的物理渲染器。这个渲染器支持面的各种材料参数和贴图，支持两类光源，并且通过设计重要性采样公式降低了蒙特卡洛采样的方差，还加了一些其他小的功能。&lt;/p&gt;
&lt;p&gt;我们发现这个渲染器尤其适合渲染 MC 中的场景。它支持了 MC 中大部分的图形功能，能够产出很有氛围的漂亮图片。&lt;/p&gt;
&lt;h3 id=&#34;personal-contribution&#34;&gt;Personal Contribution
&lt;/h3&gt;&lt;p&gt;我实现的部分主要为渲染器（hlsl）主体。包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base, Ray tracing 核心逻辑&lt;/li&gt;
&lt;li&gt;Material
&lt;ul&gt;
&lt;li&gt;透射材质&lt;/li&gt;
&lt;li&gt;principled BSDF（including metallic, roughness, transmission, IOR, subsurface, clear coat, sheen and emission）&lt;/li&gt;
&lt;li&gt;multi-layer material（对于 clear coat 单独的可调的 roughness）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Texture（与我的队友合作完成）
&lt;ul&gt;
&lt;li&gt;纯色材质&lt;/li&gt;
&lt;li&gt;纹理（颜色）贴图（含 alpha），法线贴图&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;重要性采样，除了 RR 以外：
&lt;ul&gt;
&lt;li&gt;包括关于 material 和 光源的 MIS&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;点光源与面光源&lt;/li&gt;
&lt;li&gt;anti-aliasing&lt;/li&gt;
&lt;li&gt;tone mapping&lt;/li&gt;
&lt;li&gt;scene creation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意在面光源存在的情况下，alpha shadow 是自动满足要求的。&lt;/p&gt;
&lt;p&gt;我所编写/修改的代码有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;shader.hlsl&lt;/code&gt; 中的大部分内容&lt;/li&gt;
&lt;li&gt;&lt;code&gt;app.cpp&lt;/code&gt;、&lt;code&gt;material.h&lt;/code&gt; 等文件中向 &lt;code&gt;shader.hlsl&lt;/code&gt; 传必要数据的操作&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我的队友主要负责：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Special Visual Effects 中的景深、动态模糊&lt;/li&gt;
&lt;li&gt;HDR 天空&lt;/li&gt;
&lt;li&gt;Volumetric Rendering 中的一部分功能&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;method&#34;&gt;Method
&lt;/h3&gt;&lt;h4 id=&#34;核心逻辑&#34;&gt;核心逻辑
&lt;/h4&gt;&lt;p&gt;在给出的 hlsl 模板代码的基础上，我填充了 &lt;code&gt;ClosestHitMain&lt;/code&gt; 里的逻辑。由于递归层数限制为 31，所以我选择只在 &lt;code&gt;ClosestHitMain&lt;/code&gt; 里处理单次光线弹射的逻辑，而单独写一个函数 &lt;code&gt;Calculate&lt;/code&gt; 用 &lt;code&gt;while&lt;/code&gt; 循环处理多轮弹射。在多轮弹射的过程中，我记录累计的颜色，以及当前的 throughput 系数 &lt;code&gt;coef&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;ClosestHitMain&lt;/code&gt; 中，我大致做了以下几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;得到当前三角形相关的信息，例如法线以及根据顶点法线插值&lt;/li&gt;
&lt;li&gt;处理体渲染（不是我的工作）&lt;/li&gt;
&lt;li&gt;做俄罗斯轮盘&lt;/li&gt;
&lt;li&gt;获取材质&lt;/li&gt;
&lt;li&gt;做 MIS，决定这一条光线走哪个分量&lt;/li&gt;
&lt;li&gt;光源采样&lt;/li&gt;
&lt;li&gt;决定弹射方向&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;材质&#34;&gt;材质
&lt;/h4&gt;&lt;p&gt;我实现的材质的属性包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;roughness&lt;/li&gt;
&lt;li&gt;metallic&lt;/li&gt;
&lt;li&gt;emission&lt;/li&gt;
&lt;li&gt;transmission with IoR，这个 transmission 表示透光率，是三个实数，它和漫反射互相影响能量分配，不干扰 specular。可以理解为是水、玻璃这种。&lt;/li&gt;
&lt;li&gt;alpha，这个 alpha 是单个实数，表示物体的“幽灵程度”。alpha=0 表示完全不可见，alpha=1 表示完全可见。常用于处理一个面上部分透明的情况，例如 MC 中的树叶。&lt;/li&gt;
&lt;li&gt;clear coat，包括强度和粗糙度&lt;/li&gt;
&lt;li&gt;sheen&lt;/li&gt;
&lt;li&gt;subsurface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了下面涉及的 BSDF 的数值稳定性，我限制最小的 roughness 为 0.01。&lt;/p&gt;
&lt;p&gt;材质的着色计算思路遵循以下层次结构：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;. 只有镜面反射与漫反射。根据基本的 rendering equation:
$$
L_o(p,w_o)=c_{\rm emission}+\int_\Omega L_i(p,w_i)f(p,\omega_i,\omega_o)(n\cdot\omega_i)\mathrm{d}\omega_i
$$
使用 Cook–Torrance，记 $h$ 为 half vector：
$$
f(p,\omega_i,\omega_o)=f_{\rm specular}(p,\omega_i,\omega_o)+f_{\rm diffuse}(p,\omega_i,\omega_o)
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;specular 分量&lt;/strong&gt;：
$$
f_{\rm specular}(p,\omega_i,\omega_o)=\frac{F(\omega_o,h)D(h)G(\omega_o,\omega_i)}{4(n\cdot\omega_i)(n\cdot\omega_o)+\epsilon}
$$&lt;/p&gt;
&lt;p&gt;使用 Fresnel-Schlick 近似：
$$
F(\omega_o,h)=F_0+(1-F_0)(1-\omega_o\cdot h)^5
$$&lt;/p&gt;
&lt;p&gt;基础反射率（$F$ 是一个 &lt;code&gt;float3&lt;/code&gt;，但是公式上我们可以只考虑 RGB 中的一个分量，因此变量类型上的问题我就不多加说明了）：
$$
F_0=(1-\text{metallic})\left(\frac{\eta-1}{\eta+1}\right)^2+\text{metallic}\cdot c_{\rm base}
$$&lt;/p&gt;
&lt;p&gt;微表面法线分布使用 GGX（GTR2）：
$$
D(h)=\frac{\alpha^2}{\pi((n\cdot h)^2(\alpha^2-1)+1)^2}\quad\text{where }\alpha=\text{roughness}^2
$$&lt;/p&gt;
&lt;p&gt;shadowing-masking function：
$$
G(\omega_o,\omega_i)=G_1(\omega_o)G_1(\omega_i)
$$&lt;/p&gt;
&lt;p&gt;$$
G_1(\omega)=\frac{n\cdot\omega}{(1-k)(n\cdot\omega)+k}\quad\text{where }k=\frac{(\alpha+1)^2}8
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;diffuse 分量&lt;/strong&gt;，主要考虑的是与 specular 的能量分配分配：
$$
f_{\rm diffuse}(p,\omega_i,\omega_o)=\frac1\pi(1-\text{metallic})(1-F(\omega_o,h))c_{\rm base}
$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;. 优化 diffuse 分量，考虑 sheen 和 subsurface。
$$
f_{\rm diffuse}(p,\omega_i,\omega_o)=\text{deduction terms}\cdot(f_{\text{diffuse with subsurface}}(p,\omega_i,\omega_o)+f_{\rm sheen}(p,\omega_i,\omega_o))
$$
subsurface 部分有两个分量：
$$
f_{\text{diffuse with subsurface}}(p,\omega_i,\omega_o)=(1-I_\text{subsurface})f_{\rm base}(p,\omega_i,\omega_o)+I_\text{subsurface}\cdot f_{\text{subsurface}}(p,\omega_i,\omega_o)
$$
我同时也稍微改了以下 base 分量的计算，用 Disney diffuse：
$$
f_{\rm base}(p,\omega_i,\omega_o)=\frac1\pi(1+(F_{\rm D90}-1)(1-n\cdot\omega_i)^5)(1+(F_{\rm D90}-1)(1-n\cdot\omega_o)^5)c_{\rm base}\quad\text{where }F_{\rm D90}=0.5+2\cdot\text{roughness}^2\cdot(\omega_i\cdot h)^2
$$
subsurface 用 Hanrahan-Krueger:
$$
f_{\text{subsurface}}(p,\omega_i,\omega_o)=1.25\left(F_{\rm SS90}\cdot\left(\frac{1}{(n\cdot\omega_i)(n\cdot \omega_o)}-0.5\right)+0.5\right)\quad\text{where }F_{\rm SS90}=\text{roughness}^2\cdot(\omega_i\cdot h)^2
$$
sheen：
$$
f_{\rm sheen}(p,\omega_i,\omega_o)=(\text{tint}\cdot c_{\rm base}+(1-\text{tint}))\cdot\text{sheen}\cdot(1-(\omega_i\cdot h)^5)
$$
&lt;strong&gt;Step 3&lt;/strong&gt;. 考虑透射。镜面反射的分量不变，剩余的部分分配给透射和漫反射。因此 $f_{\rm diffuse}$ 需要乘上一个 $(1-c_{\rm transmission})$。&lt;/p&gt;
&lt;p&gt;折射的分量是确定方向的。对于处于面的异侧的 $\omega_i$ 和 $\omega_o$：
$$
f_{\rm transmissive}(p,\omega_i,\omega_o)=c_{\rm transmission}(1-\text{metallic})(1-F(n,\omega_i))\frac{1}{\eta^2}\frac{\delta(\omega_o-\omega_r)}{n\cdot\omega_i}
$$
$\omega_r$ 根据 Snell&amp;rsquo;s law 计算。&lt;/p&gt;
&lt;p&gt;其中 $\eta$ 是当前面的折射率，例如如果当前面的正面是空气，背面是玻璃，从玻璃外向里看，那么 $\omega_i$ 在玻璃内，$\omega_o$ 在玻璃外。这时能量应当衰减约 $2.25$ 倍。相对地，如果从玻璃里面往外面看，会因为全反射现象只能看到一个圆锥内的事物，并且会变亮。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;. 清漆。清漆是在所有上述项之上的一层只有 specular 且按照几何法线做 specular 的层。它有强度 $I_{\rm coat}$ 和粗糙度 $\text{roughness}_{\rm coat}$。在加入它之后，
$$
f(p,\omega_i,\omega_o)=f_{\rm coat}(p,\omega_i,\omega_o)+(1-F_{\rm coat}(n,\omega_i))(1-F_{\rm coat}(n,\omega_o))(f_{\rm specular}(p,\omega_i,\omega_o)+f_{\rm diffuse}(p,\omega_i,\omega_o)+f_{\rm transmissive}(p,\omega_i,\omega_o))
$$
这个系数是因为光线既要穿入一层清漆，又要穿出一层清漆。&lt;/p&gt;
&lt;p&gt;其中
$$
f_{\rm specular}(p,\omega_i,\omega_o)=I_{\rm coat}\frac{F_{\rm coat}(\omega_o,h)D_{\rm coat}(h)G_{\rm coat}(\omega_o,\omega_i)}{4(n\cdot\omega_i)(n\cdot\omega_o)+\epsilon}
$$
清漆使用不太一样的 $F$, $D$, $G$. 其中&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$F$ 中固定 $F_0=0.04$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$G$ 中固定 $\alpha=0.25$，并使用 Smith-GGX：
$$
G_1(v) = \frac{2 (n \cdot \omega)}{(n \cdot \omega) + \sqrt{\alpha^2 + (1-\alpha^2)(n \cdot \omega)^2}}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$D$ 使用 GTR1 分布：
$$
D(h)=\frac{(\alpha^2-1)(n\cdot h)}{\pi\ln(\alpha^2)((n\cdot h)^2(\alpha^2-1)+1)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;. alpha。直接以 $1-\text{alpha}$ 的概率穿过当前这个面，方向不变。&lt;/p&gt;
&lt;h4 id=&#34;材质相关的采样&#34;&gt;材质相关的采样
&lt;/h4&gt;&lt;p&gt;首先以以下判断分支选择分量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;以 $p_{\rm coat}=I_{\rm coat}\operatorname{luminance}(F_{\rm coat}(n,\omega_o))$ 的概率选择 clear coat&lt;/li&gt;
&lt;li&gt;以 $p_{\rm specular}=\operatorname{luminance}(F(n,\omega_o))+0.1\cdot (1-\text{roughness})$ 的概率选择 specular&lt;/li&gt;
&lt;li&gt;以 $p_{\rm transmissive}=\operatorname{luminance}(c_{\rm transmission})$ 的概率选择 transmissive&lt;/li&gt;
&lt;li&gt;否则选择 diffuse&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中 $\operatorname{luminance}((r,g,b))=0.2126r+0.7152g+0.0722b$.&lt;/p&gt;
&lt;p&gt;为了避免漏下分量或在重要性采样计算结果时除以过小的数导致数值稳定性问题，这些 $p$ 都 clamp 到 $[0.05,0.95]$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;diffuse&lt;/strong&gt; 采样：下文中 $\xi_1,\xi_2\sim U(0,1)$。
$$
\begin{aligned}
\varphi &amp;amp;= 2\pi \xi_1,\\
r &amp;amp;= \sqrt{\xi_2},\\
x &amp;amp;= r\cos\varphi,\quad y=r\sin\varphi,\quad z=\sqrt{1-r^2},\\
\omega_i &amp;amp;= (x,y,z),\\
p_{\rm diffuse}(\omega_i) &amp;amp;= \frac{z}{\pi}.
\end{aligned}
$$
&lt;strong&gt;specular&lt;/strong&gt; 采样：
$$
\begin{aligned}
\varphi &amp;amp;= 2\pi \xi_1,\\
\cos\theta &amp;amp;= \sqrt{\frac{\xi_2}{\alpha^2 - \xi_2(\alpha^2 - 1)} },\\
h &amp;amp;= (\sin\theta\cos\varphi,\sin\theta\sin\varphi,\cos\theta),\\
\omega_i &amp;amp;= 2(\omega_o\cdot h)h-\omega_o\\
p_{\rm specular}(\omega_i) &amp;amp;= \frac{D(h)(n\cdot h)}{4(\omega_o \cdot h)},
\end{aligned}
$$
&lt;strong&gt;clear coat&lt;/strong&gt; 采样：
$$
\begin{aligned}
\varphi &amp;amp;= 2\pi \xi_1,\\
\cos\theta &amp;amp;= \sqrt{\frac{1-\alpha^{2\xi_2}}{1-\alpha^2}},\\
h &amp;amp;= (\sin\theta\cos\varphi,\sin\theta\sin\varphi,\cos\theta),\\
\omega_i &amp;amp;= 2(\omega_o\cdot h)h-\omega_o\\
p_{\rm coat}(\omega_i) &amp;amp;= \frac{D_{\rm coat}(h)(n\cdot h)}{4(\omega_o \cdot h)},
\end{aligned}
$$
特别地，specular 和 clear coat 的采样需要额外的 reject sampling 保证 $\omega_o$ 不跑到面的另一侧去。&lt;/p&gt;
&lt;p&gt;对于每个分量，我在计算贡献时，考虑的系数是它对应 BSDF 中的那个分量，并除以随机到这个分量和这个 $\omega_i$ 的概率密度。容易证明，这样蒙特卡洛是无偏的。&lt;/p&gt;
&lt;h4 id=&#34;光源采样&#34;&gt;光源采样
&lt;/h4&gt;&lt;p&gt;我只对于以下分量做光源采样：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;diffuse&lt;/li&gt;
&lt;li&gt;roughness &amp;gt; 0.1 的 specular&lt;/li&gt;
&lt;li&gt;coat roughness &amp;gt; 0.1 的 clear coat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这是因为，roughness 过小的镜面反射，以及折射，它们的 BSDF 都可以认为成是一个 Dirac $\delta$ 函数，这种情况光源采样反而会使噪点严重，或者出现严重的辉光/fireflies。&lt;/p&gt;
&lt;p&gt;光源采样的过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;枚举每个点光源 $q$，令 $r=|q-p|$, $\omega_i=(q-p)/r$
$$
\Delta L_o=\text{attenuation}\cdot f(p,\omega_i,\omega_o)(n\cdot\omega_i)\frac{c_{\rm light}}{r^2}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;枚举每个面光源，随机上面一点 $q=(1-\sqrt{\xi_1})p_0+\sqrt{\xi_1}\sqrt{\xi_2}p_1+\sqrt{\xi_1}(1-\sqrt{\xi_2})p_2$，令 $A$ 为面积，$n^\prime$ 为面光源的法线
$$
\Delta L_o=\text{attenuation}\cdot f(p,\omega_i,\omega_o)(n\cdot\omega_i)(n^\prime\cdot\omega_i)\frac{c_{\rm emission}}{r^2}A
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;将这些贡献加到累计颜色中，并根据情况 disable/enable 下一跳的 emission 是否要计入。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;若干解释：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;光源采样独立于材质采样的分量。无论材质采样的分量随机到的是哪一个，光源采样总是把这些全部算进去。&lt;/li&gt;
&lt;li&gt;这里的 $f$ 只算我上面说的这些分量&lt;/li&gt;
&lt;li&gt;$\rm attenuation$ 这样计算：我使用一个利用 &lt;code&gt;anyhit&lt;/code&gt; 的 &lt;code&gt;TraceRay&lt;/code&gt; 过程，将 &lt;code&gt;RayDesc&lt;/code&gt; 中的 &lt;code&gt;TMax&lt;/code&gt; 设置为 $r-\epsilon$。在 &lt;code&gt;anyhit&lt;/code&gt; 中，得到当前击中点的材质，并算出它的 alpha 和 transmission 分量的总系数，乘给 attenuation。注意，这里我相当于假设了所有的透射材质的 ior 都等于 1，这也算是个 compromise，因为考虑折射了就没法光源采样了。&lt;/li&gt;
&lt;li&gt;设置下一跳是否要计入 emission，我在 &lt;code&gt;RayPayload&lt;/code&gt; 中用 &lt;code&gt;bool count_emission&lt;/code&gt; 记录：
&lt;ul&gt;
&lt;li&gt;如果是透射，则不变&lt;/li&gt;
&lt;li&gt;如果这一次材质采样选择的分量是 diffuse / roughness &amp;gt; 0.1 的 specular / roughness &amp;gt; 0.1 的 clear coat 这三种计算光源采样的分量，下一跳不计入&lt;/li&gt;
&lt;li&gt;否则计入&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;anti-aliasing&#34;&gt;Anti-aliasing
&lt;/h4&gt;&lt;p&gt;这个很简单，就用抖动采样（对于一个像素，每次随机一个 $[0,1]\times[0,1]$ 内的点，把光线往对应方向打），数学上就是对的&lt;/p&gt;
&lt;h4 id=&#34;tone-mapping&#34;&gt;Tone mapping
&lt;/h4&gt;&lt;p&gt;对于 RGB 的每个分量，都使用映射
$$
x\mapsto\frac{Cx}{1+Cx}
$$
来将 $[0,+\infty)$ 中的数值映射到 $[0,1]$。实践上 $C=3$ 是个不错的选择。&lt;/p&gt;
&lt;h4 id=&#34;材质处理&#34;&gt;材质处理
&lt;/h4&gt;&lt;p&gt;传入材质分为以下几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;利用 &lt;code&gt;tiny_obj_loader&lt;/code&gt; 载入 &lt;code&gt;obj&lt;/code&gt; 和 &lt;code&gt;mtl&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;external&lt;/code&gt; 中的 &lt;code&gt;math_mesh&lt;/code&gt; 增加对应的，需要传给 &lt;code&gt;Entity.cpp&lt;/code&gt; 的东西&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;app.cpp&lt;/code&gt; 中通过 resource binding 传给 &lt;code&gt;shader.hlsl&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;shader.hlsl&lt;/code&gt; 中用 &lt;code&gt;SampleLevel&lt;/code&gt; 与插值得到的 uv 坐标得到材质&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;scene-creation&#34;&gt;Scene creation
&lt;/h4&gt;&lt;p&gt;我们在 final report 上 feature 了几张 MC 场景的渲染。这些是我选择 MC 自然生成的地形，或者手动在 MC 里搭建了一些场景后，用 mineways 工具导出为 &lt;code&gt;obj&lt;/code&gt;，然后再手动修改其中部分材质 &lt;code&gt;mtl&lt;/code&gt; 配置所完成的。&lt;/p&gt;
&lt;h3 id=&#34;discussion&#34;&gt;Discussion
&lt;/h3&gt;&lt;p&gt;关于我实现的部分，主要有两方面内容还值得进一步讨论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;透射材质的能量分配问题。理论上，镜面反射、漫反射、透射的能量分配应当根据一个统一的 fresnel 项，但是这里我透射用的 fresnel 项是由 $n\cdot \omega_i$ 计算的，和前两者的 $h\cdot \omega_o$ 不一致。这应该主要是由于 Schlick 近似导致的。一个更加严格的实现是 &lt;a class=&#34;link&#34; href=&#34;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cs.cornell.edu/~srm/publications/EGSR07-btdf.pdf&lt;/a&gt;，不过比较可惜我来不及实现了。&lt;/li&gt;
&lt;li&gt;光源采样的问题。我的实现中，穿过透射材质是不改变 &lt;code&gt;count_emission&lt;/code&gt;。根据 AI 的建议，说会出现“墙上的黑色玻璃球问题”，但是我认为如果穿过透射也算就会 double count，所以我最终还是不算。光源采样时不考虑折射，是不得已而为之的近似。存在 MNEE 和 BDPT，以及 photon mapping 等进阶做法。&lt;/li&gt;
&lt;li&gt;在同时有顶点法线插值和法线贴图时，计算最终法线似乎有点小问题，直接对几何法线算出的 $T$ 和 $B$ 关于插值后 $N$ 做正交化似乎会导致很轻微的 artifact，我也没管了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;gallery&#34;&gt;Gallery
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/1.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;roughness 和 metallic 的组合：从左往右 metallic 递增，从前往后 roughness 递增，天花板为面光源，中间有个点光源&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/2.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;alpha 和 transmission 的对比：左边是 alpha=0, transmission=(0,0,0), 中间是 alpha=0.5, transmission=(0,0,0)，右边是 alpha=0, transmission=(0.5,0.5,0.5)&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/3.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;specular 和 clear coat 的对比：左边是粗糙无清漆, 中间是光滑（with roughness=0.05）无清漆，右边是粗糙有清漆（with 强度=0.5, roughness=0.05）&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/4.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;sheen：左边是无 sheen，中间是有 sheen，tint=0，右边是有 sheen，tint=1。为了凸显效果我把 sheen 的强度调为 10 了。可以关注 matball 的底座部分&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/5.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;subsurface：左中右分别是 subsurface = 0/0.5/1.0&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/6.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;具有折射的玻璃&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/7.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;从玻璃内向外看&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/8.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;Cornell Box in MC&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/9.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;材质贴图与法线贴图：一个鞋子&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/10.png&#34;
	width=&#34;1920&#34;
	height=&#34;1060&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;MC 场景：村庄&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;434px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/11.png&#34;
	width=&#34;1920&#34;
	height=&#34;1060&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;MC 场景：洞穴&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;181&#34;
		data-flex-basis=&#34;434px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/12.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;透过彩色玻璃板看点光源&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/13.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;动态模糊（队友工作）&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/14.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;发光体渲染（队友工作）&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/15.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;体渲染（队友工作）&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/16.png&#34;
	width=&#34;1280&#34;
	height=&#34;720&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;MC 中透过活板门的丁达尔效应（队友工作）&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;177&#34;
		data-flex-basis=&#34;426px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-acg/17.png&#34;
	width=&#34;1536&#34;
	height=&#34;1024&#34;
	
	loading=&#34;lazy&#34;
	
		alt=&#34;封面图丢给 GPT 去除 fireflies 后的结果，有失真&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>AI Principles &amp; Techniques (人工智能原理与技术笔记)</title>
        <link>https://littlereuben.github.io/study/course-aipt/</link>
        <pubDate>Sun, 22 Feb 2026 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/course-aipt/</guid>
        <description>&lt;div style=&#34;background-color: #FFF9B9; color: #796E00; padding-left: 1em; border-left: 4px solid #DED041; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;学习笔记&lt;/b&gt;。&lt;/div&gt;
&lt;p&gt;$\gdef\e{\mathrm{e}}\gdef\sm{\operatorname{softmax}}\gdef\mat#1{\begin{bmatrix}#1\end{bmatrix}}\gdef\b#1{\boldsymbol{#1}}\gdef\p{\partial}\gdef\omat{\operatorname{mat}}\gdef\ovec{\operatorname{vec}}\gdef\d{\mathrm{d}}\gdef\E{\operatorname*{E}}\gdef\B{\operatorname{B}}\gdef\eps{\varepsilon}\gdef\argmin{\operatorname*{argmin}}\gdef\argmax{\operatorname*{argmax}}$&lt;/p&gt;
&lt;h3 id=&#34;outline&#34;&gt;Outline
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-aipt/outline.png&#34;
	width=&#34;3660&#34;
	height=&#34;2390&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;153&#34;
		data-flex-basis=&#34;367px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;classical-machine-learning&#34;&gt;Classical Machine Learning
&lt;/h3&gt;&lt;h4 id=&#34;supervised-learning&#34;&gt;Supervised Learning
&lt;/h4&gt;&lt;h5 id=&#34;notation&#34;&gt;Notation
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Input $\b x_i$
&lt;ul&gt;
&lt;li&gt;For simple regression, we usually make $\b x_i^\prime=\mat{\b x_i\\1}$ to include the bias term.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Output $\b y_i$&lt;/li&gt;
&lt;li&gt;Parameters $\b\theta$&lt;/li&gt;
&lt;li&gt;Hypothesis $f(\b x;\b\theta)$&lt;/li&gt;
&lt;li&gt;Loss $\ell(\b y,\b{\hat y})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Goal: given a set of $\set{(\b x_i,\b y_i)}$, we want to find
$$
\b\theta^{*}=\operatorname*{argmin}_{\b\theta}\frac1N\sum_{i=1}^N\ell(\b y_i,f(\b x_i;\b\theta))
$$&lt;/p&gt;
&lt;h5 id=&#34;linear-regression&#34;&gt;Linear Regression
&lt;/h5&gt;&lt;p&gt;Used for $y_i=f(\b x_i)+\mathrm{err}_i$, where $f$ is some linear function.
$$
\left\{\begin{align*}
&amp;amp;\b\theta=\mat{\b w\\b}\\
&amp;amp;f(\b x;\b \theta)=\b\theta^\top\b x^\prime=\b w^\top\b x+b\\
&amp;amp;\ell(y,\hat y)=\frac12(y-\hat y)^2
\end{align*}\right.
$$
Let $X=\mat{\b x_1&amp;amp;\cdots&amp;amp;\b x_n}^\top$, $\b y=\mat{y_1&amp;amp;\cdots&amp;amp;y_n}^\top$, then our goal is to minimize $\|X\b\theta-\b y\|_2^2$, which can be solved by the Least Square Method: $\b\theta^{*}=(X^\top X)^{-1}X^\top\b y$.&lt;/p&gt;
&lt;h5 id=&#34;logistic-regression&#34;&gt;Logistic Regression
&lt;/h5&gt;&lt;p&gt;Used for a classification problem, i.e. $y_i\in[C]$. You can think of it as the domain space being divided into regions with linear boundaries.&lt;/p&gt;
&lt;p&gt;First consider $C=2$, consider $y_i\in\set{\pm 1}$:
$$
\left\{\begin{align*}
&amp;amp;f(\b x;\b \theta)=\b w^\top\b x+b\\
&amp;amp;\ell(y,\hat y)=[y\ne\operatorname{sgn}(\hat y)]=[y\hat y&amp;lt;0]
\end{align*}\right.
$$
Not optimizable. Let $\sigma(x)=1/(1+\e^x)$, it has a property that $\sigma(-x)=1-\sigma(x)$.
$$
\left\{\begin{align*}
&amp;amp;f(\b x;\b \theta)=\b w^\top\b x+b\\
&amp;amp;\ell(y,\hat y)=-\log\sigma(y\hat y)
\end{align*}\right.\tag{1}
$$
For multiple classes, let $\sm(\b x)_i=\e^{\b x_i}/\sum_j\e^{\b x_j}$.
$$
\left\{\begin{align*}
&amp;amp;f(\b x;\b \theta)=W_{C\times n}\b x+\b b\\
&amp;amp;\ell(y,\b{\hat y})=-\log\sm(\b{\hat y})_y
\end{align*}\right.\tag{2}
$$
$(1)$ is a special case of $(2)$ because take
$$
W=\mat{\b 0\\ \b w^\top},\b b=\mat{0\\b}
$$
then
$$
\sm(W\b x+\b b)=\sm\left(\mat{0\\ \b w^\top\b x+b}\right)=\mat{\frac{1}{1+\e^{\b w^\top\b x+b}}\\ \frac{\e^{\b w^\top\b x+b}}{1+\e^{\b w^\top\b x+b}}}=\mat{1-\sigma(\b w^\top\b x+b)\\ \sigma(\b w^\top\b x+b)}
$$&lt;/p&gt;
&lt;h5 id=&#34;other-methods&#34;&gt;Other Methods
&lt;/h5&gt;&lt;p&gt;In linear regression, if we change the loss function to, say $\ell(y,\hat y)=|y-\hat y|$, then no closed-form solution exists.&lt;/p&gt;
&lt;p&gt;In logistic regression, if we change the loss function to $\ell(y,\hat y)=\max\set{1-y\hat y,0}$ called hinge loss, it can be dealt with SVM.&lt;/p&gt;
&lt;p&gt;Simply change $\b x_i^\prime=\mat{\b x_i\\1}$ to, like $\b x_i^\prime=\mat{\b x_i^{\circ 2}\\ \b x_i\\1}$ or more, we get &amp;ldquo;non-linear regression&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Kernel method:
$$
f(\b x;\b\theta)=\sum_{i=1}^N\b\theta_iK(\b x,\b x_i)
$$
Nearest neighbor method:
$$
f(\b x;\b\theta)=\frac1k\sum_{i\in\text{nearest-}k(\b x)}\b y_i
$$&lt;/p&gt;
&lt;h5 id=&#34;probabilistic-interpretation&#34;&gt;Probabilistic Interpretation
&lt;/h5&gt;&lt;p&gt;Actually, the loss functions are not chosen arbitrarily. The things above are derived from maximum likelihood estimation (MLE).&lt;/p&gt;
&lt;p&gt;We propose some distribution scheme for $p(\b y\mid\b x;\b\theta)$, so our prediction is $\b{\hat y}=\operatorname{argmax}_{\b y}p(\b y\mid\b x;\b\theta)$, and
$$
\ell=-\log\prod_{i=1}^Np(\b y_i\mid\b x_i;\b\theta)=-\sum_{i=1}^N\log p(\b y_i\mid\b x_i;\b\theta)
$$
For linear regression, we assume
$$
y\sim\mathcal{N}(\b w^\top\b x+b,\sigma^2)
$$&lt;/p&gt;
&lt;p&gt;$$
\ell=-\sum\log\left[\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y-\b w^\top\b x)^2}{2\sigma^2}\right)\right]=\frac{1}{2\sigma^2}\boxed{\sum(y_i-\b w^\top\b x_i)^2}-N\log\sqrt{2\pi\sigma^2}
$$&lt;/p&gt;
&lt;p&gt;For logistic regression,
$$
y\sim\text{Ber}(\sigma(\b w^\top\b x+b)),\b y\sim\text{Cat}(\sm(W\b x+\b b))
$$
You can verify on your own.&lt;/p&gt;
&lt;p&gt;So one more question is that where comes the cross-entropy function. We extend the Bernoulli distribution to a continuous version, so that if $y\in\set{0,1}$, the distribution degenerates into the original Bernoulli distribution:
$$
p(y;\theta)\propto\theta^y(1-\theta)^{1-y}
$$
For the multiclass case, we treat it as a binomial case, where the correct $y_i$ is $1$ and the others are $0$. Or, we can also consider a continuous categorical distribution
$$
p(\b y;\b\theta)\propto\prod\b\theta_i^{\b y_i}
$$
Anyway, since the label $y_i$ is converted into one-hot vector, both of them degenerate into $-\log\sm(W\b x_i+\b b)_{y_i}$.&lt;/p&gt;
&lt;h5 id=&#34;one-more-thing&#34;&gt;One More Thing
&lt;/h5&gt;&lt;p&gt;In multiclass NN, if we don&amp;rsquo;t put sigmoid in the final layer ($\b a=\b z$) and use quadratic loss function, we get
$$
\frac{\p\ell}{\p\b z}=\frac12\frac{\p\|\b a-\b e_y\|^2}{\p\b z}=\b a-\b e_y
$$
If we use both sigmoid ($\b a=\sigma(\b z)$) and sum of cross-entropy loss on all classes, we get
$$
\frac{\p\ell}{\p\b z}=-\frac{\p(\log\sigma(\b z_y)+\sum_{j\ne y}\log(1-\sigma(\b z_j)))}{\p\b z}=-\frac{\sigma^\prime(\b z_y)}{\sigma(\b z_y)}\b e_y+\sum_{j\ne y}\frac{\sigma^\prime(\b z_j)}{1-\sigma(\b z_j)}\b e_j=\b a-\b e_y
$$
If we use softmax ($\b a=\sm(\b z)$) and cross-entropy loss of the desired class, we get
$$
\frac{\p\ell}{\p\b z}=-\frac{\log(\e^{\b z_y}/\sum_j\e^{\b z_j})}{\p\b z}=-\b e_y+\sum_i\frac{\e^{\b z_i}\b e_i}{\sum_j\e^{\b z_j}}=\b a-\b e_y
$$
I guess it&amp;rsquo;s not a coincidence.&lt;/p&gt;
&lt;p&gt;Of course it&amp;rsquo;s not! In D2L the phenomenon is explained:&lt;/p&gt;
&lt;p&gt;Let $\b\eta$ be the result of the final layer of NN (i.e. $\b z$). We model the distribution by exponential family:
$$
p(\b x;\b\eta)=h(\b x)\exp\left(\b\eta^\top T(\b x)-A(\b\eta)\right)
$$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;notation&lt;/th&gt;
&lt;th&gt;meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$\b\eta$&lt;/td&gt;
&lt;td&gt;parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$h$&lt;/td&gt;
&lt;td&gt;base measure&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$T$&lt;/td&gt;
&lt;td&gt;sufficient statistics&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$A$&lt;/td&gt;
&lt;td&gt;cumulant function&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we can simply let $T(\b x)=\b x$, or $T(x)=\b e_x$. And $A$ is just for normalization:
$$
A(\b\eta)=\log\int h(\b x)\exp\left(\b\eta^\top T(\b x)\right)\d\b x
$$
Now fix the loss function:
$$
\ell(\b\eta)=-\log p(\b x;\b\eta)
$$
Plug in the ground data $\b x_0$ and calculate the gradient:
$$
\begin{align*}
\nabla\ell&amp;amp;=-\nabla(\b\eta^\top T(\b x_0)-A(\b\eta))\\
&amp;amp;=-T(\b x_0)+\frac{1}{\exp(A(\b\eta))}\int h(\b x)\exp\left(\b\eta^\top T(\b x)\right)T(\b x)\d\b x\\
&amp;amp;=-T(\b x_0)+\int p(\b x;\b\eta)T(\b x)\d x\\
&amp;amp;=\E_{\b x}(T(\b x);\b\eta)-T(\b x_0)
\end{align*}
$$
Looking back at multiclass NNs. Both no-sigmoid + quadratic-loss and softmax + cross-entropy-loss fall into the exponential family (they&amp;rsquo;re strange because the border between post-processing after the final NN layer and operations in the loss function is blurred). Now, $\E_{\b x}(T(\b x);\b\eta)$ is exactly the NN estimate, closely related to $\b\eta$.&lt;/p&gt;
&lt;h5 id=&#34;other-stuff-in-lecture-2&#34;&gt;Other Stuff in Lecture 2
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Underfitting &amp;amp; overfitting &amp;amp; regularization&lt;/li&gt;
&lt;li&gt;Train, validation, test data; cross validation&lt;/li&gt;
&lt;li&gt;Unsupervised learning: k-means, PCA&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deep-learning&#34;&gt;Deep Learning
&lt;/h3&gt;&lt;p&gt;If you are new to neural networks, please first read &lt;a class=&#34;link&#34; href=&#34;http://neuralnetworksanddeeplearning.com/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;http://neuralnetworksanddeeplearning.com/&lt;/a&gt;. I&amp;rsquo;ll omit the things covered in this book.&lt;/p&gt;
&lt;h4 id=&#34;neural-network&#34;&gt;Neural Network
&lt;/h4&gt;&lt;h5 id=&#34;shape-convention&#34;&gt;Shape Convention
&lt;/h5&gt;&lt;p&gt;In ML, for scalar $L$, we define $\p L/\p x$ to have the shape the same as $x$. So for $W_{n\times m}$, $\p \ell/\p W\notin\R^{1\times(nm)}$, but $\p \ell/\p W\in\R^{n\times m}$.&lt;/p&gt;
&lt;p&gt;Actually we can also define the shape when the output of $L$ is vector, using tensor, but here we just leave it out.&lt;/p&gt;
&lt;p&gt;Sometimes to indicate we are using shape convention explicitly, we may use $\nabla_{W}\ell$, so below I&amp;rsquo;ll follow this notation.&lt;/p&gt;
&lt;h5 id=&#34;ways-of-deriving-backpropagation&#34;&gt;Ways of Deriving Backpropagation
&lt;/h5&gt;&lt;p&gt;Basically, since the chain rule only holds (literally) under usual calculus convention, we must first work out $\p\ell/\p x$ as a single row vector, then change the shape. For $x$ being a column vector, simply transpose the result.&lt;/p&gt;
&lt;p&gt;In ML, usually what we meet is like $\ell=f(\b z)$, $\b z=W\b x$, where $\b z$, $\b x$ are column vectors. Here we can derive the formula for this case, to be used as a shortcut.&lt;/p&gt;
&lt;p&gt;Let $\b\delta=\p\ell/\p\b z$ be a row vector, then
$$
\frac{\p\ell}{\p W_{i,j}}=\frac{\p\ell}{\p\b z}\frac{\p\b(W\b x)}{\p W_{i,j}}=\b\delta(\b e_i\b x_j)=\b\delta_i\b x_j\implies\nabla_W\ell=\b\delta^\top\b x^\top
$$
Similarly when $\b z=\b xW$, $\nabla_W\ell=\b x^\top\b\delta$.&lt;/p&gt;
&lt;p&gt;A fancier-looking derivation was taught in class. Define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Kronecker/tensor product of $A\in\R^{m\times n}$ and $B\in\R^{p\times q}$:
$$
A\otimes B=\mat{A_{1,1}B&amp;amp;\cdots&amp;amp;A_{1,n}B\\ \vdots&amp;amp;\ddots&amp;amp;\vdots\\A_{m,1}B&amp;amp;\cdots&amp;amp;A_{m,n}B}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vectorization of matrix $A$: $\omat(A)=$ stacking its columns together to form a $(mn)\times 1$ column vector, and the inverse operator $\ovec$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Lemma: $(A\otimes B)^\top=A^\top\otimes B^\top$&lt;br&gt;
Lemma: $\ovec(ABC)=(C^\top\otimes A)\ovec(B)$.&lt;br&gt;
懒得证了. Basically a bunch of integer division, modulo, and swapping summations.&lt;/p&gt;
&lt;p&gt;Here we consider $\b z_{i+1}=\sigma(W_i\b z_i+\b b_i)$. Assume $\b\delta_{i+1}=\nabla_{\b z_{i+1}}\ell$ is known.
$$
\begin{align*}
\nabla_{W_i}\ell&amp;amp;=\omat\left(\left(\frac{\p\ell}{\p\ovec(W_i)}\right)^\top\right)\\
&amp;amp;=\omat\left(\left(\frac{\p\ell}{\p\b z_{i+1}}\cdot\frac{\p\b z_{i+1}}{\p(W_i\b z_i+\b b_i)}\cdot\frac{\p(W_i\b z_i+\b b_i)}{\p\ovec(W_i)}\right)^\top\right)\\
&amp;amp;=\omat\left(\left(\b\delta_{i+1}^\top\cdot\operatorname{diag}(\sigma^\prime(W_i\b z_i+\b b_i))\cdot(\b z_i^\top\otimes I)\right)^\top\right)\\
&amp;amp;=\omat\left((\b z_i\otimes I)\cdot(\sigma^\prime(W_i\b z_i+\b b_i)\circ\b\delta_{i+1})\right)\\
&amp;amp;=\left(\sigma^\prime(W_i\b z_i+\b b_i)\circ\b\delta_{i+1}\right)\cdot\b z_i^\top
\end{align*}
$$&lt;/p&gt;
&lt;h5 id=&#34;comments-on-mnist&#34;&gt;Comments On MNIST
&lt;/h5&gt;&lt;p&gt;For FCN with &lt;code&gt;lr=0.1&lt;/code&gt;, &lt;code&gt;momentum=0.9&lt;/code&gt;, &lt;code&gt;hiddensize=500&lt;/code&gt;: &lt;code&gt;batchsize&lt;/code&gt; $256$ better than $1024$ better than $64$. Personal best is $98.1%$ on test.&lt;/p&gt;
&lt;p&gt;I found no overfitting on MNIST (in test accuracy), but test loss has an observable increase (for both FCN and CNN). The craziest part is that when &lt;code&gt;batchsize=64&lt;/code&gt;, test loss can spike above $6$. Why does this happen?&lt;/p&gt;
&lt;h4 id=&#34;convolutional-network&#34;&gt;Convolutional Network
&lt;/h4&gt;&lt;p&gt;For tasks involving identifying features from an image based on grid structure, a fully connected network doesn&amp;rsquo;t exploit spatial structure and has too many parameters.&lt;/p&gt;
&lt;p&gt;Note that vanishing gradient problem still occurs in CNN. It is ReLU that alleviates this problem.&lt;/p&gt;
&lt;p&gt;Typically there are the following layers in a CNN:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Convolution layer. We have a convolution kernel, usually having really small size, and
$$
z_{i,j}^{(l+1,d)}=\sum_c\sum_{p=0}^{s-1}\sum_{q=0}^{s-1}W_{k,l}^{(l,c,d)}z_{i+k,j+l}^{(l,c)}+b^{(l,d)}
$$
Note that there&amp;rsquo;s a different kernel for every pair of channels.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pooling layer. This is simply for reducing the size.
$$
z_{i,j}^{(l+1,c)}=\max_{p=si}^{si+s-1}\max_{q=sj}^{sj+s-1}z_{p,q}^{(l,c)}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nonlinear layer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fully connected layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A few more words on convolutional layer. First,
$$
\text{\# of parameters}=(\text{\# of input channels}\times\text{kernel size}^2+1)\times\text{\# of output channels}
$$
Second, we have stride and padding for convolutional layer. If the original size of some dimension is $n$, then the output size is
$$
\left\lfloor\frac{n+\text{padding}-\text{kernel}}{\text{stride}}\right\rfloor+1
$$
On backpropagation of convolutional layer, we first write convolution as, like
$$
z^{(l+1,\cdot)}\gets\mat{
w_1&amp;amp;w_2&amp;amp;0&amp;amp;w_3&amp;amp;w_4&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;w_1&amp;amp;w_2&amp;amp;0&amp;amp;w_3&amp;amp;w_4&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;w_1&amp;amp;w_2&amp;amp;0&amp;amp;w_3&amp;amp;w_4&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;w_1&amp;amp;w_2&amp;amp;0&amp;amp;w_3&amp;amp;w_4\\
}z^{(l,\cdot)}+\mat{b\\b\\b\\b}
$$
where $W=\mat{w_1&amp;amp;w_2\\w_3&amp;amp;w_4}$ is the kernel. Since we know that, for $\b z_{l+1}=W_l\b z_l+\b b_l$,
$$
\nabla_{\b z_l}\ell=\left(\frac{\p\ell}{\p\b z_l}\right)^\top=\left(\frac{\p\ell}{\p\b z_{l+1}}W_l\right)^\top=W_l^\top\nabla_{\b z_{l+1}}\ell
$$
so, examine
$$
\begin{align*}
g^{(l,\cdot)}&amp;amp;=\mat{w_1&amp;amp;0&amp;amp;0&amp;amp;0\\ w_2&amp;amp;w_1&amp;amp;0&amp;amp;0\\ 0&amp;amp;w_2&amp;amp;0&amp;amp;0\\  w_3&amp;amp;0&amp;amp;w_1&amp;amp;0\\ w_4&amp;amp;w_3&amp;amp;w_2&amp;amp;w_1\\ 0&amp;amp;w_4&amp;amp;0&amp;amp;w_2\\ 0&amp;amp;0&amp;amp;w_3&amp;amp;0\\0&amp;amp;0&amp;amp;w_4&amp;amp;w_3\\ 0&amp;amp;0&amp;amp;0&amp;amp;w_4}g^{(l+1,\cdot)}\\ &amp;amp;=
\mat{
w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1&amp;amp;0\\
0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;0&amp;amp;w_4&amp;amp;w_3&amp;amp;0&amp;amp;0&amp;amp;w_2&amp;amp;w_1\\
}\mat{0\\0\\0\\0\\0\\ g^{(l+1,\cdot)}_1\\ g^{(l+1,\cdot)}_2\\0\\0\\ g^{(l+1,\cdot)}_3\\ g^{(l+1,\cdot)}_4\\0\\0\\0\\0\\0}
\end{align*}
$$
so we find that, backpropagation through the convolution layer is just convolution by flipped filter after padding.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-aipt/CNNback.png&#34;
	width=&#34;891&#34;
	height=&#34;474&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;187&#34;
		data-flex-basis=&#34;451px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;tricks&#34;&gt;Tricks
&lt;/h4&gt;&lt;h5 id=&#34;gradient-descent&#34;&gt;Gradient Descent
&lt;/h5&gt;&lt;p&gt;$$
\b\theta^\prime=\b\theta-\eta\nabla_{\b\theta}\ell(\b\theta)
$$&lt;/p&gt;
&lt;p&gt;For stochastic gradient descent, from mathematical point of view rather than &amp;ldquo;minibatch&amp;rdquo; point of view,
$$
\b\theta^\prime=\b\theta-\eta(\nabla_{\b\theta}\ell(\b\theta)+\b\xi)
$$&lt;/p&gt;
&lt;h5 id=&#34;relu&#34;&gt;ReLU
&lt;/h5&gt;&lt;p&gt;Vanishing gradient problem is caused by both small derivative value of sigmoid and small (initial) parameters, while exploding gradient problem is only caused by large parameters. ReLU can only solve the small derivative problem. However, it introduces dying ReLU problem. To tackle this, people invented leaky ReLU, SiLU, etc.&lt;/p&gt;
&lt;h3 id=&#34;planning-and-rl&#34;&gt;Planning and RL
&lt;/h3&gt;&lt;p&gt;Previously, all the ML models are about producing knowledge, but now we enter the field of interacting with the environment.&lt;/p&gt;
&lt;h4 id=&#34;search&#34;&gt;Search
&lt;/h4&gt;&lt;p&gt;The simplest, deterministic way of planning, is search.&lt;/p&gt;
&lt;p&gt;A search problem consists of state space $S$, start state $s_{\rm start}$, actions space $A$, successors (of actions), cost (of actions), and goal test. A solution is a sequence of actions that transform $s_{\rm start}$ to a state that satisfies the goal condition.&lt;/p&gt;
&lt;h5 id=&#34;a-search&#34;&gt;A* Search
&lt;/h5&gt;&lt;p&gt;The only reason why A* is better than BFS/Dijkstra is A* doesn&amp;rsquo;t explore all the vertices.&lt;/p&gt;
&lt;p&gt;A* is simply Dijkstra, changing the key of $u$ in the priority queue from $d_u$ to $d_u+e_u$, where $e_u$ is the estimated distance from $u$ to the goal, and outputting the answer when some goal is extracted from the priority queue. It&amp;rsquo;s easy to see, for $e_u=0$, it&amp;rsquo;s Dijkstra; for $e_u=\operatorname{dis}(u,t)$, only the vertices on the shortest path from $s$ to $t$ (the nearest goal) will be explored.&lt;/p&gt;
&lt;p&gt;From here we follow the convention taught in class: rename $d_u$ as $f(u)$, and $e_u$ as $h(u)$ (called a heuristic), vertices as states, and length as cost. There are three A* variants under different assumptions on $h(u)$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No constraint. A* might be incorrect.&lt;/li&gt;
&lt;li&gt;(admissible) $0\le h(u)\le h^{*}(u)$ where $h^{*}(u)=\min_{\text{goal }t}\operatorname{dis}(u,t)$. In this case some $u$ may be pushed into/popped from queue multiple times, but the lowest cost is ensured to be found because every state nearer from the start state than any goal will be expanded before the goal. We say in this case, tree search can solve the problem.&lt;/li&gt;
&lt;li&gt;(consistent) $h(u)\le h(v)+w(u,v)$. In this case we can prove that when $u$ is popped the first time, the cost is optimal. We say in this case, graph search can solve the problem.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Obviously consistent $\implies$ admissible. We can see the stupid heuristic as setting $w^\prime(u,v)=w(u,v)-h(u)+h(v)$, then consistency condition is just nonnegative weights. So A* is explained in one sentence, but non-TCS guys use an entire session to talk about it with vague expressions. I can&amp;rsquo;t stand it.&lt;/p&gt;
&lt;h4 id=&#34;mdp&#34;&gt;MDP
&lt;/h4&gt;&lt;h5 id=&#34;definition&#34;&gt;Definition
&lt;/h5&gt;&lt;p&gt;When facing the real world, there are uncertainties. When taking an action $a$ at state $s$, the next state is not deterministic, but follows a probability distribution, i.e.
$$
\Pr[s^\prime\mid s,a]=T(s,a,s^\prime)
$$
and everything else are the same as search, except the cost function is replaced by reward function $R(\text{state})$ (and the goal is to maximize it). It&amp;rsquo;s called Markov Decision Process. It has the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First-order Markovian dynamics (history independence). $\Pr(s_{t+1}\mid s_t,a_t,s_{t-1},a_{t-1},\cdots,s_0)=\Pr(s_{t+1}\mid s_t,a_t)$.&lt;/li&gt;
&lt;li&gt;Reward is deterministic.&lt;/li&gt;
&lt;li&gt;The probability distribution does not depend on time.&lt;/li&gt;
&lt;li&gt;We can observe $s_{t+1}$ after it&amp;rsquo;s sampled.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the first-order Markovian dynamics does not mean that the process is necessarily independent of the &amp;ldquo;history&amp;rdquo; because we can let ($\circ$ means concatenation)
$$
s^\prime=s\circ a\circ\Delta\text{ w.p. }T(s,a,\Delta)
$$
But usually it&amp;rsquo;s not the case in practice.&lt;/p&gt;
&lt;p&gt;In the agent&amp;rsquo;s point of view, there&amp;rsquo;s an additional concept called &amp;ldquo;observation&amp;rdquo;. The observation space is denoted as $\Omega$, and the observation $o\in\Omega$ is determined by the action and the succeeding state:
$$
\text{there is a distribution }\Pr[o\mid s^\prime,a]
$$
A history $h_t$ is defined as
$$
h_t=(a_0,o_1,R_1,a_1,o_2,R_2,\cdots,a_{t-1},o_t,R_t)
$$
and the &lt;strong&gt;internal&lt;/strong&gt; state of the agent is defined as a summary of experience
$$
\text{internal state}=f(h_t)
$$
and the internal state is also called $h_t$. Usually $f$ will not keep all the information. Think of, for example, LSTM. Furthermore, we call &amp;ldquo;fully observed environment&amp;rdquo;, when the true state can be inferred from the observation. In this case, clearly, we don&amp;rsquo;t need to distinguish the true state and the internal state. On the flip side, if the environment is not fully observed, we call the process a &amp;ldquo;partially observed Markov decision process&amp;rdquo; (POMDP). Then conditioned on the current internal state, the agent can form a &amp;ldquo;guess&amp;rdquo; on the true state, that is a &amp;ldquo;belief state&amp;rdquo; $b_t=b(h_t)\in\Delta(S)$.&lt;/p&gt;
&lt;p&gt;Some real-world examples for better understanding:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go. Fully observed.&lt;/li&gt;
&lt;li&gt;Poker (德扑). The optimal decision requires you to remember past actions, bets, and the number of players who folded, etc.&lt;/li&gt;
&lt;li&gt;Robot navigation. If there&amp;rsquo;s a map/GPS, then it is fully observed. Otherwise, it&amp;rsquo;s partially observed and the robot needs to use its history to reduce uncertainty.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are two types of problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Planning, the dynamics model (probability distribution) is known.&lt;/li&gt;
&lt;li&gt;RL, not known.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;theoretical-guarantee&#34;&gt;Theoretical Guarantee
&lt;/h5&gt;&lt;p&gt;To solve an MDP, we want to find a policy $\pi:S\to A$ or $\pi:S\to \Delta(A)$. There are two types of policies:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-stationary policy. The action also depends on the steps to go.&lt;/li&gt;
&lt;li&gt;Stationary policy. There are infinitely many future steps.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the four properties of an MDP still hold.&lt;/p&gt;
&lt;p&gt;For infinite horizon case, to evaluate a policy, we define a value function:
$$
V_\pi(s)=\E\left(\sum_{t\ge 0}\gamma^tR(s_t)\middle|\pi,s\right)
$$
or
$$
V_\pi(s)=R(s)+\gamma\E(V_\pi(s^\prime)\mid s,\pi(s))\tag{Bellman equation}
$$
Maybe not that ideal, just compromise on convergence.&lt;/p&gt;
&lt;p&gt;We claim that&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;. $\exists\pi$, $\forall s$, $\forall \pi^\prime$, $V_\pi(s)\ge V_{\pi^\prime}(s)$, that is, such &amp;ldquo;best&amp;rdquo; policy exists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;. Here we define $V_{*}$ recursively:
$$
V_{*}(s)=R(s)+\gamma\max_a\E(V_{*}(s^\prime)\mid s,a)
$$
&lt;strong&gt;Lemma&lt;/strong&gt;. $V_{*}$ uniquely exists, and $\forall s$, $V_{*}(s)=\max_\pi V_\pi(s)$.&lt;/p&gt;
&lt;p&gt;Then we construct
$$
\pi_{*}(s)=\operatorname*{argmax}_a\E(V_{*}(s^\prime)\mid s,a)
$$
Then such $\pi_{*}$ satisfies the condition.&lt;/p&gt;
&lt;p&gt;Now we prove the lemma. Consider the Bellman operator $\B:\R^{|S|}\to\R^{|S|}$:
$$
\B V(s)=R(s)+\gamma\max_a\E(V(s^\prime)\mid s,a)
$$
$$
\begin{align*}
\lVert\B V_1-\B V_2\rVert_{\infty}
&amp;amp;=\gamma\max_s\left\lvert\max_a\E(V_1(s^\prime)\mid s,a)-\max_a\E(V_2(s^\prime)\mid s,a)\right\rvert\\
&amp;amp;\le\gamma\max_s\max_a\left\lvert\E(V_1(s^\prime)\mid s,a)-\E(V_2(s^\prime)\mid s,a)\right\rvert\\
&amp;amp;\le\gamma\max_s\max_a\sum_{s^\prime}\Pr[s^\prime\mid s,a]\left\lvert V_1(s^\prime)-V_2(s^\prime)\right\rvert\\
&amp;amp;\le\gamma\max_{s^\prime}\left\lvert V_1(s^\prime)-V_2(s^\prime)\right\rvert\\
&amp;amp;=\gamma\|V_1-V_2\|_{\infty}
\end{align*}
$$
So by the Banach fixed-point theorem, there exists unique $V_{*}$ such that $\B V_{*}=V_{*}$. Also a corollary is linear convergence.&lt;/p&gt;
&lt;p&gt;If for some $V_\pi$, $V_\pi(s)&amp;gt;V_{*}(s)$, then $V_\pi$ is not a fixed point, and $\exists s^\prime$, $\B V_\pi(s^\prime)&amp;gt;V_\pi(s^\prime)$. If we only change $\pi(s^\prime)$, we&amp;rsquo;ll get a strictly better policy because the probability served as coefficient in the Bellman equation is nonnegative. Here we assume the policy space is finite, so we&amp;rsquo;ll reach a contradiction.&lt;/p&gt;
&lt;h5 id=&#34;value-iteration&#34;&gt;Value Iteration
&lt;/h5&gt;&lt;p&gt;Just repeatedly calculate $V$. By the Banach fixed-point theorem,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\|V_k-V_{*}\|\le\gamma^kR_{\max}/(1-\gamma)$&lt;/li&gt;
&lt;li&gt;$\|V_k-V_{k-1}\|\le\eps\implies\|V_k-V_{*}\|\le\eps\gamma/(1-\gamma)$.&lt;/li&gt;
&lt;li&gt;Let $V_g$ be the value of $\pi_V$ constructed by $\operatorname{argmax}$, then $\|V-V_{*}\|\le\eps\implies\|V_g-V_{*}\|\le2\eps\gamma/(1-\gamma)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;policy-iteration&#34;&gt;Policy Iteration
&lt;/h5&gt;&lt;p&gt;Each iteration, compute $V_\pi$ by current $\pi$ then update $\pi$ by $V_\pi$ (still, $\operatorname{argmax}$), until $\pi$ fixes.&lt;/p&gt;
&lt;p&gt;Since
$$
\max_a\E(V_\pi(s^\prime)\mid s,a)\ge\E(V_\pi(s^\prime)\mid s,\pi(s))
$$
We can iteratively use this inequality to show $\pi^\prime$ must improve over $\pi$ (if equality holds, it means the fix-point is reached). In theory, the algorithm might run through every policy then result in exponential time, but it&amp;rsquo;s not the case in practice.&lt;/p&gt;
&lt;p&gt;How to calculate $V_\pi$? We need to solve a linear equation system.&lt;/p&gt;
&lt;p&gt;In short, value iteration needs more iterations than policy iteration empirically. Value iteration is $O(S^2A)$, while policy iteration is $O(S^3)$ but can be reduced to $O(kS^2)$: just like value iteration, calculate $V_\pi$ by a few iterations. This is called modified policy iteration.&lt;/p&gt;
&lt;h5 id=&#34;other-tricks&#34;&gt;Other Tricks
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;In-place value iteration. Every time randomly choose an $s$ and update $V(s)$.&lt;/li&gt;
&lt;li&gt;Prioritized sweeping. Every time select $s$ s.t. $\delta(s)=\left\lvert R(s)+\max_a\E(V(s^\prime)\mid s,a)-V(s)\right\rvert$ is the largest. Need to reversely update those $\delta(s_{\rm pre})$ such that $\Pr[s\mid s_{\rm pre},a_{\rm best}]&amp;gt;0$. Use priority queue to maintain.&lt;/li&gt;
&lt;li&gt;Run the policy, use agent’s experience to guide the selection of $s$.&lt;/li&gt;
&lt;li&gt;When $V$ requires too much memory, we can use NN to fit $V$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mdp-variations&#34;&gt;MDP Variations
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Infinite MDP: countably infinite states/actions; continuous state/action; continuous time.&lt;/li&gt;
&lt;li&gt;POMDP: can be reduced to an (infinite) history tree, or an continuous MDP with belief states.&lt;/li&gt;
&lt;li&gt;Ergodic MDP. A few words is needed here.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An ergodic MP is an irreducible, aperiodic MP. An ergodic MDP is an MDP whose MP induced by any policy is ergodic. By 计智应数, an ergodic MP has a unique stationary distribution. Thus the following quantity is well-defined w.r.t. a policy $\pi$:
$$
\rho_\pi=\lim_{T\to\infty}\frac1T\E\left(\sum_{t=1}^TR_t\right)
$$
it&amp;rsquo;s called the average reward per time-step.&lt;/p&gt;
&lt;p&gt;The value function of an undiscounted ($\gamma=1$), ergodic MDP can be expressed in terms of average reward.
$$
\tilde V_\pi(s)=\E\left(\sum_{t\ge 1}(R_t-\rho_\pi)\middle| s_0=s\right)
$$
by theory of mixing time, $R_t-\rho^\pi=\Omicron(\lambda^t)$ for some $0&amp;lt;\lambda&amp;lt;1$, so the infinite sum converges.&lt;/p&gt;
&lt;p&gt;Bellman equation form:
$$
\tilde V_\pi(s)=\E((R^\prime-\rho_\pi)+\tilde V_\pi(s^\prime)\mid s)
$$&lt;/p&gt;
&lt;h3 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning
&lt;/h3&gt;&lt;p&gt;RL is MDP, minus the knowledge about transition $T$ and/or reward $R$.&lt;/p&gt;
&lt;p&gt;In RL, a learning algorithm is called an agent. There are two main attributes of an agent that we differentiate, that are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Passive learning vs. active learning. Passive learning is just following a fixed policy, and try to estimate the utility of the policy $V_\pi(\cdot)$. Active learning select the policy. While it aims at finding the optimal policy, it utilize the current policy to explore, that is, gaining information about the MDP that guides the further policy selection.&lt;/li&gt;
&lt;li&gt;Model-based vs. model-free. A model-based agent explicitly stores a representation of $T$ and $R$, then the overall problem is more like MDP previously introduced, and may be solved by, e.g. policy iteration. A model-free agent learns $V$ or $\pi$ directly from trial-and-error experience with the environment, without explicitly learning the model (MDP).&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Model-Free&lt;/th&gt;
&lt;th&gt;Model-Based&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Passive Learning&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;MC&lt;/td&gt;
&lt;td&gt;ADP, TD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Active Learning&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Q-Learning, SARSA, DQN, PPO, SAC&lt;/td&gt;
&lt;td&gt;Dyna-Q&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;passive-learning&#34;&gt;Passive Learning
&lt;/h4&gt;&lt;p&gt;From here on, $T$, $R$, and $V$ all refer to estimated values; formally they should be written as $\hat T$, $\hat R$, and $\hat V$.&lt;/p&gt;
&lt;p&gt;Now a policy $\pi$ to be evaluated is given.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Direct estimation, or Monte Carlo (MC). Just run a lot of episodes (each time act according to $\pi$ until a terminal state is reached), and average reward-to-go. It converges but very slowly. It doesn&amp;rsquo;t utilize Bellman equation.&lt;/li&gt;
&lt;li&gt;Adaptive dynamic programming (ADP). It follows $\pi$ for a while, to get $T$ and $R$, then solve for $V_\pi(\cdot)$. Correctness: can bound error with Chernoff bounds.&lt;/li&gt;
&lt;li&gt;Temporal difference learning (TD). The idea is to do local updates of the value function on a per-action basis, instead of learning the model and solving the Bellman equation at the end. Details are as follows:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Consider updating the value function in some way:
$$
V_\pi^\prime(s)\gets\begin{cases}V_\pi(s)&amp;amp;(\text{old})\\R(s)+\gamma\E(V_\pi(s^\prime)\mid s,\pi(s))&amp;amp;(\text{new})\end{cases}
$$
Here $\text{new}$ is obtained by sampling:
$$
\text{new}=R(s)+\gamma V_\pi(s^\prime),s^\prime\sim T(s,\pi(s),\cdot)
$$
so that $\E(\text{new})=R(s)+\gamma\E(V_\pi(s^\prime)\mid s,\pi(s))$.&lt;/p&gt;
&lt;p&gt;To guarantee some kind of stability, we set a &amp;ldquo;learning rate&amp;rdquo; $\alpha$, then
$$
V_\pi^\prime(s)=(1-\alpha)(\text{old})+\alpha(\text{new})
$$
Thus
$$
V^\prime_\pi(s)=V_\pi(s)+\alpha(R(s)+\gamma V_\pi(s^\prime)-V_\pi(s))
$$
With appropriately decreasing $\alpha$, one can prove convergence.&lt;/p&gt;
&lt;h4 id=&#34;active-learning&#34;&gt;Active Learning
&lt;/h4&gt;&lt;p&gt;Now we need to find a good policy $\pi$.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;s a main challenge called the Exploration-Exploitation Dilemma.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC-based naïve approach. Explore (take random action each time) for a long long time to learn $T$ and $R$, then turn the problem into MDP problem. This is one extreme of complete exploration.&lt;/li&gt;
&lt;li&gt;ADP-based naïve approach. Every cycle, follow the policy $\pi$ solved by the previous cycle for a while and estimate $T$ and $R$, then compute $V_\pi$ by the gained information. Finally compute the new policy $\pi^\prime$ by $\argmax$. This is the other extreme of complete exploitation. It is usually incorrect because it can easily get stuck in local minima.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;glie&#34;&gt;GLIE
&lt;/h5&gt;&lt;p&gt;We can modify &amp;ldquo;following the previously obtained greedy policy&amp;rdquo; to avoid getting stuck:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each step with probability $1-p(t)$ select the greedy action, and with probability $p(t)$ act randomly. $p(t)=1/t$.&lt;/li&gt;
&lt;li&gt;Select action according to Boltzmann distribution $\Pr[a\mid s]=\exp(Q(s,a)/T)/\sum_{a^\prime}\exp(Q(s,a^\prime)/T)$.&lt;/li&gt;
&lt;li&gt;Optimistic exploration. Assign the highest possible value to any state-action pair that has not been explored enough. Under this framework, there is an algorithm called $\text{Rmax}$. Specifically, it redirects the transition of any state-action pair that has been visited fewer than $N_e$ times to itself and sets the reward to $R_{\max}$ (so its $Q$ value is $V_{\max}=R_{\max}/(1-\gamma)$). $\text{Rmax}$ has a theoretical convergence guarantee called a PAC guarantee.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These exploration policies are all so-called GLIE, greedy in the limit of infinite exploration.&lt;/p&gt;
&lt;h5 id=&#34;q-learning&#34;&gt;Q-Learning
&lt;/h5&gt;&lt;p&gt;Notice that there comes a new value function $Q$:
$$
Q(s,a):=R(s)+\gamma\E\left(\max_{a^\prime}Q(s^\prime,a^\prime)\middle|s,a\right)
$$
It&amp;rsquo;s especially useful for optimizing TD-based active RL. When trying to analogize TD-based active RL to ADP-based active RL, it seems like the only thing to be changed is the updating process of $V_\pi$. However the greedy policy requires $\argmax_a\set{\cdots}$, where $T$ and $R$ are needed. So TD-based active RL is no longer model-free. In order to prevent learning the model, we should directly estimate $Q$ instead of $V_\pi$. This leads to Q-learning. The updating step of $Q$ is similar to that in TD:
$$
Q^\prime(s,a)=Q(s,a)+\alpha\left(R(s)+\gamma\max_{a^\prime}Q(s^\prime,a^\prime)-Q(s,a)\right)
$$&lt;/p&gt;
&lt;p&gt;With the same exploration policy such as Boltzmann exploration, one can prove that with learning rate satisfying $\sum\alpha_n=\infty\land\sum\alpha_n^2&amp;lt;\infty$, Q-learning converges to the true, optimal $Q$ function.&lt;/p&gt;
&lt;p&gt;For goal-based problems where the big reward appears only in the goal state, in each episode only one more $Q(s,a)$ can see that big reward. To accelerate convergence, we can use optimizations that look like &amp;ldquo;back-propagation&amp;rdquo;, including trajectory replay and reverse updates.&lt;/p&gt;
&lt;h5 id=&#34;sarsa&#34;&gt;SARSA
&lt;/h5&gt;&lt;p&gt;Now consider another pair of attributes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Off-policy vs. on-policy. If the policy used to select the action and the policy used to evaluate the next action are the same, then such agent is called on-policy, otherwise it&amp;rsquo;s called off-policy. Q-learning is obviously off-policy, because it follows a mixture of greedy action and random action when deciding the next step (exploring), while it chooses $\max_{a^\prime}Q$ when calculating $Q$ function (exploiting).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;State-Action-Reward-State-Action (SARSA) is an on-policy type of agent similar to Q-learning. The only difference is that, after the update:
$$
Q^\prime(s,a)=Q(s,a)+\alpha\left(R(s)+\gamma Q(s^\prime,a^\prime)-Q(s,a)\right)\text{ where }a^\prime=\begin{cases}\argmax_{a^\prime}Q(s^\prime,a^\prime),&amp;amp;\text{w.p. }1-\epsilon\\ \text{an action uniformly at random},&amp;amp;\text{w.p. }\epsilon\end{cases}
$$
The action taken (giving $s^{\prime\prime}$) is exactly $a^\prime$. Notice that in Q-learning, the $a^\prime=\argmax Q$ used for update is not necessarily the actual action leading the trajectory.&lt;/p&gt;
&lt;p&gt;Also it&amp;rsquo;s almost the same as the first exploration strategy in ADP-based learning, except that $\epsilon$ is a fixed constant.&lt;/p&gt;
&lt;p&gt;SARSA learns safer policy than Q-learning. This behavior significantly manifests in the classical cliff walking problem. An easy way to understand that is to regard the $\epsilon$-greedy policy as the agent itself occasionally loses its mind and does some crazy decisions, so in order to prevent catastrophic negative reward it tends to choose a safer pathway.&lt;/p&gt;
&lt;p&gt;Note that when I say &amp;ldquo;SARSA learns safer policy&amp;rdquo;, I refer to &amp;ldquo;the output policy&amp;rdquo; of SARSA, which is still a deterministic policy obtained by taking $\pi(s)=\argmax_aQ(s,a)$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-aipt/SARSA.png&#34;
	width=&#34;974&#34;
	height=&#34;370&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;631px&#34;
	
&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;MC Passive&lt;/th&gt;
&lt;th&gt;ADP Passive&lt;/th&gt;
&lt;th&gt;TD Passive&lt;/th&gt;
&lt;th&gt;MC Active&lt;/th&gt;
&lt;th&gt;ADP Active&lt;/th&gt;
&lt;th&gt;TD Active&lt;/th&gt;
&lt;th&gt;Q-Learning&lt;/th&gt;
&lt;th&gt;SARSA&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Time/it&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;$O(kS^2)$&lt;/td&gt;
&lt;td&gt;$O(1)$&lt;/td&gt;
&lt;td&gt;?&lt;/td&gt;
&lt;td&gt;$O(kS^2)$&lt;/td&gt;
&lt;td&gt;$O(1)$&lt;/td&gt;
&lt;td&gt;$O(1)$&lt;/td&gt;
&lt;td&gt;$O(1)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Space&lt;/td&gt;
&lt;td&gt;$O(S^2A)$&lt;/td&gt;
&lt;td&gt;$O(S^2A)$&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;$O(S^2A)$&lt;/td&gt;
&lt;td&gt;$O(S^2A)$&lt;/td&gt;
&lt;td&gt;$O(S^2A)$&lt;/td&gt;
&lt;td&gt;$O(SA)$&lt;/td&gt;
&lt;td&gt;$O(SA)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Model&lt;/td&gt;
&lt;td&gt;based&lt;/td&gt;
&lt;td&gt;based&lt;/td&gt;
&lt;td&gt;free&lt;/td&gt;
&lt;td&gt;based&lt;/td&gt;
&lt;td&gt;based&lt;/td&gt;
&lt;td&gt;based&lt;/td&gt;
&lt;td&gt;free&lt;/td&gt;
&lt;td&gt;free&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;deep-reinforcement-learning&#34;&gt;Deep Reinforcement Learning
&lt;/h4&gt;&lt;h5 id=&#34;rl-with-function-approximation&#34;&gt;RL with Function Approximation
&lt;/h5&gt;&lt;p&gt;When solving real-world RL problems, we can&amp;rsquo;t afford to store the full tabular $V$ or $Q$. So we consider approximating them using kernel methods or linear approximation. Define a set of features $f_1(s)\sim f_n(s)$, and
$$
V_\theta(s)=\theta_0+\sum_{i=1}^n\theta_if_i(s)
$$
Now first we consider model-based TD. The expected &amp;ldquo;correct&amp;rdquo; $V$ is $v(s)=R(s)+\gamma V_\theta(s^\prime)$, and the current value is $V_\theta(s)$. Directly using GD with quadratic loss function, the gradient update should be
$$
\theta^\prime=\theta-\alpha\nabla_\theta\left(\frac12\left(v(s)-\theta_0-\sum_{i=1}^n\theta_if_i(s)\right)^2\right)=\theta+\alpha(R(s)+\gamma V_\theta(s^\prime)-V_\theta(s))\mat{1\\f(s)}
$$
For model-free TD, it&amp;rsquo;s also basically identical to Q-learning:
$$
Q_\theta(s,a)=\theta_0+\sum_{i=1}^n\theta_if_i(s,a)
$$&lt;/p&gt;
&lt;p&gt;$$
\theta^\prime=\theta+\alpha\left(R(s)+\gamma\max_{a^\prime}Q_\theta(s^\prime,a^\prime)-Q_\theta(s,a)\right)\mat{1\\f(s,a)}
$$&lt;/p&gt;
&lt;p&gt;Furthermore, why not use NN to approximate $Q$? (the convergence is not guaranteed, though) This leads to Deep Q-Networks, i.e. DQN.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://littlereuben.github.io/study/course-aipt/DQN.jpg&#34;
	width=&#34;946&#34;
	height=&#34;541&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;419px&#34;
	
&gt;&lt;/p&gt;
&lt;h5 id=&#34;stability-problem-and-solutions&#34;&gt;Stability Problem and Solutions
&lt;/h5&gt;&lt;p&gt;Naïve Q-learning on DNNs (with the past few frames as input) oscillates or diverges because the data is sequential and successive samples are correlated, and also correlated with the taken action. Since the loss landscape of DNNs is complicated, the policy may oscillate (between several behavior modes). Also, because there are unstable gradient issues, we need to tune hyperparameters carefully. But there&amp;rsquo;re many different environment conditions. There&amp;rsquo;re several tricks that make it more stable:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Experience replay. This is to remove correlations. Do not use the most recent observed transitions, but store millions of transitions in replay memory $D$. Every time sample mini-batches from it and optimize.&lt;/li&gt;
&lt;li&gt;Fixed target Q-Network. So we want $R(s)+\gamma\max_{a^\prime}Q_\theta(s^\prime,a^\prime)=Q_{\theta^\prime}(s,a)\cdots(*)$. Previously we make recurrently update to minimize the loss, and in the next step, the reference $Q$ is changed to $Q_{\theta^\prime}$. Now we fix the reference $\theta$ (in LHS of $(*)$) for thousands of steps and optimize $\theta^\prime$.&lt;/li&gt;
&lt;li&gt;Control the reward/value range like clipping to $[-1,+1]$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By 1 and 2, the optimization target is changed to:
$$
\begin{array}{c}
\min.\left(R(s)+\gamma\max_{a^\prime}Q_{\theta_t}(s^\prime,a^\prime)-Q_{\theta_{t+1}}(s,a)\right)^2\\
\downarrow\\
\min.\E_{(s,a,r,s^\prime)\sim U(D)}\left(r+\gamma\max_{a^\prime}Q_{\theta_t}(s^\prime,a^\prime)-Q_{\theta_{t+1}}(s,a)\right)^2\\
\downarrow\\
\min.\E_{(s,a,r,s^\prime)\sim U(D)}\left(r+\gamma\max_{a^\prime}Q_{\theta_{\lfloor t/T\rfloor T}}(s^\prime,a^\prime)-Q_{\theta_{t+1}}(s,a)\right)^2
\end{array}
$$&lt;/p&gt;
&lt;h5 id=&#34;maximization-bias-and-double-q-learning&#34;&gt;Maximization Bias and Double Q-Learning
&lt;/h5&gt;&lt;p&gt;Another problem is called the maximization bias. Recall the update formula of tabular Q-learning:
$$
Q^\prime(s,a)=Q(s,a)+\alpha\left(R(s)+\gamma\max_{a^\prime}Q(s^\prime,a^\prime)-Q(s,a)\right)
$$
One can see that
$$
\E\left(\max_ix_i\right)\ge\E(x_{*})=\max_i\E(x_i)
$$
so with randomness (even the previous $Q$ has unbiased noise), $Q$ will be overestimating. The solution is double Q-learning: Train two $Q$ functions, every time randomly choose one to update, like
$$
Q_1^\prime(s,a)=Q_1(s,a)+\alpha\left(R(s)+\gamma Q_2(s^\prime,\argmax_{a^\prime}Q_1(s^\prime,a^\prime))-Q_1(s,a)\right)
$$
It can be proven that under some assumptions, it underestimates. But the key is that because of the $\max$, underestimation is less damaging than overestimation, since the latter leads to a feedback loop, but the former cannot.&lt;/p&gt;
&lt;p&gt;When determining the next action, take $\argmax$ over $Q_1+Q_2$.&lt;/p&gt;
&lt;p&gt;The same idea can be applied to DQN. Here the old Q-network can be used as $Q_2$:
$$
\min.\left(r+\gamma Q_{\theta_{\text{old}}}(s^\prime,\argmax_{a^\prime}Q_\theta(s^\prime,a^\prime))-Q_\theta(s,a)\right)^2
$$
Combined with experience replay, here&amp;rsquo;s another trick: for multiple past experiences $\set{(s,a)}$, choose among them according to the above quadratic error, like
$$
P(i)=\frac{\mathrm{err}_i^\alpha}{\sum_k\mathrm{err}_k^\alpha}
$$
&lt;img src=&#34;https://littlereuben.github.io/study/course-aipt/DQN.png&#34;
	width=&#34;2696&#34;
	height=&#34;805&#34;
	
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;334&#34;
		data-flex-basis=&#34;803px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;This plot shows that double DQN is less biased and has lower variance. Note that the &amp;ldquo;true value&amp;rdquo; is $\E(\sum\gamma^tr_t)$ when following the policy yielded by $\argmax$.&lt;/p&gt;
&lt;h4 id=&#34;policy-based-rl&#34;&gt;Policy-Based RL
&lt;/h4&gt;&lt;p&gt;Instead of learning $V$ or $Q$ by simulating some policy for a huge number of steps, policy-based RL methods directly learn the model that determine the actions, that is, $\pi_\theta(s,a)=\Pr[a\mid s;\theta]$, learn $\theta$ by GD.&lt;/p&gt;
&lt;p&gt;There&amp;rsquo;re a few intuitive reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Often the feature-based policies that work well aren&amp;rsquo;t the ones that approximate $V$/$Q$ best.&lt;/li&gt;
&lt;li&gt;The final goal of RL is finding a good policy, which can be achieved by only getting the ordering of $Q$ values right. But Q-learning prioritizes getting $Q$ values close.&lt;/li&gt;
&lt;li&gt;$V$ or $Q$ leads to deterministic (or near-deterministic like $\epsilon$-greedy) policies, while in some problems, mixed strategy is the optimal.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The advantages of policy-based RL:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Better convergence properties: $\pi_\theta(s,a)$ changes continuously, unlike the $\argmax$ of value-based RL changing suddenly.&lt;/li&gt;
&lt;li&gt;Effective in high-dimensional or continuous action spaces.&lt;/li&gt;
&lt;li&gt;Can learn stochastic policies.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Disadvantages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Typically converge to a local optimum.&lt;/li&gt;
&lt;li&gt;Evaluation of the policy has high variance, resulting in slow convergence.&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;objectives&#34;&gt;Objectives
&lt;/h5&gt;&lt;p&gt;We assume the Markov process induced by the policy $\pi_\theta$ is ergodic, so that there exists a unique stable distribution $d_{\pi_\theta}(\cdot)$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In episodic environments like video games, the starting state matters. So we consider $J_1(\theta)=V_{\pi_\theta}(s_0)$.&lt;/li&gt;
&lt;li&gt;In continuing environments we consider the limit value $J_{\rm avV}(\theta)=\sum_sd_{\pi_\theta}(s)V_{\pi_\theta}(s)$.&lt;/li&gt;
&lt;li&gt;However if we want $\gamma=1$ then $J_{\rm avV}$ is not available. Consider $J_{\rm avR}(\theta)=\sum_sd_{\pi_\theta}(s)\sum_a\pi_\theta(s,a)R(s,a)$. When $\gamma&amp;lt;1$, $J_{\rm avV}=\frac{1}{1-\gamma}J_{\rm avR}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;finite-difference-policy-gradient&#34;&gt;Finite Difference Policy Gradient
&lt;/h5&gt;&lt;p&gt;Compute
$$
\frac{\p J(\theta)}{\p\theta_i}\approx\frac{J(\theta+\epsilon e_i)-J(\theta)}{\epsilon}
$$
Stupid.&lt;/p&gt;
&lt;h5 id=&#34;reinforce&#34;&gt;REINFORCE
&lt;/h5&gt;&lt;p&gt;Here we assume the reward depends on the state and the action.&lt;/p&gt;
&lt;p&gt;The motivation is to &amp;ldquo;sample the gradient&amp;rdquo;. To do that, extracting a $\pi_\theta(s,a)$ term from the gradient is favorable. Starting with $\nabla_\theta\pi_\theta(s,a)$:
$$
\nabla_\theta\pi_\theta(s,a)=\pi_\theta(s,a)\nabla_\theta\log\pi_\theta(s,a)
$$
An easy case is the first step. Suppose $s_0\sim d(\cdot)$:
$$
\nabla_\theta\E(R(s_0,a_0))=\nabla_\theta\sum_sd(s)\sum_a\pi_\theta(s,a)R(s,a)=\sum_sd(s)\sum_a\pi_\theta(s,a)\nabla_\theta\log\pi_\theta(s,a)R(s,a)=\E\left(\nabla_\theta\log\pi_\theta(s,a)R(s,a)\right)
$$
The key point is, $\nabla_\theta\E$ cannot be calculated by sampling, while $\E(\nabla_\theta)$ can, because $\pi_\theta(s,a)$ is usually produced by an NN, thus $\nabla_\theta\log\pi_\theta(s,a)$ can be analytically computed.&lt;/p&gt;
&lt;p&gt;Now,
$$
\begin{align*}
\nabla_\theta V_{\pi_\theta}(s)&amp;amp;=\nabla_\theta\left(\sum_a\pi_\theta(s,a)\left(R(s,a)+\gamma\sum_{s^\prime}\Pr[s^\prime\mid s,a]V_{\pi_\theta}(s^\prime)\right)\right)\\
&amp;amp;=\sum_a\nabla_\theta\pi_\theta(s,a)\left(R(s,a)+\gamma\sum_{s^\prime}\Pr[s^\prime\mid s,a]V_{\pi_\theta}(s^\prime)\right)+\sum_a\pi_\theta(s,a)\gamma\sum_{s^\prime}\Pr[s^\prime\mid s,a]\nabla_\theta V_{\pi_\theta}(s^\prime)\\
&amp;amp;=\sum_a\nabla_\theta\pi_\theta(s,a)Q_{\pi_\theta}(s,a)+\gamma\sum_{s^\prime}\Pr[s^\prime\mid s]\nabla_\theta V_{\pi_\theta}(s^\prime)\\
&amp;amp;=\cdots\\
&amp;amp;=\sum_t\gamma^t\sum_{s^\prime}\Pr[s^\prime\mid s;\text{after }t\text{ steps}]\sum_a\nabla_\theta\pi_\theta(s,a)Q_{\pi_\theta}(s,a)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\nabla_\theta J_1(\theta)&amp;amp;=\sum_sd(s)\nabla_\theta V_{\pi_\theta}(s)\\
&amp;amp;=\sum_sd(s)\sum_t\gamma^t\sum_{s^\prime}\Pr[s^\prime\mid s;\text{after }t\text{ steps}]\sum_a\nabla_\theta\pi_\theta(s,a)Q_{\pi_\theta}(s,a)\\
&amp;amp;=\sum_t\gamma^t\sum_{s^\prime}\Pr[s_t=s^\prime]\sum_a\nabla_\theta\pi_\theta(s,a)Q_{\pi_\theta}(s,a)\\
&amp;amp;=\sum_s\left(\sum_t\gamma^t\Pr[s_t=s]\right)\left(\sum_a\nabla_\theta\pi_\theta(s,a)Q_{\pi_\theta}(s,a)\right)
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;If we define discounted visitation distribution
$$
d_{\pi_\theta}^\prime(s)\propto\sum_t\gamma^t\Pr[s_t=s]
$$
we get
$$
\nabla_\theta J_1(\theta)=\frac{1}{1-\gamma}\E_{s\sim d_{\pi_\theta}^\prime(\cdot)}\left(\sum_a\nabla_\theta\pi_\theta(s,a)Q_{\pi_\theta}(s,a)\right)=\frac1{1-\gamma}\E_{s\sim d_{\pi_\theta}(\cdot),a\sim \pi_\theta(s,\cdot)}\left(\nabla_\theta\log\pi_\theta(s,a)Q_{\pi_\theta}(s,a)\right)
$$
Actually it&amp;rsquo;s the Policy Gradient Theorem: For any differentiable policy $\pi_\theta(s,a)$,
$$
\nabla_\theta J(\theta)=\E\left(\nabla_\theta\log \pi_\theta(s,a)Q_{\pi_\theta}(s,a)\right)
$$
This holds for any of the choices $J=J_1,J_{\rm avR},\frac1{1-\gamma}J_{\rm avV}$. For $J_1$, the $1/(1-\gamma)$ coefficient is gotten rid of by forcing $d_{\pi_\theta}(s)=\sum_t\gamma^t\Pr[s_t=s]$ (So it&amp;rsquo;s notation abuse since now $\E$ doesn&amp;rsquo;t really denote expectation). For $J_{\rm avR}$ and $\frac1{1-\gamma}J_{\rm avV}$, $d_\pi$ is just the stationary distribution, while $Q$ is changed to differential Q-value:
$$
Q_{\pi_\theta}^\prime(s, a)=R(s,a)-J_{\rm avR}(\theta)+\E(Q_{\pi_\theta}^\prime(s^\prime,a^\prime))
$$
The proof from Gemini is &lt;a class=&#34;link&#34; href=&#34;https://gemini.google.com/share/b46323c42a37&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So the final algorithm is just $\theta\xleftarrow{+}\alpha v_t\nabla_\theta\log\pi_\theta(s_t,a_t)$ following a sampled trajectory. Here $v_t$ is the reward-to-go.&lt;/p&gt;
&lt;h4 id=&#34;actor-critic-methods&#34;&gt;Actor-Critic Methods
&lt;/h4&gt;&lt;p&gt;Recall that value-based and policy-based RL each have their own advantages and disadvantages. How about composing them together?&lt;/p&gt;
&lt;p&gt;The critic is a network parameterized by $\phi$, denoted $Q_\phi(s,a)$.&lt;/p&gt;
&lt;p&gt;The actor is the same as above, $\pi_\theta(s,a)$.&lt;/p&gt;
&lt;p&gt;Following a trajectory, the parameters are updated:
$$
\theta\xleftarrow{+}\alpha Q_\phi(s_t,a_t)\nabla_\theta\log\pi_\theta(s_t,a_t)
$$&lt;/p&gt;
&lt;p&gt;$$
\phi\xleftarrow{+}\beta(R(s_t,a_t)+\gamma Q_\phi(s_{t+1},a_{t+1})-Q_\phi(s_t,a_t))\nabla_\phi Q_\phi(s_t,a_t)
$$&lt;/p&gt;
&lt;p&gt;You might have already caught the questionable point: this doesn&amp;rsquo;t conform to any version ($J_1,J_{\rm avR},\frac1{1-\gamma}J_{\rm avV}$) of the Policy Gradient Theorem! By Gemini, the objective is another variant, called the average performance with discounted Q-values:
$$
J_{\text{hybrid}}(\theta) = \E_{s \sim d_\pi(\cdot)} (V_\pi(s))
$$
Forget about it.&lt;/p&gt;
&lt;p&gt;Now we aim to reduce the variance of $\nabla_\theta\log \pi_\theta(s,a)Q_{\pi_\theta}(s,a)$. Notice that for any function only of $s$:
$$
\E(\nabla_\theta\log \pi_\theta(s,a)B(s))=\sum_sd_{\pi_\theta}(s)\sum_a\nabla_\theta\pi_\theta(s,a)B(s)=\sum_sd_{\pi_\theta}(s)\nabla_\theta B(s)=0
$$
We can choose $B(s)=\E(Q(s,a))$, so
$$
\nabla_\theta J(\theta)=\E\left(\nabla_\theta\log \pi_\theta(s,a)(Q_{\pi_\theta}(s,a)-V_{\pi_\theta}(s))\right)
$$
to reduce the variance. Notice that it&amp;rsquo;s just the advantage function $A_{\pi_\theta}(s,a)$. This leads to some elegant update formulas:
$$
\delta=R(s_t,a_t)+\gamma V_\phi(s_{t+1})-V_\phi(s_t)
$$&lt;/p&gt;
&lt;p&gt;$$
\theta\xleftarrow{+}\alpha\delta\nabla_\theta\log\pi_\theta(s_t,a_t)
$$&lt;/p&gt;
&lt;p&gt;$$
\phi\xleftarrow{+}\beta\delta\nabla_\phi V_\phi(s_t)
$$&lt;/p&gt;
&lt;p&gt;Similar to Q-learning, we&amp;rsquo;re also worried about bias in the algorithm. There&amp;rsquo;s a theorem for this:&lt;/p&gt;
&lt;p&gt;If $\nabla_\phi Q_\phi(s,a)=\nabla_\theta\log\pi_\theta(s,a)$ and $\phi$ minimizes the error $\eps=\E(Q_{\pi_\theta}(s,a)-Q_\phi(s,a))^2$ then
$$
\nabla_\phi\eps=0\implies\E((Q_{\pi_\theta}(s,a)-Q_\phi(s,a))\nabla_\phi Q_\phi(s,a))=0\implies\E(Q_{\phi}(s,a)\nabla_\theta\log\pi_\theta(s,a))=\E(Q_{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(s,a))
$$
Obviously the minimal $\eps$ cannot be rigorously achieved, but the first condition is easy: train a feature-learning network that learns $\Phi(s,a)$, and let $\log\pi_\theta(s,a)=\theta^\top\Phi(s,a)$ and $Q_\phi(s,a)=\phi^\top\Phi(s,a)$.&lt;/p&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference
&lt;/h3&gt;&lt;p&gt;I&amp;rsquo;m just leaving a catalog here since this part is easy and I have no more time.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Definition of conditional independence&lt;/li&gt;
&lt;li&gt;Definition of a Bayesian net, and the equivalence of different characterizations&lt;/li&gt;
&lt;li&gt;Inference by join and elimination&lt;/li&gt;
&lt;li&gt;D-separation (the hardest)&lt;/li&gt;
&lt;li&gt;Sampling methods&lt;/li&gt;
&lt;li&gt;Inference on MMs and HMMs
&lt;ol&gt;
&lt;li&gt;Forward algorithm &amp;amp; forward-backward algorithm&lt;/li&gt;
&lt;li&gt;Particle filtering&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>段设 Survey：APASP Algorithms</title>
        <link>https://littlereuben.github.io/study/course-galgo-survey/</link>
        <pubDate>Sat, 21 Feb 2026 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/course-galgo-survey/</guid>
        <description>&lt;div style=&#34;background-color: #FFF9B9; color: #796E00; padding-left: 1em; border-left: 4px solid #DED041; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;学习笔记&lt;/b&gt;。&lt;/div&gt;
&lt;p&gt;This is a survey for the final project of the course Design and Analysis of Algorithms.&lt;/p&gt;
&lt;embed src= &#34;main.pdf&#34; width= &#34;100%&#34; height= &#34;800px&#34; type=&#34;application/pdf&#34; &gt;
&lt;a href=&#34;main.pdf&#34; style=&#34;display:block;text-align:center;&#34;&gt;&lt;b&gt;[PDF file]&lt;/b&gt;&lt;/a&gt;
&lt;p&gt;The following presentation slide contains some delicate diagrams, illustrating the intuitions for some of these abstruse algorithms, so that you can grasp the core ideas efficiently.&lt;/p&gt;
&lt;embed src= &#34;pre.pdf&#34; width= &#34;100%&#34; height= &#34;800px&#34; type=&#34;application/pdf&#34; &gt;
&lt;a href=&#34;pre.pdf&#34; style=&#34;display:block;text-align:center;&#34;&gt;&lt;b&gt;[PDF file]&lt;/b&gt;&lt;/a&gt;
</description>
        </item>
        <item>
        <title>读论文系列 #4——stretch 2 APSP 的最新进展</title>
        <link>https://littlereuben.github.io/study/paper4/</link>
        <pubDate>Sun, 14 Sep 2025 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/paper4/</guid>
        <description>&lt;div style=&#34;background-color: #FFE1B9; color: #794500; padding-left: 1em; border-left: 4px solid #DE9B41; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;摘抄笔记&lt;/b&gt;。&lt;/div&gt;
&lt;h3 id=&#34;引入&#34;&gt;引入
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2307.09258&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;，&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/shiys22/p/17736505.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;APSP 近似的一些其他情形 sys 的笔记&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;问题&#34;&gt;问题
&lt;/h4&gt;&lt;p&gt;对于 APSP 问题，记 $\pi(u,v)$ 为最短路，$d(u,v)$ 为 $\pi(u,v)$ 的长度。如果有一个算法能得到近似 $\delta(u,v)$ 满足 $d(u,v)\le\delta(u,v)\le m\cdot d(u,v)+a$，那么就称为一个 $(m,a)$ 近似。$a=0$ 时称为 stretch $m$ 近似，$m=1$ 时称为 surplus $a$ 近似，下文中简写作 $m$ 近似与 $+a$ 近似。&lt;/p&gt;
&lt;p&gt;distance oracle 指的是以常数时间（？）回答一组 $\delta(u,v)$ 的算法，这样避免了输出的 $\Omega(n^2)$ 界，可以（在稀疏图中）做到平方以下。&lt;/p&gt;
&lt;p&gt;下文中“非确定性算法”指必然正确且以高概率在对应时间内运行结束的算法；期望时间算法可以并列运行 $\log$ 个，通过 Chernoff bound 变成非确定性。&lt;/p&gt;
&lt;h4 id=&#34;一些上下界&#34;&gt;一些上下界
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;大家都知道准确的带权 APSP 被猜想是 $\tilde\Omega(n^3)$ 的，而不带权的可以归约矩乘做。&lt;/li&gt;
&lt;li&gt;$2-\epsilon$ 近似不弱于 01 矩乘，有向图的任意近似都不弱于 01 矩乘。&lt;/li&gt;
&lt;li&gt;当 $m=\Theta(n)$ 时，oracle 有 $\tilde\Omega(m^{5/3})$ 的时空下界（基于一些 fine-grained complexity 的猜想）。&lt;/li&gt;
&lt;li&gt;已知无向无权图的 $(2,1)$ 近似可以 $\tilde\Omicron(n^2)$，所以现在不确定的是，无权/有权的 $2$ 近似能不能 $n^2$，以及 oracle 能不能 sub-$n^2$。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;无权&#34;&gt;无权
&lt;/h3&gt;&lt;h4 id=&#34;已知结果&#34;&gt;已知结果
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;[DHZ00] 存在非确定性与确定性的算法，能在线性时间内得到一张图中所有度 $\ge s$ 点的 hitting-set，大小 $\tilde\Omicron(n/s)$。&lt;/li&gt;
&lt;li&gt;[DHZ00] 无权无向图的 $+\log n$ 近似 APSP 可以在时间 $\tilde\Omicron(n^2)$ 内求解。&lt;/li&gt;
&lt;li&gt;[BK10] 非负整数权无向图的 $2$ 近似 APSP 可以在时间 $\tilde\Omicron(mn^{0.5}+n^2)$ 内用非确定性算法求解。&lt;/li&gt;
&lt;li&gt;[Zwi02] $\langle n,n^r,n\rangle$ 的 $(\min,+)$ 矩阵乘法的 $1+\epsilon$ 近似能在时间 $\tilde\Omicron(n^{\omega(r)}\epsilon^{-1}\log W)$ 内求解，其中 $\omega(r)$ 是 $\langle n,n^r,n\rangle$ 的普通矩阵乘法复杂度，原矩阵值域为 $[W]\cup\set{\infty}$。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;整体处理思路&#34;&gt;整体处理思路
&lt;/h4&gt;&lt;p&gt;我们希望找到一些关键点，只求关键点到全体的最短路，将必经关键点的最短路作为近似结果。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;1.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;ol&gt;
&lt;li&gt;如果 $\pi(u,v)$ 经过大度点，可以通过 hitting-set 的套路做到 $+2$ 近似（优于 $2$ 近似）。
&lt;ol&gt;
&lt;li&gt;对于 hitting-set $S$ 内的点，做 MSSP。&lt;/li&gt;
&lt;li&gt;求形如 $\delta(u,v)=\min_{x\in S}\set{d(u,x)+d(x,v)}$ 的 $(\min,+)$ 矩乘。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;否则是稀疏图上的 APSP。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这里 1.1 中的 MSSP 我们固定用 Dijkstra，那么剩余的就是需要指定矩乘和稀疏图 APSP 的算法。如果取度数阈值为 $n^{1-r}$ 的话，那么时间就是
$$
\tilde\Omicron(mn^r+T_{1.2}(n,n^r,n)+T_2(n,n^{2-r}))
$$
接下来认为 $m=\Omicron(n^2)$。&lt;/p&gt;
&lt;h4 id=&#34;倍增优化&#34;&gt;倍增优化
&lt;/h4&gt;&lt;p&gt;考虑进一步分治。如果 $\pi(u,v)$ 经过度数在 $[a,b)$ 内的点，那么用第 1 部分的方法，Dijkstra 是点数 $n/a$，边数 $nb$。因此如果我们取一列区间
$$
[n^{1-r},2n^{1-r}),[2n^{1-r},4n^{1-r}),\cdots
$$
分别交给第 1 部分做，时间就降为
$$
\tilde\Omicron(n^2+T_{1.2}(n,n^r,n))
$$&lt;/p&gt;
&lt;h4 id=&#34;近似归约&#34;&gt;近似归约
&lt;/h4&gt;&lt;p&gt;我们知道 $(\min,+)$ 矩乘是没法做的，所以只能近似。如果有 $(m,a)$ 近似的矩乘，那么第 1 部分整体就是 $(m,a+2m)$ 近似。由于只需考虑 $d\ge 2$ 的情况，$(m,a+2m)$ 近似就必然是 $(2m,a)$ 近似。&lt;/p&gt;
&lt;p&gt;于是根据已知 4，第 1 部分存在一个 $2+\epsilon$ 近似，时间关于 $\epsilon^{-1}$ 线性。 取 $\epsilon=1/\log n$。如果 $d(u,v)&amp;lt;\log n$，$2+\epsilon$ 近似相当于 $2$ 近似；否则调用已知 2。&lt;/p&gt;
&lt;h4 id=&#34;套用算法&#34;&gt;套用算法
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;全部用朴素做法，取 $r=0.5$ 得 $\tilde\Omicron(n^{2.5})$。&lt;/li&gt;
&lt;li&gt;第 2 部分用已知 3，就是 $n^2+n^{2+r}+n^{2.5-r}$，取 $r=0.25$ 得 $\tilde\Omicron(n^{2.25})$。&lt;/li&gt;
&lt;li&gt;上一条基础上，第 1 部分用已知 4，就是 $n^2+n^{\omega(r)}+n^{2.5-r}$，可以平衡到 $\tilde\Omicron(n^{2.032})$。&lt;/li&gt;
&lt;li&gt;还已知一个 parameterized 的算法是做 $+k$ 近似的，与矩乘结合得到一个 $(1+\epsilon,k)$ 近似算法。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;有权&#34;&gt;有权
&lt;/h3&gt;&lt;h4 id=&#34;已知结果-1&#34;&gt;已知结果
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;[TZ01], [TZ05] 对于“密度” $p$，存在非确定性算法能在 $\tilde\Omicron(m/p)$ 的时间内得到一组大小为 $\tilde\Omicron(np)$ 的关键点集并计算出每个束内的基本信息，满足束和簇的大小均不超过 $\tilde\Omicron(1/p)$。&lt;/li&gt;
&lt;li&gt;[EN22] 非负整数权无向图的 $1+\epsilon$ 近似 MSSP 能在时间 $\tilde\Omicron(m^{1+\omicron(1)}+n^{\omega(r)}\epsilon^{-\Omicron(1)}\log W)$ 内求解，其中起点集合大小 $n^r$，边权值域 $[W]$。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;整体处理思路-1&#34;&gt;整体处理思路
&lt;/h4&gt;&lt;p&gt;同样取一些关键点，但是现在我们考虑这样的结构：对于点 $u$，找到与它距离最近的关键点 $p(u)$，将距离比 $d(u,p(u))$ 小的点，和 $p(u)$ 一起组成 $u$ 的束（branch）$B(u)$。束的逆称为簇 $C(v)=\set{u\mid v\in B(u)}$。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;2.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;font color=&#39;gray&#39; size=2&gt;蓝色点表示最近关键点，称为“枢轴”&lt;/font&gt;&lt;/center&gt;
&lt;p&gt;通过束的定义以及三角不等式，可以放缩得到一些近似。首先有个 $3$ 近似：若 $v\notin B(u)$，则
$$
d(u,p(u))+d(p(u),v)\le d(u,p(u))+d(p(u),u)+d(u,v)\le 3d(u,v)
$$
若 $v\in B(u)$，已知 1 的做法里能算出 $u$ 到 $B(u)$ 各点的最短路，所以就不用管了。&lt;/p&gt;
&lt;p&gt;我们现在称 $\pi(u,v)\setminus(B(u)\cup B(v))$ 中的点为“束间点”。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;3.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;（情况 1）如果 $\pi(u,v)$ 有束间点 $x$，那么 $d(u,x),d(x,v)$ 中至少有一个 $\le d(u,v)/2$，不妨设为 $d(u,x)$，则
$$
d(u,p(u))+d(p(u),v)\le 2d(u,p(u))+d(u,v)\le 2d(u,x)+d(u,v)\le 2d(u,v)\tag{1}
$$
剩余的情况（情况 2）是关注的重点。&lt;/p&gt;
&lt;h4 id=&#34;暴力做法&#34;&gt;暴力做法
&lt;/h4&gt;&lt;p&gt;若 $\pi(u,v)$ 无束间点，那么 $\pi(u,v)$ 一定形如 $u\rightsquigarrow u^\prime\to v^\prime\rightsquigarrow v$，其中 $u^\prime\in B(u)$，$v^\prime\in B(v)$。这样的 $(u,u^\prime,v^\prime,v)$ 的数量是 $m|C|^2=\tilde\Omicron(m/p^2)$。总的时间：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;情况 1 从关键点出发跑 Dijkstra，$\tilde\Omicron(nmp)$。&lt;/li&gt;
&lt;li&gt;情况 2 暴力更新，$\tilde\Omicron(m/p^2)$。&lt;/li&gt;
&lt;li&gt;两类情况取 $\min$，$\Omicron(n^2)$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;我们可以做一个 oracle，省去 3，取 $p=n^{-1/3}$ 得 $\tilde\Omicron(mn^{2/3})$。&lt;/p&gt;
&lt;p&gt;注意到如果暴力做无束间边（$\tilde\Omicron(n/p^2)$），而恰有一条束间边的情况归到情况 1，可以得到一个 $\tilde\Omicron(nm^{2/3})$ 的 $(2,\max w)$ 近似。&lt;/p&gt;
&lt;h4 id=&#34;稠密情况&#34;&gt;稠密情况
&lt;/h4&gt;&lt;p&gt;对于情况 1 用已知 2 的 $1+\epsilon/2$ 近似，得到 $2+\epsilon$ 近似。&lt;/p&gt;
&lt;p&gt;对于情况 2，分两步做（各束边指的是，每个 $u$ 向 $B(u)$ 内点连边权为距离的边）：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;4.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;font color=&#39;gray&#39; size=2&gt;② 对应第 1 次 Dijkstra，③ 对应第 2 次&lt;/font&gt;&lt;/center&gt;
&lt;ol&gt;
&lt;li&gt;对每个 $v^\prime$，保留其邻边与各束边，跑 Dijkstra，得到 $d^\prime(v^\prime,u)$。&lt;/li&gt;
&lt;li&gt;对每个 $u$，保留各束边，建边 $(u,v^\prime)$ 权 $d^\prime(v^\prime,u)$，跑 Dijkstra。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;时间均为 $\tilde\Omicron(n^2/p)$，这样算出来的是准确 SP。若 $p=n^{r-1}$，则总时间为
$$
\tilde\Omicron(n^{3-r}+n^{\omega(r)}\epsilon^{-\Omicron(1)}\log W)
$$
可以平衡到 $n^{2.213}$。&lt;/p&gt;
&lt;p&gt;注意到建束内边对非稠密图过于激进了，接下来的做法进行了进一步的平衡。&lt;/p&gt;
&lt;h4 id=&#34;倍增优化-1&#34;&gt;倍增优化
&lt;/h4&gt;&lt;p&gt;相比情况 2，情况 1 更容易优化。注意到有权图的“束”结构与无权图的简单的 hitting-set 结构有以下共同点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过一个中介点 $x$，三角不等式放缩多出两倍的 $d(x,?)$（在无权图中就是 $+2$），得到 $2$（或 $2+\epsilon$） 近似。&lt;/li&gt;
&lt;li&gt;关键点出发全图跑 MSSP，这使我们无法不受限制地增加关键点数，否则这部分会成为瓶颈。&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;不借助中介点&lt;/th&gt;
&lt;th&gt;借助中介点&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;无权图&lt;/td&gt;
&lt;td&gt;去除大度点&lt;/td&gt;
&lt;td&gt;必经大度点&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;无权图做法&lt;/td&gt;
&lt;td&gt;跑稀疏图 APSP&lt;/td&gt;
&lt;td&gt;找 hitting-set，MSSP + 矩乘&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;有权图&lt;/td&gt;
&lt;td&gt;$\pi(u,v)$ 无束间点&lt;/td&gt;
&lt;td&gt;$\pi(u,v)$ 有束间点&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;有权图做法&lt;/td&gt;
&lt;td&gt;暴力松弛&lt;/td&gt;
&lt;td&gt;MSSP&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;center&gt;&lt;img src=&#34;6.png&#34; style=&#34;zoom:30%;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;font color=&#39;gray&#39; size=2&gt;对数尺度；每个格子的右边界为该格算法的关键点密度&lt;/font&gt;&lt;/center&gt;
&lt;p&gt;在无权图中，通过考虑子问题：经过度数在 $[a,b)$ 间点的最短路，对其分治，得到倍增做法，减少了边数，使得 Dijkstra 总时间由 $mn^r$ 降至 $n^2$。在有权图中，模仿这一思路：考虑关键点集 $S_{i+1}\subset S_i$，现在需要对于所有 $(u,v)$ 满足 $\pi(u,v)$ 在 $S_{i+1}$ 下无束间点，而在 $S_i$ 下有束间点的情况，去求 $d(u,v)$ 的近似。同样，我们的目标是减少关键点出发 MSSP 的边数，同时保证近似的论证不失效。&lt;/p&gt;
&lt;p&gt;仍然假定 $d(p_i(u),u)\le d(u,v)/2$。回顾不等式 $(1)$：
$$
d(u,p(u))+\boxed{d(p(u),v)}\le d(u,p(u))+\boxed{d(p(u),u)+d(u,v)}
$$
实际不需要准确求出 $d(p_i(u),v)$，而只求一个近似的 $\delta(p_i(u),v)\in[d(p_i(u),v),d(p_i(u),u)+d(u,v)]$ 即可。进一步，如果 $\pi(u,v)$ 形如 $u\rightsquigarrow u^\prime\to x\rightsquigarrow v$，其中 $u^\prime\in B_{i+1}(u)$，$x\notin B_{i+1}(u)$，那么可以拆成
$$
d(p_i(u),u)+d(u,u^\prime)+w(u^\prime,x)+d(x,v)
$$&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;5.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;建立这样的图：$p_i(u)$ 向所有与 $B_{i+1}(u)$ 相邻的点（及上述 $x$ 这样的点）连边，边权
$$
\delta^\prime(p_i(u),x)=\min_{u^\prime}\set{d(p_i(u),u)+d(u,u^\prime)+w(u^\prime,x)}
$$
另外对于所有的点 $v$，将与 $v$ 相邻的权 $\le d(v,p_{i+1}(v))$ 的边加入。跑 Dijkstra。设 $\pi(x,v)=x\to\cdots\to v_1\to v$。由于 $x\in B_{i+1}(v)$，故 $(v_1,v)$ 是保留的；由于 $d(v_1,p_{i+1}(v_1))+w(v_1,v)\ge d(v,p_i(v))$，故 $x\in B_{i+1}(v_1)$，所以 $(v_2,v_1)$ 也保留，以此类推。&lt;/p&gt;
&lt;p&gt;求 $\delta^\prime$ 所需的时间是
$$
\sum_{u^\prime}\mathrm{deg}_{u^\prime}|C_{i+1}(u)|=\tilde\Omicron(m|C_{i+1}|)
$$
我们相当于把菊花形的各束边改成了第二类边，它们的数量这么考虑：如果 $S_{i+1}$ 是以 $q$ 的概率选点的话，那么对于不在 $S_{i+1}$ 中的点，将其邻边按边权从小到大扫描，每次有 $q$ 的概率把剩余的扔掉。因此期望边数为 $\sum (1-q)^i\le 1/q$，用 Chernoff bound 一类的东西放缩下就是高概率 $\tilde\Omicron(n/q)$。从这个意义上来看，束结构也是限制度数的，基本打通了无权和有权的思想。&lt;/p&gt;
&lt;p&gt;因此总的复杂度是
$$
\tilde\Omicron\left(\frac mq+|S_i|\frac nq+n^2\right)=\tilde\Omicron\left(\frac{mn}{|S_{i+1}|}+\frac{n^2|S_i|}{|S_{i+1}|}\right)
$$
考虑关键点集包含链 $V=S_0\supset S_1\supset\cdots\supset S_k$。其中 $S_i$ 中的每个点以 $1/2$ 概率出现在 $S_{i+1}$ 中。如果 $\pi(u,v)$ 在 $S_k$ 的下仍有束间点，那么就只能用原来情况 1 的做法，从 $S_k$ 出发跑 MSSP；否则一定存在某个 $S_{i}-S_{i+1}$ 交界，就用上面的方法。设 $k=(1-r)\log_2n$，则 $|S_k|=\tilde\Theta(n^r)$，时间为 $\tilde\Omicron(mn^{1-r}+n^2+T_1(n,m,n^r))$，$T_1$ 是 MSSP 的复杂度。&lt;/p&gt;
&lt;p&gt;实际实现时，第一类边权的 $\min$ 只需要拿 $B_k(u)$ 内所有点的邻边去松弛就行，那么为了保证 $|C_k(u)|$ 的大小，需要强制往 $S_k$ 里加已知 1 里的构造，不过这个不影响复杂度。总的做法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;求出 $\set{S_i}$，建出束结构。&lt;/li&gt;
&lt;li&gt;对于每个 $u$，每个 $i$，每个 $B_k(u)$ 内点的邻边，更新 $\delta^\prime(p_i(u),x)$。&lt;/li&gt;
&lt;li&gt;对于每个 $p_i(u)$ 建图跑 Dijkstra。&lt;/li&gt;
&lt;li&gt;对于 $S_k$ 在原图上跑 MSSP。&lt;/li&gt;
&lt;li&gt;对每个 $(u,v)$ 求答案。&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;套用算法-1&#34;&gt;套用算法
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;MSSP 直接用 Dijkstra，时间 $mn^r$，取 $r=0.5$ 得 $\tilde\Omicron(mn^{0.5}+n^2)$，这就是 [BK10]。&lt;/li&gt;
&lt;li&gt;MSSP 用已知 2，这样是 $2+\epsilon$ 近似，是 $mn^{1-r}+n^2+n^{\omega(r)}\epsilon^{-\Omicron(1)}\log W$，对于稠密图的平衡结果，和稠密情况那个一样。&lt;/li&gt;
&lt;/ol&gt;
</description>
        </item>
        <item>
        <title>读论文系列 #5——多带图灵机中的空间换时间</title>
        <link>https://littlereuben.github.io/study/paper5/</link>
        <pubDate>Sun, 14 Sep 2025 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/paper5/</guid>
        <description>&lt;div style=&#34;background-color: #FFE1B9; color: #794500; padding-left: 1em; border-left: 4px solid #DE9B41; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;摘抄笔记&lt;/b&gt;。&lt;/div&gt;
&lt;h3 id=&#34;引入&#34;&gt;引入
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2502.17779&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;paper&lt;/a&gt;，&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=1qwDO5ulUFs&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;部分讲解视频&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;$\gdef\b#1{\boldsymbol{#1}}$下文中 $d$、$m$ 代表常数。&lt;/p&gt;
&lt;p&gt;考虑运行时间为 $t(n)$ 的 $d$ 带图灵机，我们考虑的问题是可以用多少空间的多带图灵机模拟它。这一问题的重要性在于：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果 $\forall\varepsilon&amp;gt;0$，$\mathsf{TIME}[t(n)]\subseteq\mathsf{SPACE}[t(n)^\varepsilon]$，则 $\mathsf{P}\ne\mathsf{PSPACE}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最新的这篇文章做到了 $\mathsf{TIME}[t(n)]\subseteq\mathsf{SPACE}[\sqrt{t(n)\log t(n)}]$，大致的思路是缝合了空间模拟时间的基本思想，以及一个 $\mathsf{L}\text{ vs }\mathsf{P}$ 方面的最新进展。&lt;/p&gt;
&lt;h3 id=&#34;单带情况&#34;&gt;单带情况
&lt;/h3&gt;&lt;p&gt;在限制空间的情况下，其实显而易见的思路只有一条，就是做一个 time-space tradeoff，以重复计算的代价换取不存储完整纸带的能力。&lt;/p&gt;
&lt;p&gt;考虑每时每刻，只在 $M^\prime$ 中存储 $M$ 的纸带上的一小段，在上面进行模拟。如果读写头离开了当前段进入其他部分，就 recover 另一小段的当前情况，继续模拟。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;1.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;具体到单带图灵机上，考虑小段长为 $b(n)$，那么在跨段时，需要重新模拟新进入段的所有历史操作，但模拟过程不能递归依赖其他段的信息。为此，只需存储历史上每次跨段时的信息即可，这所需记录条目数是平均 $\frac{t(n)}{b(n)}$ 的，每个条目需要记录段号与状态，是 $\log\frac{t(n)}{b(n)}+\Omicron(1)$ 的，如果改成记录 $\pm 1$ 就是 $\Omicron(1)$（这个和四毛子优化 RMQ 是一样的）。为了避免 $M$ 在段边界反复横跳导致卡到最劣，可以枚举分段起始点的偏移，根据鸽巢原理一定存在一个偏移量满足跨段次数 $\le\frac{t(n)}{b(n)}$。&lt;/p&gt;
&lt;p&gt;显然这里取 $b(n)=\sqrt{t(n)}$，平衡到 $\Omicron(\sqrt{t(n)})$ 的空间复杂度。&lt;/p&gt;
&lt;p&gt;但是这个思路在多带完全不行：各带读写头的跨段没法同步，就算同步了，考虑 $(x,y)$ 表示当前第一条纸带的读写头在第 $x$ 段，第二条在 $y$，那么比如当前在 $(2,3)$ 恢复所需要的历史操作可能是 $(2,1)$、$(2,2)$、$(1,3)$、$(3,3)$ 这些情形，它就爆炸了。简而言之，依赖关系从单带的链，变成多带的 DAG。&lt;/p&gt;
&lt;h3 id=&#34;多带历史思路&#34;&gt;多带历史思路
&lt;/h3&gt;&lt;p&gt;首先展开说一下多带跨段的同步以及标准化。我们希望将位置和时间同时分块：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于多带图灵机，存在等效的多带“分块图灵机” with 块大小 $b(n)$，时间 $\Theta(t(n))$（实际上要求 $\log t(n)\le b(n)\le t(n)$，以及 $t(n)$、$b(n)$ time-constructible，不过这些不太重要）。分块图灵机指的是，各纸带划分为 $b(n)$ 长的小段，读写头跨段只能出现在 $b(n)$ 倍数的时刻。&lt;/li&gt;
&lt;/ul&gt;
&lt;details style=&#34;background-color: rgba(127, 127, 127, 0.1); padding: 5px;&#34;&gt;
    &lt;summary&gt;构造思路&lt;/summary&gt;
    加的一条带作为“时钟”，读写头持续在 $b(n)$ 的范围内来回移动，如果其他头需要跨段但“时钟”未到“整点”（下文我称时间的分段为时间块），就等着到整点再动。但这仍然会遇到段边界反复横跳的问题。处理方法是，扩展字母表/带数使得每带的每段都记录相邻两段的信息，这样就无需等待，只需在当前时间块完成后，额外花若干（常数个）时间块把更新的信息复制到相邻段即可。
    &lt;center&gt;&lt;img src=&#34;prf.png&#34; style=&#34;zoom:50%;&#34; /&gt;&lt;/center&gt;
&lt;/details&gt;
&lt;p&gt;设纸带数为 $d$。&lt;/p&gt;
&lt;p&gt;刻画多带的历史依赖关系的结构被称为 computation graph：每个时间块 $i$ 对应一个节点，其存储 $\text{content}_i$，内容为该时间块结尾时：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图灵机的状态 $q_i$；&lt;/li&gt;
&lt;li&gt;各读写头位置 $h_{i,1}\sim h_{i,d}$；&lt;/li&gt;
&lt;li&gt;各读写头所在纸带段的最终内容 $c_{i,1}\sim c_{i,d}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;空间为 $\Omicron(b(n)+\log t(n))$。&lt;/p&gt;
&lt;p&gt;为了得到 $\text{content}_i$，我们需要 $\text{content}_{i-1}$，以及另外某些 $\text{content}_{k_1,\cdots,k_d}$。如果对于某纸带 $j$，
$$
\left\lfloor\frac{h_{i,j}}{b(n)}\right\rfloor=\left\lfloor\frac{h_{k,j}}{b(n)}\right\rfloor\land\forall k^\prime\in(k,i),\left\lfloor\frac{h_{i,j}}{b(n)}\right\rfloor\ne\left\lfloor\frac{h_{k^\prime,j}}{b(n)}\right\rfloor
$$
也就是说，$\text{content}_k$ 中有时间块 $i$ 所需的纸带 $j$ 的最新信息，那么连边 $k\to i$。因此，每个点入度 $\le d+1$。&lt;/p&gt;
&lt;p&gt;两个注：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;发现不强制标准化分块也是可以的，就是要每带记录两个块，入度 $\le 2d+1$。&lt;/li&gt;
&lt;li&gt;处理输入有两种思路，一种是加个只存输入的 $\text{content}_0$，一种是在每个点额外存历史首次访问到的段的内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假定已知 computation graph 的结构，待求 $\text{content}_{t(n)/b(n)}$。当前的模型形如：给定一张 DAG，指定源和汇。每次如果一个点是源，或它的所有入点上都有石子，可以在这个点上也放石子；也可以任意拿掉石子。问使得汇上有石子，最少需要备多少石子。这被称为 (standard/black) pebble game。&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;eg.png&#34; style=&#34;zoom:70%;&#34; /&gt;&lt;/center&gt;
&lt;p&gt;例如这张图应该至少需要四个石子：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;+0, +1, +2, -1, +3, -0, +4, -3, +0, +1, +2, -0, -1, +5
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;记 $P_k(n)$ 为最大入度为 $k$ 的 $n$ 点图中，最多的所需石子数（可以任意指定一个汇）。[HPV75] 中证明了对于常数 $k$，$P_k(n)=\Omicron(n/\log n)$。&lt;/p&gt;
&lt;details style=&#34;background-color: rgba(127, 127, 127, 0.1); padding: 5px;&#34;&gt;
    &lt;summary&gt;证明&lt;/summary&gt;
    记 $R_k(n)$ 为最大入度为 $k$，所需 $n$ 个石子的图中，最少的边数。只需证明 $R_k(n)=\Omega(n\log n)$ 即可，考虑用割 + 归纳的证法。取这样一张图 $G=(V,E)$ 中，用 $\le n/2$ 个石子就能达到的所有点 $V_1$，剩余为 $V_2$。如果将 $V_2$ 的导出子图拿出，入度为 $0$ 的点可以随意放石子，那么这部分需要至少 $n/2-k$ 个石子。这是因为，在整张图中，如果要将 $V_2$ 中的一个入度为 $0$ 的点上放上石子，就需要在 $V_1$ 中至多 $k$ 个点上放石子，就需要备至多 $n/2+k-1$ 个石子。如果 $V_2$ 部分只需 ${{}&lt;{}}n/2-k$，两者相加就与需要 $n$ 个石子矛盾了（似乎常数项可以更紧一点？）。另外 $V_1$ 中一定有需 $\ge n/2-k$ 个石子的点，因此
    $$
    R_k(n)\ge 2R_k\left(\frac n2-k\right)+\text{cut}(V_1,V_2)
    $$
    只需 $\text{cut}(V\_1,V\_2)$ 不太小（$=\Theta(n)$）就行。如果 $\text{cut}(V_1,V_2){{}&lt;{}}n/4$，那么可以在 $V_1$ 中与 $V_2$ 有边的节点上全留石子，因此 $V_2$ 部分单独拿出来做，需要至少 $3n/4$ 的石子。于是
    $$
    R_k(n)\ge\min\Set{2R_k\left(\frac n2-k\right)+\frac n4,R_k\left(\frac n2-k\right)+R_k\left(\frac{3n}4\right)}
    $$
    归纳可证 $R_k(n)\ge cn\log n$。
&lt;/details&gt;
&lt;p&gt;另外存在卡到 $\Theta(n/\log n)$ 的构造，详见 &lt;a class=&#34;link&#34; href=&#34;https://dl.acm.org/doi/10.1145/800113.803643&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Space bounds for a game on graphs&lt;/a&gt;，似乎是形如 FFT 电路。&lt;/p&gt;
&lt;p&gt;这就意味着无论怎么取 $b(n)$，空间被限制在 $\Omega(n/\log n)$。为完成整个证明，还需解决两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何求出 computation graph？直接枚举每个头每个时间块的位置，是 $\Omicron\big(\frac{t(n)}{b(n)}\log\frac{t(n)}{b(n)}\big)$ bits，当然也可以枚举差分。如果模拟过程中发现读写头走的方向与枚举的不符，就返回 &lt;code&gt;FAIL&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;如何求出一组达到 $\Omicron\big(\frac{t(n)}{b(n)}/\log\frac{t(n)}{b(n)}\big)$ 的放石子顺序？直接枚举的话递归栈大小会爆，但无论如何每次猜一步，总还是 $\mathsf{NSPACE}\big[\frac{t(n)}{b(n)}\big]$ 的。$b(n)$ 稍微取大点，用下 Savitch 即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;小结&#34;&gt;小结
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;整体的思想是，局部纸带段—历史跨段记录的平衡。&lt;/li&gt;
&lt;li&gt;computation graph 是对整个结构的很本质的刻画，但是直接在这上面跑组合算法是没前途的。&lt;/li&gt;
&lt;li&gt;两个细节处理的 trick：分块化，以及猜 computation graph。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;容易感受到，之前研究者们都偏向于相信，$n/\log n$ 没法再优化了。&lt;/p&gt;
&lt;h3 id=&#34;tree-evaluation-problem&#34;&gt;tree evaluation problem
&lt;/h3&gt;&lt;p&gt;如果把 computation graph 展开，我们可以得到一棵高度为 $\Omicron\big(\frac{t(n)}{b(n)}\big)$，$\Omicron(d)$ 叉的树。尽管看似变得冗余了，但是树上的这一问题有代数做法，这就给了突破口。tree evaluation problem (TEP) 指的是，给定一颗高度为 $h$ 的满 $d$ 叉树，每个节点 $u$ 上有 $b$ 位 $01$ 串 $\text{content}_u$。初始给定叶子的串，以及每个节点由其各儿子串决定其本身串的函数，即 $(\set{0,1}^b)^d\to\set{0,1}^b$ 的函数，求根的串。&lt;/p&gt;
&lt;p&gt;输入的长度是 $\Theta(b2^{bd}d^h)=2^{\Theta(b+h)}$ 的，而这一问题是否 $\in\mathsf{NL}$ 还是 open 的（大家偏向于否）。但是 [CM24] 提出了一个空间 $\Omicron(db+h\log(db))$ 的做法，对于大小为 $n$ 的输入而言，这是 $\Omicron(\log n\log\log n)$ 的（据说还能多除个 $\log\log\log n$？）。&lt;/p&gt;
&lt;p&gt;一般的 dfs 做法需要 $\Omicron(dhb)$ 的空间：&lt;/p&gt;
&lt;center&gt;&lt;img src=&#34;2.png&#34; style=&#34;zoom:30%;&#34; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;font color=&#39;gray&#39; size=2&gt;紫色为当前节点，黄色为需存储节点&lt;/font&gt;&lt;/center&gt;
&lt;p&gt;而 [CM24] 的想法是，把上面各层的信息给叠起来，但利用单位根的性质（说白了就是单位根反演）把叠起来的垃圾抵消了。&lt;/p&gt;
&lt;p&gt;具体而言，每个节点的函数可以看作 $b$ 个 $\mathbb{F}_2^{bd}\to\mathbb{F}_2$ 的函数，具体来说，如果原函数是 $f(\b x_1,\cdots,\b x_d)$（每个 $\b x_i$ 是一个儿子的 $b$ 位），那么对应的函数是：
$$
\tilde{f}(\b x_1,\cdots,\b x_d)_i=\sum_{\b a_1,\cdots,\b a_d\in\mathbb{F}_2^b}f(\b a_1,\cdots,\b a_d)_i\prod_{j=1}^d\prod_{k=1}^b(\b x_{j,k}-\b a_{j,k}+1)
$$
这个东西的想法和拉格朗日插值/CRT 是一样的，在特征为 $2$ 的域中，$x-y+1$ 等价于 $[x=y]$。我们将定义域和值域扩到 $\mathbb{F}_{2^q}$，公式不变。&lt;/p&gt;
&lt;p&gt;现在只需存储 $(d+1)b$ 个 $\mathbb{F}_{2^q}$ 内的元素便可以计算。记这些寄存器为 $\b x_1,\cdots,\b x_{d+1}\in\mathbb{F}_{2^q}^b$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\text{calc}(u,\b y):$&lt;br&gt;
——将 $\text{content}_u$ 加给 $\b y$，其他寄存器的内容不变。方便起见记其他寄存器为 $\b x_1\sim\b x_d$。&lt;/p&gt;
&lt;div style=&#34;padding-left: 1em;&#34;&gt;&lt;blockquote&gt;	
$\text{for }i=0\to m-1:$
&lt;div style=&#34;padding-left: 1em;&#34;&gt;&lt;blockquote&gt;
$(1)$ 对于每个儿子调用 $\text{calc}$，使 $\b x_j$ 变为 $\omega_m^i\b x_j+\text{content}\_{\text{son}_{u,j}}$
&lt;p&gt;$\b y\xleftarrow{+}\tilde f(\b x_1,\cdots,\b x_d)$&lt;/p&gt;
&lt;p&gt;原样撤回 $(1)$&lt;/p&gt;
&lt;/blockquote&gt;&lt;/div&gt;
&lt;/blockquote&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里 $\omega_m$ 是 $m$ 次单位根，有性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于 $&amp;lt;m$ 次多项式 $P$，$\displaystyle\sum_{i=0}^{m-1}P(\omega_m^iu_1+v_1,\cdots,\omega_m^iu_k+v_k)=P(v_1,\cdots,v_k)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;证明很简单：考虑 $P(x_1,\cdots,x_k)=x_1^{p_1}\cdots x_k^{p_k}$，那么
$$
\displaystyle\sum_{i=0}^{m-1}P(\omega_m^iu_1+v_1,\cdots,\omega_m^iu_k+v_k)=\sum_{q_1,\cdots,q_k}\left(\sum_{i=0}^{m-1}\omega^{i\cdot\sum_{j=1}^kq_j}\right)\prod_{j=1}^k\binom{p_j}{q_j}u_j^{q_j}v_j^{p_j-q_j}=v_1^{p_1}\cdots v_k^{p_k}
$$
只有 $m\mid\sum q_j$ 时打括号的和式才非零，而且特征为 $2$，$m$ 是奇数，所以后一个就直接取等过去了。&lt;/p&gt;
&lt;p&gt;空间是（再次注意，这里不计输入）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u$，全局记着就是 $h\log d$&lt;/li&gt;
&lt;li&gt;所有寄存器，$(d+1)bq$&lt;/li&gt;
&lt;li&gt;局部循环变量，$h\log(d^2b)$&lt;/li&gt;
&lt;li&gt;求 $\tilde f$ 时所需的临时空间，$db+q$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在只要求 $db&amp;lt;m&amp;lt;2^q$，因此总共 $\Omicron((h+db)\log(db))$。一个显而易见的优化空间是，可以把若干 $01$ 位压到 $\tilde f$ 的一个参数里：考虑 $\set{0,1}^p\to\mathbb{F}_{2^q}$ 的单射，设值域为 $S$。这时对于 $x,y\in S$，
$$
[x=y]=\frac{\prod_{z\in S\setminus\set{y}}(x-z)}{\prod_{z\in S\setminus\set{y}}(y-z)}
$$
因此 $\deg\tilde f=db2^p/p$，寄存器的空间是 $(d+1)bq/p$，只需要 $db/p&amp;lt;2^{q-p}$ 即可，那索性取 $p=\log(db)=q/2$。这样空间就优化到 $\Omicron(h\log(db)+db)$。&lt;/p&gt;
&lt;p&gt;假定 computation graph 已知，直接套用，代入 $h=t(n)/b(n)$，$b=b(n)$，就直接平衡到了惊人的 $\Omicron(\sqrt{t(n)\log t(n)})$。&lt;/p&gt;
&lt;h3 id=&#34;细节处理&#34;&gt;细节处理
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;仍然将原图灵机先分块化。&lt;/li&gt;
&lt;li&gt;为了处理输入 $\text{content}_0$ 在 $t(n)=\omicron(n^2)$ 时过长，需要将它再分块。所以索性将所有节点存储的信息都缩减成单带的一段。现在节点有两种：$(h,i)$ 表示第 $h$ 带的第 $i$ 时间块，$(h,0,i)$ 表示第 $h$ 带初始时第 $i$ 段。连边还是类似的。&lt;/li&gt;
&lt;li&gt;枚举 computation graph。只需枚举每个时间块结束时每个读写头怎么移动，总共是 $(\log_23)d\frac{t(n)}{b(n)}$ bits。没必要把图显式建出，求儿子可以枚举后暴力区间求和检验。&lt;/li&gt;
&lt;li&gt;在调用上述 TEP 算法过程中，求 $\tilde f$ 时需要对于 $f$ 的每组输入模拟。如果输入中有 &lt;code&gt;FAIL&lt;/code&gt; 直接返回 &lt;code&gt;FAIL&lt;/code&gt;，否则可以原地模拟，再检查最后一步的走向是否与枚举的相符，不符也返回 &lt;code&gt;FAIL&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;TEP 算法中的 $u$ 是不是可以直接用节点号描述啊……反正瓶颈不在这。另外 computation graph 对应的树不是满的，补补齐就行。&lt;/li&gt;
&lt;li&gt;另外有个小细节，我们不要求 $t(n)$ 是 time/space-constructible 的，因为我们可以在整个模拟外套上枚举 $t(n)$ 的循环。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;多维推广&#34;&gt;多维推广
&lt;/h3&gt;&lt;p&gt;设维数为 $m$。将每个纸带划分为边长为 $b(n)$ 的超立方体。论文里说多维不存在分块图灵机，我不太理解，应该也可以用类似上面叠相邻块内容的方法啊？&lt;/p&gt;
&lt;p&gt;不分块也没什么问题。同一时间块内，每一维可能跨两段，因此访问到的总段数不超过 $2^m$，从而 computation graph 中每个节点的入度不超过 $(2^m+1)d$，每个节点的 content 仍包括：状态、时间块结束时读写头位置（$d\log t(n)$）、访问到的段的相对位置及内容（$2^db(n)^d$ 或 $3^db(n)^d$），因此位数是 $\Theta(b(n)^d+d\log t(n))$ 的，方便起见认为 $\log t(n)=\omicron(b(n))$。代入 TEP 的复杂度：
$$
\Omicron\left(\frac{t(n)}{b(n)}\log b(n)+b(n)^d\right)
$$
取 $b(n)=\sqrt[d+1]{t(n)\log t(n)}$ 得 $\Omicron((t(n)\log t(n))^{1-\frac{1}{d+1}})$。&lt;/p&gt;
&lt;h3 id=&#34;推论&#34;&gt;推论
&lt;/h3&gt;&lt;p&gt;对于 space-constructible 的 $t(n)$，
$$
\mathsf{TIME}[t(n)]\subseteq\mathsf{SPACE}[\sqrt{t(n)\log t(n)}]\subsetneq\mathsf{SPACE}[t(n)^{\frac12+\varepsilon}]
$$
特别地，$\mathsf{SPACE}[n]$ 中有语言 $\notin\mathsf{TIME}[n^{2-\varepsilon}]$ 及 $\notin\mathsf{TIME}[n^2/\log^{&amp;gt;1}n]$。可以考虑一个“$\mathsf{SPACE}[n]$-complete”的例子：
$$
\set{\langle M,x,1^k\rangle\mid \lvert M\rvert\le k\text{ 且 }M\text{ 在 }k\text{ 空间内停机}}
$$
另外，如果 $\mathsf{TIME}[t(n)]\subseteq\mathsf{SPACE}[t(n)^{\frac12-\varepsilon}]$，那么交替图灵机和随机存取图灵机都可以在时间 $t(n)^{1-2\varepsilon}$ 内模拟 $t(n)$ 的图灵机。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>OIer 速通编入门</title>
        <link>https://littlereuben.github.io/study/course-cppstudio/</link>
        <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/course-cppstudio/</guid>
        <description>&lt;img src="https://littlereuben.github.io/study/course-cppstudio/cover.png" alt="Featured image of post OIer 速通编入门" /&gt;&lt;div style=&#34;background-color: #FFF9B9; color: #796E00; padding-left: 1em; border-left: 4px solid #DED041; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;学习笔记&lt;/b&gt;。&lt;/div&gt;
&lt;h3 id=&#34;c-动态内存&#34;&gt;C 动态内存
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;malloc&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;字节数&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// 返回 void* 类型，可以隐式与别的指针类型转换
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;calloc&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;数量&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;单个元素字节数&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// 全置 0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;realloc&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;指针&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;新的字节数&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// 指针可以是先前申请的或 NULL，如果是申请的，相当于 free，malloc 并复制。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;free&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;指针&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// 释放
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;糖：&lt;code&gt;int * p = calloc(10, sizeof * p);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;对了，关于 C++ 的 &lt;code&gt;new&lt;/code&gt; 有两点提一下，一个时 &lt;code&gt;new int [1]&lt;/code&gt; 必须用 &lt;code&gt;delete []&lt;/code&gt; 因为 &lt;code&gt;new&lt;/code&gt; 数组时会额外申请空间记录数组大小。另一个是 &lt;a class=&#34;link&#34; href=&#34;https://en.m.wikipedia.org/wiki/Placement_syntax&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;placement new&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;多文件编译&#34;&gt;多文件编译
&lt;/h3&gt;&lt;h4 id=&#34;odr&#34;&gt;ODR
&lt;/h4&gt;&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;In any translation unit, a template, type, function, or object can have no more than one definition. Some of these can have any number of declarations. A definition provides an instance.&lt;/li&gt;
&lt;li&gt;In the entire program, an object or non-inline function cannot have more than one definition; if an object or function is used, it must have exactly one definition. You can declare an object or function that is never used, in which case you don&amp;rsquo;t have to provide a definition. In no event can there be more than one definition.&lt;/li&gt;
&lt;li&gt;Some things, like types, templates, and extern inline functions, can be defined in more than one translation unit. For a given entity, each definition must have the same sequence of tokens. Non-extern objects and functions in different translation units are different entities, even if their names and types are the same.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;头文件-ifndef&#34;&gt;头文件 ifndef
&lt;/h4&gt;&lt;p&gt;多文件编译的原理为：将 &lt;code&gt;.h&lt;/code&gt; 中的内容加到每个 &lt;code&gt;.cpp&lt;/code&gt; 中（preprocess），然后编译这些 &lt;code&gt;.cpp&lt;/code&gt;，然后把他们链接起来。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;.h&lt;/code&gt; 里的 &lt;code&gt;#ifndef&lt;/code&gt; 那些东西是为了避免同一个 &lt;code&gt;.cpp&lt;/code&gt; &lt;code&gt;include&lt;/code&gt; 多次同一个 &lt;code&gt;.h&lt;/code&gt;。例如&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// a.cpp
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;#34;h1.h&amp;#34;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;#34;h2.h&amp;#34;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// h1.h
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;#34;h.h&amp;#34;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// h2.h
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;#34;h.h&amp;#34;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// h.h
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PI&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;3.14&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;常量、类定义等内容不能在一个翻译单元中定义多次。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注意，这个技巧不能避免多个文件之间的重复定义，例如在 &lt;code&gt;.h&lt;/code&gt; 里加了 &lt;code&gt;int a;&lt;/code&gt; 之类的照样会造成 redefinition。&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;链接性&#34;&gt;链接性
&lt;/h4&gt;&lt;p&gt;链接性分为外部、内部、无。外部链接性意味着作用域可以在多个 &lt;code&gt;.cpp&lt;/code&gt;，前提是在定义所在的文件以外的文件中，要写 &lt;code&gt;extern&lt;/code&gt;（并且不初始化）。内部链接性意味着作用域只在当前文件内，无链接性就是在代码块内。&lt;/p&gt;
&lt;p&gt;全局变量、函数都是默认外部的，这就意味着它们不能在多个文件里或头文件里定义（但是函数可以多次声明，甚至在同一个文件里连续写两次 &lt;code&gt;int f(int) ;&lt;/code&gt; 都没问题）。如果要变成内部就在开头加 &lt;code&gt;static&lt;/code&gt;。&lt;/p&gt;
&lt;h4 id=&#34;头文件里可以定义什么&#34;&gt;头文件里可以定义什么
&lt;/h4&gt;&lt;p&gt;&lt;code&gt;#define&lt;/code&gt; 肯定没什么问题。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;const&lt;/code&gt; 比较特殊，它是内部链接的，所以可以写在头文件里。如果要强制它变成外部的，就加 &lt;code&gt;extern&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;struct&lt;/code&gt; 和 &lt;code&gt;class&lt;/code&gt; 可以，但是成员函数不能在头文件里放在外面。类不能重复定义，但是可以在链接是出现重复，但是两个重复的不能不同，否则会出一些奇怪的情况。例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// a.cpp
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;#34;h.h&amp;#34;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// b.cpp
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;#include&lt;/span&gt; &lt;span class=&#34;cpf&#34;&gt;&amp;#34;h.h&amp;#34;&lt;/span&gt;&lt;span class=&#34;cp&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;cp&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;// h.h
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这样不会 CE，但是会出现一些诡异的问题，比如一个文件里得按另一个的名字来。&lt;/p&gt;
&lt;p&gt;类定义不能在外部链接，也就是说不能在另一个文件里写 &lt;code&gt;extern node x;&lt;/code&gt; 来定义一个 &lt;code&gt;node&lt;/code&gt;（在另一个文件里定义的类）的实例。&lt;/p&gt;
&lt;h3 id=&#34;cmake&#34;&gt;CMake
&lt;/h3&gt;&lt;p&gt;&lt;code&gt;cmake 目录&lt;/code&gt; 指令会对目录中的文件（含 &lt;code&gt;CMakeLists.txt&lt;/code&gt;、源文件）生成 build system。这一步还没有编译，可以理解为生成了以 &lt;code&gt;Makefile&lt;/code&gt; 为代表的一系列文件。&lt;/p&gt;
&lt;p&gt;如果要进一步指定目录，&lt;code&gt;-B&lt;/code&gt; 可指定输出目录，&lt;code&gt;-S&lt;/code&gt; 可指定输入目录。如果没有这个目录，直接 &lt;code&gt;cmake 目录&lt;/code&gt; 会报错，但 &lt;code&gt;cmake -B 目录&lt;/code&gt; 则会自动创建一个。&lt;/p&gt;
&lt;p&gt;如果 &lt;code&gt;cmake&lt;/code&gt; 报错&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  Running
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &amp;#39;nmake&amp;#39; &amp;#39;-?&amp;#39;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  failed with:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   系统找不到指定的文件。
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;就在 &lt;code&gt;cmake&lt;/code&gt; 后面加 &lt;code&gt;-G &amp;quot;MinGW Makefiles&amp;quot;&lt;/code&gt;。或者直接改环境变量：&lt;code&gt;setx CMAKE_GENERATOR &amp;quot;MinGW Makefiles&amp;quot; /m&lt;/code&gt;（需要管理员模式运行 Powershell，或者直接图形界面改也行）。&lt;/p&gt;
&lt;p&gt;注意一旦 &lt;code&gt;cmake&lt;/code&gt; 失败一次，就需要删掉所有相关文件再重新 &lt;code&gt;cmake&lt;/code&gt;，不然会傻傻地仍然报错，或提示已经 build 过。&lt;/p&gt;
&lt;p&gt;然后正式编译，使用 &lt;code&gt;make [-C 目录]&lt;/code&gt;（不写 &lt;code&gt;-C&lt;/code&gt; 只能 &lt;code&gt;make&lt;/code&gt; 当前目录）或 &lt;code&gt;cmake --build [目录]&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;以后改代码，只需重新 &lt;code&gt;make&lt;/code&gt; 就行。&lt;/p&gt;
&lt;p&gt;一般习惯把 build system 的部分放到子文件夹里。汇总：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir build 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; build
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cmake ..
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;make
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;./对应名字
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;参考：&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://wshibin.github.io/misc/cmake/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://wshibin.github.io/misc/cmake/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://cmake.org/cmake/help/latest/guide/tutorial/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://cmake.org/cmake/help/latest/guide/tutorial/index.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV14h41187FZ/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV14h41187FZ/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;vcpkg&#34;&gt;vcpkg
&lt;/h3&gt;&lt;p&gt;vs 或 wsl，请。&lt;/p&gt;
&lt;h3 id=&#34;类&#34;&gt;类
&lt;/h3&gt;&lt;p&gt;基本结构、重载运算符、后置 &lt;code&gt;const&lt;/code&gt;、构造析构略。&lt;/p&gt;
&lt;h4 id=&#34;c-struct&#34;&gt;C struct
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;typedef&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;node&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这里 &lt;code&gt;_&lt;/code&gt; 可省，直接写成匿名结构体。&lt;/p&gt;
&lt;h4 id=&#34;静态成员变量和函数&#34;&gt;静态成员变量和函数
&lt;/h4&gt;&lt;p&gt;顾名思义，加 &lt;code&gt;static&lt;/code&gt;，就是与类关联而不是与某个对象关联。注意 C++11 之前，类里是不能 &lt;code&gt;const&lt;/code&gt; 的，只能 &lt;code&gt;static const&lt;/code&gt;，或者 &lt;code&gt;enum&lt;/code&gt;。&lt;/p&gt;
&lt;h4 id=&#34;成员函数&#34;&gt;成员函数
&lt;/h4&gt;&lt;p&gt;成员函数如果传同类参，可以访问对方的 &lt;code&gt;private&lt;/code&gt; 成员。&lt;/p&gt;
&lt;p&gt;类内定义的成员函数是自动内联的。&lt;/p&gt;
&lt;p&gt;成员函数可以类外定义，必须再类内先声明，定义时要加 &lt;code&gt;::&lt;/code&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;::&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;众所周知，在成员函数头后加 &lt;code&gt;const&lt;/code&gt; 可以保证成员不被改变。但是它还能用于重载，拓宽了可使用情景：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cerr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;non-const&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cerr&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;const&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这段代码会输出 &lt;code&gt;non-const\nconst\n&lt;/code&gt;。&lt;/p&gt;
&lt;h4 id=&#34;友元&#34;&gt;友元
&lt;/h4&gt;&lt;p&gt;友元函数是一种非成员函数，其声明必须在类内，定义可以在类内或类外，但在类内时不能直接使用成员变量。其关键作用是能在类外访问私有成员。显然这种东西不可能是私有的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;friend&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ostream&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;operator&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ostream&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;ostream&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;operator&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ostream&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;out&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;out&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;/&amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;out&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意声明时参列里 &lt;code&gt;&amp;amp;&lt;/code&gt; 不能少。&lt;/p&gt;
&lt;p&gt;友元类是在类定义中写 &lt;code&gt;friend class 另一个类的名字&lt;/code&gt;，然后另一个类就可以访问这个类的私有成员了。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;friend&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sa&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;sa&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sb&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;如果只希望 &lt;code&gt;B&lt;/code&gt; 的某个函数可以访问 &lt;code&gt;A&lt;/code&gt; 的私有成员（以限制 &lt;code&gt;B&lt;/code&gt; 不在别的地方瞎搞），可以用友元成员函数：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sa&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// 必须要有！
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;        &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;friend&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;::&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;::&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;add&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sa&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sb&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们来仔细考虑一下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt; 定义 → &lt;code&gt;B&lt;/code&gt; 定义：&lt;code&gt;A&lt;/code&gt; 里根本写不了 &lt;code&gt;B&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;B&lt;/code&gt; 定义 → &lt;code&gt;A&lt;/code&gt; 定义：&lt;code&gt;B&lt;/code&gt; 里根本写不了 &lt;code&gt;A&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;A&lt;/code&gt; 声明 → &lt;code&gt;B&lt;/code&gt; 定义 → &lt;code&gt;A&lt;/code&gt; 定义：&lt;code&gt;B&lt;/code&gt; 定义里要用 &lt;code&gt;A&lt;/code&gt; 的成员，但 &lt;code&gt;A&lt;/code&gt; 没定义。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;B&lt;/code&gt; 声明 → &lt;code&gt;A&lt;/code&gt; 定义 → &lt;code&gt;B&lt;/code&gt; 定义：&lt;code&gt;A&lt;/code&gt; 要写 &lt;code&gt;friend void B :: ...&lt;/code&gt;，&lt;code&gt;B&lt;/code&gt; 是得定义完的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以最后只能 &lt;code&gt;A&lt;/code&gt; → &lt;code&gt;B&lt;/code&gt; → &lt;code&gt;A&lt;/code&gt; → &lt;code&gt;B&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;唐。&lt;/p&gt;
&lt;h4 id=&#34;类继承&#34;&gt;类继承
&lt;/h4&gt;&lt;p&gt;思想：两个类有共同特征，不想重复写这部分。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;class 派生类名 : [public|protected|private] 基类名 { 定义派生类独有的内容 } ;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;如果要继承多个，就用 &lt;code&gt;,&lt;/code&gt; 分隔。注意 &lt;code&gt;class C : public A, B&lt;/code&gt; 相当于 &lt;code&gt;class C : public A, private B&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;基类中 &lt;code&gt;private&lt;/code&gt; 的无法在派生类中访问，如果想访问但不希望被外面访问，就用 &lt;code&gt;protected&lt;/code&gt;。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;继承方式\基类内权限&lt;/th&gt;
&lt;th&gt;&lt;code&gt;public&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;protected&lt;/code&gt;&lt;/th&gt;
&lt;th&gt;&lt;code&gt;private&lt;/code&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;public&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;public&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;protected&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;inaccessible&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;protected&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;protected&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;protected&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;inaccessible&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;&lt;code&gt;private&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;private&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;private&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;inaccessible&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;构造函数：推荐在派生类初始化列表中调用基类的构造函数，而不是直接赋值基类的元素。如果不写，则调用基类的默认构造函数。注意，唯一不行的是在初始化列表中初始化基类的元素，因为构造函数是先调用基类的构造函数，如果这样写就重复了。&lt;/p&gt;
&lt;p&gt;如果不写构造函数，也可以用初始化列表的方式（C++ 17），注意列表对应顺序是先基类后派生类。&lt;/p&gt;
&lt;p&gt;派生类无法使用基类的重载运算符和友元函数。&lt;/p&gt;
&lt;h4 id=&#34;多态&#34;&gt;多态
&lt;/h4&gt;&lt;p&gt;派生类可以重载基类的变量和函数，如果要使用基类中的，就用 &lt;code&gt;::&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;多态可以展开讲的最大的点就是虚方法。考虑：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Value = &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Sum = &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sum&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;C&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;rec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    	&lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;s&#34;&gt;&amp;#34;Product = &amp;#34;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prod&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;endl&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;// do something to b1, b2, c1, c2.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;};&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// 指针和引用可以从派生类隐式转换到基类
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;++&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;显然他会输出四行 &lt;code&gt;Value = ...&lt;/code&gt;，但是我们希望它能输出两行 &lt;code&gt;Sum&lt;/code&gt;，两行 &lt;code&gt;Product&lt;/code&gt;，又懒得一个个 &lt;code&gt;. print()&lt;/code&gt;。那怎么办呢？&lt;/p&gt;
&lt;p&gt;我们在基类的函数开头加一个 &lt;code&gt;virtual&lt;/code&gt;（必须），然后在派生类的函数大括号前加 &lt;code&gt;override&lt;/code&gt;（可选）。在多重继承时，会自动找到最深的重载。&lt;/p&gt;
&lt;p&gt;如果强制一个类的派生类不能重载，就加 &lt;code&gt;final&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;问题：如何使继承链中途不再虚？&lt;/p&gt;
&lt;p&gt;问题：如果真的基类对象调用纯虚函数会怎样？&lt;/p&gt;
&lt;p&gt;抽象基类是具有纯虚函数的类，纯虚函数是不给出定义的虚函数：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;virtual&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这种情况下，不能建立该类的对象，且必须在（要有对象的）派生类中给出纯虚函数的定义。&lt;/p&gt;
&lt;p&gt;析构函数是一类在类继承中往往必须要是虚函数的函数。考虑以下代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// 析构函数必须是公有的或保护的，不然程序结束时都调用不了，直接 CE
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;        &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;程序结束时会调用 &lt;code&gt;b&lt;/code&gt; 的析构函数，先执行 &lt;code&gt;~B()&lt;/code&gt;，再执行 &lt;code&gt;~A()&lt;/code&gt;，输出 &lt;code&gt;ba&lt;/code&gt;。这没啥问题。（注意：动态分配内存的对象在程序结束时不会调用析构函数）&lt;/p&gt;
&lt;p&gt;但是如果 &lt;code&gt;main&lt;/code&gt; 里有这样的代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;delete&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;那就会导致只有基类的析构函数被调用。这在派生类里没有单独的动态内存时是没问题的（有静态变量是没问题的，也会跟着删掉），但是如果：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{};&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;delete&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;那就会造成内存泄露。所以如果存在基类指针指向派生类对象的情况，析构函数就必须要是虚的。&lt;/p&gt;
&lt;p&gt;虚函数的多态的原理，是动态联编，大概就是每个对象都记录了一个函数指针表，用于查找它应该调用哪一级的重载的函数。因为类指针指向的对象类型可能取决于输入之类的，所以不得不边运行边确定，导致很慢。&lt;/p&gt;
&lt;h4 id=&#34;虚继承&#34;&gt;虚继承
&lt;/h4&gt;&lt;p&gt;复杂的继承会出问题，比如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;B&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;C&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;D&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;一个 &lt;code&gt;D&lt;/code&gt; 对象，没法直接访问 &lt;code&gt;.a&lt;/code&gt;，必须指定是 &lt;code&gt;.B::a&lt;/code&gt; 还是 &lt;code&gt;.C::a&lt;/code&gt;，函数同理。&lt;/p&gt;
&lt;p&gt;如果希望合并两个 &lt;code&gt;a&lt;/code&gt;，需要把 &lt;code&gt;B&lt;/code&gt; 和 &lt;code&gt;C&lt;/code&gt; 的继承方式改成虚继承 &lt;code&gt;virtual public A&lt;/code&gt;，与此同时当 &lt;code&gt;D&lt;/code&gt; 的构造函数递归调用 &lt;code&gt;B&lt;/code&gt; 和 &lt;code&gt;C&lt;/code&gt; 的构造函数时，将会忽略对 &lt;code&gt;A&lt;/code&gt; 的构造函数的递归调用，需要单独调用 &lt;code&gt;A&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;但是重载的函数还是没法合并，所以最好 &lt;code&gt;D&lt;/code&gt; 自己写一个，当然可以直接调用 &lt;code&gt;B::&lt;/code&gt; 和 &lt;code&gt;C::&lt;/code&gt; 的，这个的重复就无法避免了。&lt;/p&gt;
&lt;h3 id=&#34;模板&#34;&gt;模板
&lt;/h3&gt;&lt;p&gt;最基础的略。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;double res = max(2.5, 3);&lt;/code&gt; 不行，&lt;code&gt;double res = max &amp;lt;double&amp;gt; (2.5, 3);&lt;/code&gt; 可以。&lt;/p&gt;
&lt;p&gt;显式实例化：&lt;code&gt;template 返回类型 函数名 &amp;lt;具体化类型&amp;gt; (参数列表) ;&lt;/code&gt; 表示生成一个对应的实例。&lt;/p&gt;
&lt;p&gt;显式具体化：&lt;code&gt;template &amp;lt;&amp;gt; 返回类型 函数名 (&amp;lt;具体化类型&amp;gt;) (参数列表) { ... }&lt;/code&gt; 其中第二个 &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt; 是可选的。和实例化不同的是，它相当于一种“内容不同的重载”，必须有函数定义，且出现在第一次调用之前（否则与隐式实例化冲突）。例如&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&amp;gt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;char&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;strcmp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;?&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;模板类：开头加一样的东西就行。类外定义函数： &lt;code&gt;template &amp;lt;typename T&amp;gt; 返回类型 类名 &amp;lt;T&amp;gt; :: 函数头 { ... }&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;然后我们考虑一个例子：希望写一个 &lt;code&gt;max&lt;/code&gt;，它能接受一个 &lt;code&gt;int&lt;/code&gt; 和一个 &lt;code&gt;double&lt;/code&gt;（之类的），并返回 &lt;code&gt;double&lt;/code&gt;（选择级别高的类型）。如果这样：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;typename&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;typename&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;?&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;?&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;?&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;cout&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;3.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;sc&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;3.5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个就会有问题。可以用 &lt;code&gt;C++14&lt;/code&gt; 的特性，自动推导返回类型：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;typename&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;typename&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;auto&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;U&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;?&lt;/span&gt; &lt;span class=&#34;nl&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;stl-查漏补缺&#34;&gt;STL 查漏补缺
&lt;/h3&gt;&lt;p&gt;遍历一些 STL 对象时，建议用 &lt;code&gt;const_iterator&lt;/code&gt;，对应 &lt;code&gt;. cbegin()&lt;/code&gt; 和 &lt;code&gt;. cend()&lt;/code&gt;。作用是无法修改内容。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;++ it&lt;/code&gt; 比 &lt;code&gt;it ++&lt;/code&gt; 快，考虑 &lt;code&gt;it ++&lt;/code&gt; 具体该怎么实现：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;iterator&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;operator&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;++&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;iterator&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;tmp&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;this&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;// do increment
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tmp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;STL 提供了一个一般的 &lt;code&gt;find&lt;/code&gt;，它是暴力，不要将它用于已经有自己的 &lt;code&gt;find&lt;/code&gt; 的类。&lt;/p&gt;
&lt;h3 id=&#34;移动语义和右值引用&#34;&gt;移动语义和右值引用
&lt;/h3&gt;&lt;p&gt;左值是有地址的，可以赋值的变量。&lt;/p&gt;
&lt;p&gt;右值的例子包括字面量、四则运算的结果、函数的非引用返回值等。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;能否引用&lt;/th&gt;
&lt;th&gt;非常量左值引用&lt;/th&gt;
&lt;th&gt;常量左值引用&lt;/th&gt;
&lt;th&gt;非常量右值引用&lt;/th&gt;
&lt;th&gt;常量右值引用&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;非常量左值&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;常量左值&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;右值&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;×&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;目前不知道常量右值是什么，也不知道常量右值引用有啥用。&lt;/p&gt;
&lt;p&gt;右值引用实际上是开了空间的。因此可以这样：&lt;code&gt;int &amp;amp;&amp;amp; p = 1; p = 2;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;可以用 &lt;code&gt;move&lt;/code&gt;（C++ 11）将左值转成右值。&lt;/p&gt;
&lt;p&gt;目前看起来右值引用没啥用。在复杂的类的应用中，考虑提高这段代码的效率：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;public&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;operator&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;this&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;this&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;k&#34;&gt;delete&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;memcpy&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;sizeof&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;));&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;			&lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;this&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;o&#34;&gt;~&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;delete&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;	&lt;span class=&#34;k&#34;&gt;private&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;		&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意必须重载复制构造函数和赋值，否则会导致多对一。&lt;/p&gt;
&lt;p&gt;如果将一个对象复制给另一个对象，且这个对象不再使用（例如，这个对象是函数的返回值），那么暴力赋值就可以优化——我们希望直接把赋值者的内容转移给被赋值者，但是为了避免多对一析构爆炸，得把赋值者清空（&lt;code&gt;delete nullptr&lt;/code&gt; 不会发生任何事）。这就必须要求我们能修改等号右边的东西，这就是右值引用在移动语义中的作用。我们可以新增一个“移动构造函数”：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;nullptr&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;移动赋值同理。如果要将 &lt;code&gt;a&lt;/code&gt; 赋值给 &lt;code&gt;b&lt;/code&gt; 且 &lt;code&gt;a&lt;/code&gt; 不再用了，写 &lt;code&gt;b = move(a)&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;智能指针&#34;&gt;智能指针
&lt;/h3&gt;&lt;p&gt;我们现在希望偷懒不写 &lt;code&gt;delete&lt;/code&gt;，于是类似上面，我们会写个指针类来自动 &lt;code&gt;delete&lt;/code&gt;，但是与上面不同的是，指针类并不负责申请内存，也不负责复制内容，只负责自动释放空间。这就导致如果两个指针指向同一片内存就会爆。&lt;/p&gt;
&lt;p&gt;头文件 &lt;code&gt;memory&lt;/code&gt;。C++98 有 &lt;code&gt;auto_ptr&lt;/code&gt;，略。C++11 有 &lt;code&gt;unique_ptr&lt;/code&gt; 和 &lt;code&gt;shared_ptr&lt;/code&gt;。&lt;code&gt;unique_ptr&lt;/code&gt; 禁止复制（构造和赋值），只能 &lt;code&gt;move&lt;/code&gt; 之后再赋；&lt;code&gt;shared_ptr&lt;/code&gt; 会维护一个计数器，等最后一个相关智能指针对象析构时才 &lt;code&gt;delete&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;用法 1：&lt;code&gt;unique/shared_ptr &amp;lt;int&amp;gt; p (new int [100]);&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;不推荐用法 2：&lt;code&gt;int * p = new int [100]; unique/shared_ptr &amp;lt;int&amp;gt; q (p);&lt;/code&gt; 会导致多对一。&lt;/p&gt;
&lt;p&gt;用法 3：&lt;code&gt;auto p (make_unique/shared &amp;lt;int&amp;gt; (100));&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;类型转换&#34;&gt;类型转换
&lt;/h3&gt;&lt;p&gt;C++ 的类型转换：&lt;code&gt;static_cast &amp;lt;转换成的类型&amp;gt; (原变量)&lt;/code&gt;。在编译时会检查转换是否合法。&lt;/p&gt;
&lt;p&gt;C 中，这样的代码是可以的（逆天）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;const&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;C++ 中，不能这样 &lt;code&gt;static_cast &amp;lt;int &amp;amp;&amp;gt;&lt;/code&gt; 但是可以 &lt;code&gt;const_cast &amp;lt;int &amp;amp;&amp;gt;&lt;/code&gt;。只有特殊情况需要修改时才用。&lt;/p&gt;
&lt;p&gt;C 中，这样的代码是可以的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;struct&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;main&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;new&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;node&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;p&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;C++ 中，不能这样 &lt;code&gt;static_cast &amp;lt;node *&amp;gt;&lt;/code&gt; 但是可以 &lt;code&gt;reinterpret_cast &amp;lt;node *&amp;gt;&lt;/code&gt;。总的来说，感觉就是避免你无意识地瞎转类型，但是你硬要转也是可以的，效果和 C 的一样，就是“棒转”，不做任何事情。&lt;/p&gt;
&lt;p&gt;有且只有 &lt;code&gt;dynamic_cast&lt;/code&gt; 只能用于指针和引用。&lt;code&gt;dynamic_cast&lt;/code&gt; 可以把基类转换到派生类（&lt;code&gt;static_cast&lt;/code&gt; 也可以做）。如果被转换者类型不对（包括把本身就指向基类对象的指针转成派生类），会返回 &lt;code&gt;nullptr&lt;/code&gt;。&lt;code&gt;dynamic_cast&lt;/code&gt; 转换的必须是多态类型，即至少声明或继承（可以在本身 &lt;code&gt;override&lt;/code&gt;）了一个虚函数的类。&lt;/p&gt;
&lt;h3 id=&#34;异常&#34;&gt;异常
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;抛出异常 &lt;code&gt;throw 信息&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;try&lt;/code&gt; 开始代码块，里面可以用 &lt;code&gt;throw&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;捕获异常 &lt;code&gt;catch (信息类型 [变量名字]) { ... }&lt;/code&gt; 接在 &lt;code&gt;try&lt;/code&gt; 的代码块后，可以接多个处理方式。&lt;code&gt;catch (...)&lt;/code&gt; 通用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;可以跨函数处理（再抛出）。跨的函数有返回值的赋值也会跳过。&lt;/p&gt;
&lt;p&gt;为了区分不同（大类的）异常情况，可以建立各种异常类。值得注意的是，如果抛出的类型是派生类，而捕获的类型中既有基类又有派生类，且前者写在前面，那就会被前者捕获。&lt;/p&gt;
&lt;p&gt;STL 中的东西的异常都是 &lt;code&gt;std :: exception&lt;/code&gt; 的派生类，可以调用 &lt;code&gt;. what()&lt;/code&gt; 知道它是哪种。&lt;/p&gt;
&lt;h3 id=&#34;优化原理&#34;&gt;优化原理
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;分支预测&lt;/li&gt;
&lt;li&gt;循环展开&lt;/li&gt;
&lt;li&gt;并行处理、SIMD&lt;/li&gt;
&lt;li&gt;循环顺序&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;标准属性&#34;&gt;标准属性
&lt;/h3&gt;&lt;p&gt;发现有个 project 里有些 &lt;code&gt;[[...]]&lt;/code&gt; 这种东西，加在声明或定义的类型名前面。这是起到修改编译器对一些特殊情况的反应的。包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[[nodiscard]]&lt;/code&gt; 加在函数声明前，如果该函数的返回值未被使用会报警告，可以通过 &lt;code&gt;[[nodiscard(&amp;quot;...&amp;quot;)]]&lt;/code&gt; 加警告信息。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[[deprecated]]&lt;/code&gt; 表示对应东西可能被弃用，不鼓励使用（使用会警告），同上。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[[noreturn]]&lt;/code&gt; 表示对应函数不会正常返回（&lt;code&gt;exit&lt;/code&gt; 或 &lt;code&gt;throw&lt;/code&gt; 之类），提示编译器优化。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[[likely]]&lt;/code&gt; 和 &lt;code&gt;[[unlikely]]&lt;/code&gt; 加在分支语句后，表示更可能进入哪个分支，提示编译器优化。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;可变参数&#34;&gt;可变参数
&lt;/h3&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/694317432&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/694317432&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;例题&#34;&gt;例题
&lt;/h3&gt;&lt;p&gt;有个题感觉还是很 educational 的。如何实现一个高维数组，每维的长度都是 $n$，但维数 $d$ 不定？可以用模板类：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;typename&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Array&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;Array&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;d&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;];&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;但这里的问题是 $d=0$ 的时候爆了。可以用部分具体化：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;template&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;typename&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;int&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;Array&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
        </item>
        <item>
        <title>算设 Survey：一些经典问题的 log 因子优化算法</title>
        <link>https://littlereuben.github.io/study/course-algo-survey/</link>
        <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
        
        <guid>https://littlereuben.github.io/study/course-algo-survey/</guid>
        <description>&lt;div style=&#34;background-color: #FFF9B9; color: #796E00; padding-left: 1em; border-left: 4px solid #DED041; line-height: 25pt;&#34;&gt;这是一篇&lt;b&gt;学习笔记&lt;/b&gt;。&lt;/div&gt;
&lt;embed src= &#34;main.pdf&#34; width= &#34;100%&#34; height= &#34;800px&#34; type=&#34;application/pdf&#34; &gt;
&lt;a href=&#34;main.pdf&#34; style=&#34;display:block;text-align:center;&#34;&gt;&lt;b&gt;[PDF file]&lt;/b&gt;&lt;/a&gt;
</description>
        </item>
        
    </channel>
</rss>
